{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"**Quiz-11 (2024.12.10)**  // ë²”ìœ„: 12wk-1 ê¹Œì§€\"\n",
    "author: \"ìµœê·œë¹ˆ\"\n",
    "date: \"12/10/2024\"\n",
    "comments: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/guebin/MP2024/blob/main/quiz/Quiz-11.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **í•­ëª©**               | **í—ˆìš© ì—¬ë¶€**        | **ë¹„ê³ **                                          |\n",
    "|------------------------|----------------------|---------------------------------------------------|\n",
    "| **ê°•ì˜ë…¸íŠ¸ ì°¸ê³ **      | í—ˆìš©                 | ìˆ˜ì—… ì¤‘ ì œê³µëœ ê°•ì˜ë…¸íŠ¸ë‚˜ ë³¸ì¸ì´ ì •ë¦¬í•œ ìë£Œë¥¼ ì°¸ê³  ê°€ëŠ¥       |\n",
    "| **êµ¬ê¸€ ê²€ìƒ‰**          | í—ˆìš©                 | ì¸í„°ë„·ì„ í†µí•œ ìë£Œ ê²€ìƒ‰ ë° ì •ë³´ í™•ì¸ ê°€ëŠ¥        |\n",
    "| **ìƒì„± ëª¨í˜• ì‚¬ìš©**           | í—ˆìš©            | ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ë„êµ¬(GPT ë“±) ì‚¬ìš© ê°€ëŠ¥            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import transformers \n",
    "import datasets\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. COVID19 tweets -- 100ì "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid19_tweets_df = pd.read_csv('https://raw.githubusercontent.com/guebin/STML2022/main/posts/Corona_NLP_train.csv',encoding=\"ISO-8859-1\")\n",
    "covid19_tweets_df_train = covid19_tweets_df[::2].reset_index(drop=True)\n",
    "covid19_tweets_df_test = covid19_tweets_df[1::2].reset_index(drop=True).loc[:,'UserName':'OriginalTweet'].assign(Sentiment = '???')\n",
    "del covid19_tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3805</td>\n",
       "      <td>48757</td>\n",
       "      <td>35.926541,-78.753267</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Cashier at grocery store was sharing his insig...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3807</td>\n",
       "      <td>48759</td>\n",
       "      <td>Atlanta, GA USA</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Due to COVID-19 our retail store and classroom...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName              Location     TweetAt  \\\n",
       "0      3799       48751                London  16-03-2020   \n",
       "1      3801       48753             Vagabonds  16-03-2020   \n",
       "2      3803       48755                   NaN  16-03-2020   \n",
       "3      3805       48757  35.926541,-78.753267  16-03-2020   \n",
       "4      3807       48759       Atlanta, GA USA  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "2  Me, ready to go at supermarket during the #COV...  Extremely Negative  \n",
       "3  Cashier at grocery store was sharing his insig...            Positive  \n",
       "4  Due to COVID-19 our retail store and classroom...            Positive  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid19_tweets_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>???</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>???</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3804</td>\n",
       "      <td>48756</td>\n",
       "      <td>ÃƒÂœT: 36.319708,-82.363649</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>As news of the regionÃ‚Â’s first confirmed COVID...</td>\n",
       "      <td>???</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3806</td>\n",
       "      <td>48758</td>\n",
       "      <td>Austria</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Was at the supermarket today. Didn't buy toile...</td>\n",
       "      <td>???</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3808</td>\n",
       "      <td>48760</td>\n",
       "      <td>BHAVNAGAR,GUJRAT</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>For corona prevention,we should stop to buy th...</td>\n",
       "      <td>???</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName                   Location     TweetAt  \\\n",
       "0      3800       48752                         UK  16-03-2020   \n",
       "1      3802       48754                        NaN  16-03-2020   \n",
       "2      3804       48756  ÃƒÂœT: 36.319708,-82.363649  16-03-2020   \n",
       "3      3806       48758                    Austria  16-03-2020   \n",
       "4      3808       48760           BHAVNAGAR,GUJRAT  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet Sentiment  \n",
       "0  advice Talk to your neighbours family to excha...       ???  \n",
       "1  My food stock is not the only one which is emp...       ???  \n",
       "2  As news of the regionÃ‚Â’s first confirmed COVID...       ???  \n",
       "3  Was at the supermarket today. Didn't buy toile...       ???  \n",
       "4  For corona prevention,we should stop to buy th...       ???  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid19_tweets_df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ ë°ì´í„°ëŠ” COVID-19 ê´€ë ¨ íŠ¸ìœ— ë°ì´í„°ë¥¼ í¬í•¨í•˜ë©°, ê° íŠ¸ìœ—ì— ëŒ€í•´ ì‘ì„±ìì˜ ê°ì •ì„ ë‚˜íƒ€ë‚´ëŠ” Sentiment(ê°ì •) ë ˆì´ë¸”ì´ ì§€ì •ë˜ì–´ ìˆë‹¤. \n",
    "\n",
    "**ì£¼ìš” ì—´ ì„¤ëª…**\n",
    "\n",
    "`1`. OriginalTweet: ì‚¬ìš©ìê°€ ì‘ì„±í•œ íŠ¸ìœ— ë‚´ìš©ìœ¼ë¡œ. COVID-19ì™€ ê´€ë ¨ëœ ë‹¤ì–‘í•œ ì˜ê²¬, ê²½í—˜, ìƒí™© ë“±ì´ í¬í•¨ë˜ì–´ ìˆìŒ. ì˜ˆì‹œëŠ” ì•„ë˜ì™€ ê°™ìŒ. \n",
    "\n",
    "- \"Coronavirus Australia: Woolworths to give elders priority shopping amid coronavirus pandemic.\"\n",
    "- â€œDue to COVID-19 our retail store and classroom...â€\n",
    "\n",
    "`2`. Sentiment: íŠ¸ìœ—ì˜ ê°ì •ì„ ë‚˜íƒ€ë‚´ëŠ” ë ˆì´ë¸”ìœ¼ë¡œ ê°€ëŠ¥í•œê°’ì€ ì•„ë˜ì™€ ê°™ìŒ. \n",
    "\n",
    "- Extremely Positive: ê°•í•œ ê¸ì •ì  ê°ì •ì„ ë‹´ì•„ í¬ë§ê³¼ ì—°ëŒ€ê°ì„ ì „ë‹¬í•˜ëŠ” íŠ¸ìœ—.\n",
    "- Positive: ê¸ì •ì ì¸ í•´ê²°ì±…ì´ë‚˜ í¬ë§ì ì¸ ë©”ì‹œì§€ë¥¼ ë‹´ê³  ìˆìŒ.\n",
    "- Neutral: ì¤‘ë¦½ì ì´ê³  ê°ì • í‘œí˜„ì´ ì—†ëŠ” íŠ¸ìœ— (ì‚¬ì‹¤ ì „ë‹¬ì— ì´ˆì ).\n",
    "- Negative: ë¬¸ì œì ì´ë‚˜ ë¶ˆë§Œì„ í‘œí˜„í•œ ë¶€ì •ì ì¸ íŠ¸ìœ—.\n",
    "- Extremely Negative: ê·¹ë‹¨ì ìœ¼ë¡œ ë¶€ì •ì ì¸ ìƒí™©ì— ëŒ€í•œ íŠ¸ìœ—.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1)` `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ \"distilbert/distilbert-base-uncased\" ëª¨ë¸ì„ í™œìš©í•˜ì—¬ `OriginalTweet`ì„ ì…ë ¥ìœ¼ë¡œ `Sentiment`ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í…ìŠ¤íŠ¸ ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµí•˜ë¼. \n",
    "\n",
    "**ìš”êµ¬ ì‚¬í•­**\n",
    "\n",
    "`1`. ë°ì´í„° ë¶„í• : ì£¼ì–´ì§„ ë°ì´í„°ì…‹ `covid19_tweets_df_train` ì„ 7:3 ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ì–´, 70%ëŠ” í›ˆë ¨ ë°ì´í„°ë¡œ, ë‚˜ë¨¸ì§€ 30%ëŠ” ê²€ì¦ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ë¼. \n",
    "\n",
    "`2`. í‰ê°€ì§€í‘œ: ëª¨ë¸ì˜ ì„±ëŠ¥í‰ê°€ë¥¼ ìœ„í•˜ì—¬ ì•„ë˜ì˜ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë¼. \n",
    "\n",
    "```Python\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)  # ì˜ˆì¸¡ í´ë˜ìŠ¤\n",
    "\n",
    "    # í‰ê°€ ë©”íŠ¸ë¦­ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    acc = evaluate.load(\"accuracy\")\n",
    "    f1 = evaluate.load(\"f1\")\n",
    "\n",
    "    # í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "    dct1 = acc.compute(predictions=predictions, references=labels)  \n",
    "    dct2 = f1.compute(predictions=predictions, references=labels, average='weighted')\n",
    "\n",
    "    # ë‘ ê°œì˜ ë©”íŠ¸ë¦­ ê²°ê³¼ë¥¼ í•©ì¹˜ê¸°\n",
    "    return dct1 | dct2  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(í’€ì´)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Extremely Negative', 'Extremely Positive', 'Negative', 'Neutral', 'Positive'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(covid19_tweets_df_train['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Positive', 1: 'Neutral', 2: 'Extremely Negative', 3: 'Extremely Positive', 4: 'Negative'}\n",
      "{'Positive': 0, 'Neutral': 1, 'Extremely Negative': 2, 'Extremely Positive': 3, 'Negative': 4}\n"
     ]
    }
   ],
   "source": [
    "id2label = {i:label for i,label in enumerate(set(covid19_tweets_df_train['Sentiment']))}\n",
    "label2id = {label:i for i,label in enumerate(set(covid19_tweets_df_train['Sentiment']))}\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = list(covid19_tweets_df_train['OriginalTweet'])\n",
    "test_text = list(covid19_tweets_df_test['OriginalTweet'])\n",
    "train_label = list(covid19_tweets_df_train.Sentiment.map(label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "train_dct = tokenizer(train_text,truncation=True) | {'labels': train_label}\n",
    "test_dct = tokenizer(test_text,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14405\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 6174\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 20578\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = datasets.Dataset.from_dict(train_dct)\n",
    "test = datasets.Dataset.from_dict(test_dct)\n",
    "d = train.train_test_split(test_size=0.3)\n",
    "covid19_tweets = datasets.DatasetDict({'train':d['train'], 'eval':d['test'], 'test':test})\n",
    "covid19_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", \n",
    "    num_labels=5,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)  # ì˜ˆì¸¡ í´ë˜ìŠ¤\n",
    "\n",
    "    # í‰ê°€ ë©”íŠ¸ë¦­ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    acc = evaluate.load(\"accuracy\")\n",
    "    f1 = evaluate.load(\"f1\")\n",
    "\n",
    "    # í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "    dct1 = acc.compute(predictions=predictions, references=labels)  \n",
    "    dct2 = f1.compute(predictions=predictions, references=labels, average='weighted')\n",
    "\n",
    "    # ë‘ ê°œì˜ ë©”íŠ¸ë¦­ ê²°ê³¼ë¥¼ í•©ì¹˜ê¸°\n",
    "    return dct1 | dct2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/tmp/ipykernel_113605/2082248890.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = transformers.Trainer(\n",
      "***** Running training *****\n",
      "  Num examples = 14,405\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2,703\n",
      "  Number of trainable parameters = 66,957,317\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2703' max='2703' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2703/2703 01:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.413400</td>\n",
       "      <td>0.959950</td>\n",
       "      <td>0.785876</td>\n",
       "      <td>0.786625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.285800</td>\n",
       "      <td>0.793307</td>\n",
       "      <td>0.788468</td>\n",
       "      <td>0.788806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.148400</td>\n",
       "      <td>0.852429</td>\n",
       "      <td>0.798348</td>\n",
       "      <td>0.798777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6174\n",
      "  Batch size = 16\n",
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.77k/6.77k [00:00<00:00, 14.6MB/s]\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/checkpoint-1500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1500/special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6174\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/checkpoint-2000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-2500\n",
      "Configuration saved in ./results/checkpoint-2500/config.json\n",
      "Model weights saved in ./results/checkpoint-2500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-2703\n",
      "Configuration saved in ./results/checkpoint-2703/config.json\n",
      "Model weights saved in ./results/checkpoint-2703/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-2703/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2703/special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6174\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2703, training_loss=0.21161614542002155, metrics={'train_runtime': 114.3206, 'train_samples_per_second': 378.016, 'train_steps_per_second': 23.644, 'total_flos': 1019315616068040.0, 'train_loss': 0.21161614542002155, 'epoch': 3.0})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Arguments ì„¤ì •\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer ì„¤ì •\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=covid19_tweets['train'],\n",
    "    eval_dataset=covid19_tweets['eval'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2)` (1)ì—ì„œ í•™ìŠµí•œ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ `covid19_tweets_df_test`ì˜ `OriginalTweet` ì—´ì— ëŒ€í•œ `Sentiment`ë¥¼ ì˜ˆì¸¡í•˜ì‹œì˜¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ë‹µì•ˆì˜ˆì‹œ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3804</td>\n",
       "      <td>48756</td>\n",
       "      <td>ÃƒÂœT: 36.319708,-82.363649</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>As news of the regionÃ‚Â’s first confirmed COVID...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3806</td>\n",
       "      <td>48758</td>\n",
       "      <td>Austria</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Was at the supermarket today. Didn't buy toile...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3808</td>\n",
       "      <td>48760</td>\n",
       "      <td>BHAVNAGAR,GUJRAT</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>For corona prevention,we should stop to buy th...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName                   Location     TweetAt  \\\n",
       "0      3800       48752                         UK  16-03-2020   \n",
       "1      3802       48754                        NaN  16-03-2020   \n",
       "2      3804       48756  ÃƒÂœT: 36.319708,-82.363649  16-03-2020   \n",
       "3      3806       48758                    Austria  16-03-2020   \n",
       "4      3808       48760           BHAVNAGAR,GUJRAT  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  advice Talk to your neighbours family to excha...            Positive  \n",
       "1  My food stock is not the only one which is emp...  Extremely Positive  \n",
       "2  As news of the regionÃ‚Â’s first confirmed COVID...            Positive  \n",
       "3  Was at the supermarket today. Didn't buy toile...             Neutral  \n",
       "4  For corona prevention,we should stop to buy th...            Negative  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid19_tweets_df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(í’€ì´)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(í’€ì´)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 20578\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 6.052478  , -0.25911158, -4.757802  , -1.4280325 , -1.1493124 ],\n",
       "       [ 2.3945487 , -1.8138888 , -4.4416656 ,  2.885043  , -1.0756998 ],\n",
       "       [ 5.9334354 , -1.897063  , -5.115348  ,  0.6702355 , -1.7291662 ],\n",
       "       ...,\n",
       "       [-1.9611454 , -3.7045465 , -3.4850702 ,  6.3820515 , -2.5484905 ],\n",
       "       [-2.0311882 , -1.2512956 , -0.4017841 , -3.9326744 ,  5.3791475 ],\n",
       "       [ 1.1640915 ,  2.671375  , -3.3171444 , -3.6160667 ,  1.4595261 ]],\n",
       "      dtype=float32), label_ids=None, metrics={'test_runtime': 12.0254, 'test_samples_per_second': 1711.21, 'test_steps_per_second': 107.023})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = trainer.predict(covid19_tweets['test'])\n",
    "out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid19_tweets_df_test['Sentiment'] = [id2label[i] for i in out.predictions.argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3804</td>\n",
       "      <td>48756</td>\n",
       "      <td>ÃƒÂœT: 36.319708,-82.363649</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>As news of the regionÃ‚Â’s first confirmed COVID...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3806</td>\n",
       "      <td>48758</td>\n",
       "      <td>Austria</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Was at the supermarket today. Didn't buy toile...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3808</td>\n",
       "      <td>48760</td>\n",
       "      <td>BHAVNAGAR,GUJRAT</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>For corona prevention,we should stop to buy th...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName                   Location     TweetAt  \\\n",
       "0      3800       48752                         UK  16-03-2020   \n",
       "1      3802       48754                        NaN  16-03-2020   \n",
       "2      3804       48756  ÃƒÂœT: 36.319708,-82.363649  16-03-2020   \n",
       "3      3806       48758                    Austria  16-03-2020   \n",
       "4      3808       48760           BHAVNAGAR,GUJRAT  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  advice Talk to your neighbours family to excha...            Positive  \n",
       "1  My food stock is not the only one which is emp...  Extremely Positive  \n",
       "2  As news of the regionÃ‚Â’s first confirmed COVID...            Positive  \n",
       "3  Was at the supermarket today. Didn't buy toile...             Neutral  \n",
       "4  For corona prevention,we should stop to buy th...            Negative  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid19_tweets_df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. IMDB -- ê°€ì‚°ì  50ì "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì•„ë˜ëŠ” 2ì£¼ì°¨ì—ì„œ ë°°ìš´ `imdb` ë¶„ì„ì½”ë“œë¥¼ ì •ë¦¬í•œ ê²ƒì´ë‹¤. ë¶„ì„ì˜ í¸ì˜ë¥¼ ìœ„í•˜ì—¬ 1000ê°œì˜ ìë£Œë§Œ ì‚¬ìš©í•˜ì—¬ í•™ìŠµê³¼ í‰ê°€ì— ì‚¬ìš©í–ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_116097/1501501835.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = transformers.Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 00:30, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.003095</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=126, training_loss=0.042853578688606384, metrics={'train_runtime': 30.9969, 'train_samples_per_second': 64.523, 'train_steps_per_second': 4.065, 'total_flos': 260867634212640.0, 'train_loss': 0.042853578688606384, 'epoch': 2.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step1 \n",
    "imdb = datasets.load_dataset('imdb')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") \n",
    "imdb_transformed = imdb.map(lambda dct: tokenizer(dct['text'],truncation=True), batched=True)\n",
    "## Step2 \n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=2)\n",
    "## Step3 \n",
    "def accuracy(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"my_awesome_model\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\"\n",
    "    ),\n",
    "    train_dataset=imdb_transformed['train'].select(range(1000)), # 1000ê°œë§Œ ì‚¬ìš©\n",
    "    eval_dataset=imdb_transformed['test'].select(range(1000)), # 1000ê°œë§Œ ì‚¬ìš©\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=transformers.DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=accuracy,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë³´ëŠ”ê²ƒì²˜ëŸ¼ í•™ìŠµì€ ì™„ë²½í•˜ê²Œ ë˜ì—ˆë‹¤. (accuracyê°€ 100%) ê·¸ë ‡ì§€ë§Œ `imdb_transformed['test']` ì „ì²´ì—ì„œ í‰ê°€ë¥¼ í•´ë³´ë‹ˆ ê²°ê³¼ê°€ ì¢‹ì§€ ì•Šì•˜ë‹¤. (accuracyê°€ 50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 3.2670784, -2.9905088],\n",
       "       [ 3.2648833, -3.0288398],\n",
       "       [ 3.224249 , -2.9409087],\n",
       "       ...,\n",
       "       [ 3.2539847, -3.0032833],\n",
       "       [ 3.2596397, -3.0228188],\n",
       "       [ 3.2206337, -3.0072105]], dtype=float32), label_ids=array([0, 0, 0, ..., 1, 1, 1]), metrics={'test_loss': 3.1024723052978516, 'test_accuracy': 0.5, 'test_runtime': 81.9806, 'test_samples_per_second': 304.95, 'test_steps_per_second': 19.065})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step4 \n",
    "out = trainer.predict(imdb_transformed['test'])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.metrics['test_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¬´ì—‡ì´ ë¬¸ì œì¼ê¹Œ? í˜¹ì‹œ ì•„ë˜ì˜ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì—¬ í•™ìŠµì˜ ì„±ëŠ¥ì„ ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆì„ê¹Œ? (ë‹¨ ë°ì´í„°ì˜ ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•œë‹¤) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"my_awesome_model\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\"\n",
    "    ),\n",
    "    train_dataset=imdb_transformed['train'].select(range(1000)), # 1000ê°œë§Œ ì‚¬ìš©\n",
    "    eval_dataset=imdb_transformed['test'].select(range(1000)), # 1000ê°œë§Œ ì‚¬ìš©\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=transformers.DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=accuracy,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(ë¬¸ì œì˜ì›ì¸)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` í•™ìŠµì— ì‚¬ìš©ëœ ë°ì´í„°ë¥¼ ì‚´í´ë³´ì. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(imdb_transformed['train'].select(range(1000))['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `imdb_transformed['train'].select(range(1000))` ì— ë¶€ì •ì ì¸ ì˜í™”í‰ê°€ì— í•´ë‹¹í•˜ëŠ” ë¼ë²¨ë§Œ ì¡´ì¬í•¨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "ì°¸ê³ ë¡œ `imdb_transformed['train']`ìë£ŒëŠ” ì²˜ìŒ 12500ëŠ” ëª¨ë‘ ë¶€ì •í‰ê°€ë§Œ, ë‹¤ìŒ 12500ì€ ëª¨ë‘ ê¸ì •í‰ê°€ë§Œ ìˆë‹¤. ì¦‰ `imdb_transformed['train']`ìë£ŒëŠ” \"ì •ë ¬\"ë˜ì–´ ìˆë‹¤.\n",
    "\n",
    "*ì²˜ìŒ 12500*ê°œì˜ ë¼ë²¨\n",
    "```Python\n",
    "set(imdb_transformed['train'].select(range(12500))['label'])\n",
    "```\n",
    "```\n",
    "{0} \n",
    "```\n",
    "\n",
    "*ë‹¤ìŒ 12500*ê°œì˜ ë¼ë²¨\n",
    "```Python\n",
    "set(imdb_transformed['train'].select(range(12500,25000))['label'])\n",
    "```\n",
    "```\n",
    "{1}\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` ë”°ë¼ì„œ `imdb_transformed['train'].select(range(1000))`ì´ ë°ì´í„°ë¡œë§Œ í•™ìŠµí•œë‹¤ë©´ ì¸ê³µì§€ëŠ¥ì˜ ì…ì¥ì—ì„œëŠ” \n",
    "\n",
    "> \"ë°ì´í„° ìƒê´€ì—†ì´ 0ì´ë¼ê³ ë§Œ ëŒ€ë‹µí•˜ë©´ ë˜ì–ì•„?\"\n",
    "\n",
    "ë¼ê³  ì¸ì‹í•  ìˆ˜ ìˆë‹¤. ì‹¤ì œë¡œ ì¸ê³µì§€ëŠ¥ì€ `imdb_transformed['test']`ì— ëŒ€ì‘í•˜ëŠ” ëª¨ë“  ì˜ˆì¸¡ê°’ì„ í•­ìƒ 0ìœ¼ë¡œë§Œ ì˜ˆì¸¡í•œë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(out.predictions.argmax(axis=1)) # ëª¨ë“ ì˜ˆì¸¡ê°’ì€ 0ì´ë‹¤. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` ì‹¤ì œë¡œëŠ” `imdb_transformed`ì— 0ê³¼ 1ì€ ë”± ì ˆë°˜ì´ ìˆìœ¼ë¯€ë¡œ, accuracyëŠ” 50%ê°€ ë‚˜ì˜¨ë‹¤. (ì¸ê³µì§€ëŠ¥ì€ í•­ìƒ 0ì´ë¼ê³  ì˜ˆì¸¡) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 12500, 1: 12500}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{i:imdb_transformed['test']['label'].count(i) for i in set(imdb_transformed['test']['label'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ìƒê°í•´ë³´ë‹ˆê¹Œ.. trainê³¼ì •ì—ì„œ accuracyê°€ í•­ìƒ 100%ê°€ ë‚˜ì˜¨ì´ìœ ë„ ë‹¹ì—°í•˜ë‹¤.\n",
    "- `imdb_transformed['test'].select(range(1000))` ì—¬ê¸°ì—ëŠ” `label = 0` ì— í•´ë‹¹í•˜ëŠ” ìë£Œë§Œ ìˆì„í…Œë‹ˆê¹Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(í•´ê²°ì±…)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„°ë¥¼ ì„ìœ¼ë©´ ëœë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(imdb_transformed['train'].shuffle().select(range(1000))['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_116097/3188304965.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = transformers.Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 00:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.441965</td>\n",
       "      <td>0.826000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.345760</td>\n",
       "      <td>0.853000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=126, training_loss=0.48426180037241134, metrics={'train_runtime': 31.3854, 'train_samples_per_second': 63.724, 'train_steps_per_second': 4.015, 'total_flos': 262388939494080.0, 'train_loss': 0.48426180037241134, 'epoch': 2.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step1 \n",
    "imdb = datasets.load_dataset('imdb')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") \n",
    "imdb_transformed = imdb.map(lambda dct: tokenizer(dct['text'],truncation=True), batched=True)\n",
    "## Step2 \n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=2)\n",
    "## Step3 \n",
    "def accuracy(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"my_awesome_model\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\"\n",
    "    ),\n",
    "    train_dataset=imdb_transformed['train'].shuffle().select(range(1000)), # 1000ê°œë§Œ ì‚¬ìš©\n",
    "    eval_dataset=imdb_transformed['test'].shuffle().select(range(1000)), # 1000ê°œë§Œ ì‚¬ìš©\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=transformers.DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=accuracy,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.88408"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step4 \n",
    "out = trainer.predict(imdb_transformed['test'])\n",
    "out.metrics['test_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ê²Œ ì •ìƒì ì¸ í•™ìŠµ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
