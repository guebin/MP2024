{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4788ea4b-e26a-4507-89cd-70d6d7a14363",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"**Quiz-9 (2024.12.03)**  // 범위: 11wk-1 까지\"\n",
    "author: \"최규빈\"\n",
    "date: \"12/03/2024\"\n",
    "comments: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c33e7ae-d5cb-410c-8ce1-a338f438b6a0",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/guebin/MP2024/blob/main/quiz/Quiz-9.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ada7b-aef4-4262-a830-d3917a1e292d",
   "metadata": {},
   "source": [
    "| **항목**               | **허용 여부**        | **비고**                                          |\n",
    "|------------------------|----------------------|---------------------------------------------------|\n",
    "| **강의노트 참고**      | 허용                 | 수업 중 제공된 강의노트나 본인이 정리한 자료를 참고 가능       |\n",
    "| **구글 검색**          | 허용                 | 인터넷을 통한 자료 검색 및 정보 확인 가능        |\n",
    "| **생성 모형 사용**           | 허용           | 인공지능 기반 도구(GPT 등) 사용 가능            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1465c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c527018b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets \n",
    "import transformers\n",
    "import torch\n",
    "import torch.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e63e81d",
   "metadata": {},
   "source": [
    "# 1. `sms_spam` -- 40점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635235a",
   "metadata": {},
   "source": [
    "아래의 코드를 실행하여 `model`, `tokenizer`, `spam` 을 불러오라. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10ca1bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sms', 'label'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sms', 'label'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "spam = datasets.load_dataset('guebin/spam-tiny')['train'].train_test_split(test_size=0.2)\n",
    "spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaecd71",
   "metadata": {},
   "source": [
    "`???`에 적절한 `data_collator` 혹은 `collate_fn` 을 설계하여 아래의 코드를 완성하라."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb36520",
   "metadata": {},
   "source": [
    "```Python\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    data_collator=???,\n",
    "    train_dataset=spam['train'],\n",
    "    args = transformers.TrainingArguments(\n",
    "        output_dir=\"asdf\",\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    ")\n",
    "trainer.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9c66c-b9ac-488b-87b1-80ae1c7f00c2",
   "metadata": {},
   "source": [
    "`(풀이)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1820f9d-eb1e-4c1e-9af1-0e63f3c1b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(single_batch):\n",
    "    out = tokenizer(\n",
    "        [dct['sms'] for dct in single_batch],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    out['labels'] = torch.tensor([dct['label'] for dct in single_batch])\n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09d035f9-c482-47a2-a3a0-02959844e5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.5972510576248169, metrics={'train_runtime': 1.2616, 'train_samples_per_second': 19.023, 'train_steps_per_second': 2.378, 'total_flos': 347726921472.0, 'train_loss': 0.5972510576248169, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=spam['train'],\n",
    "    eval_dataset=spam['test'],\n",
    "    args = transformers.TrainingArguments(\n",
    "        output_dir=\"asdf\",\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fdd364",
   "metadata": {},
   "source": [
    "# 2. `Food101` -- 60점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58ef3f",
   "metadata": {},
   "source": [
    "아래의 코드를 실행하여 `model`, `image_processor`, `food` 를 불러오라. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74e13dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=101\n",
    ")\n",
    "image_processor = transformers.AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "food = datasets.load_dataset(\"guebin/food101-tiny\")['train'].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b6460",
   "metadata": {},
   "source": [
    "아래의 변환을 적용하라. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e25bde15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_trans(examples):\n",
    "    return examples | image_processor(examples['image'])\n",
    "food = food.with_transform(w_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5281126",
   "metadata": {},
   "source": [
    "`???`에 적절한 `data_collator` 혹은 `collate_fn` 을 설계하여 아래의 코드를 완성하라."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347fdead",
   "metadata": {},
   "source": [
    "```Python\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    data_collator=???,\n",
    "    train_dataset=food['train'],\n",
    "    args = transformers.TrainingArguments(\n",
    "        output_dir=\"asdf\",\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    ")\n",
    "trainer.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11856a9-54ba-4aed-91de-8a1d6dff28e7",
   "metadata": {},
   "source": [
    "`(풀이)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b328c2d-bf20-4676-a112-50de27fba3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(single_batch):\n",
    "    dct = dict()\n",
    "    dct['pixel_values'] = torch.tensor(np.stack([o['pixel_values'] for o in single_batch]))\n",
    "    dct['labels'] = torch.tensor([o['label'] for o in single_batch])\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a03d1043-5024-4569-bbc6-650b6d0d6e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=4.46260134379069, metrics={'train_runtime': 1.4387, 'train_samples_per_second': 16.682, 'train_steps_per_second': 2.085, 'total_flos': 1861457968742400.0, 'train_loss': 4.46260134379069, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=food['train'],\n",
    "    eval_dataset=food['test'],\n",
    "    args = transformers.TrainingArguments(\n",
    "        output_dir=\"asdf\",\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
