{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"09wk-2: `model`의 입력파악, `model`의 사용연습\"\n",
    "author: \"최규빈\"\n",
    "date: \"11/09/2024\"\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/guebin/MP2024/blob/main/posts/09wk-2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 강의영상 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{<video https://youtu.be/playlist?list=PLQqh36zP38-xsRgsXpLEUNWClxLi0N9Mk&si=Lssxzw70RRT1adiJ >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "import torch\n",
    "import torchvision\n",
    "import pytorchvideo.data\n",
    "import PIL\n",
    "import tarfile\n",
    "import mp2024pkg as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model 입력파악"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 아래중 하나의 방법순으로.. \n",
    "\n",
    "- 방법1: `model.forward?` 에서 시그니처를 확인\n",
    "- 방법2: `model.forward?` 에서 사용예제를 확인\n",
    "- 방법3: 인터넷을 활용한 외부 자료 확인 (공식문서, 공식튜토리얼, 신뢰할만한 블로그, ChatGPT등)\n",
    "- 방법4: `model.forward??` 를 보고 모든 코드를 뜯어봄 <--- 하지마세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 모델의 입력이 어떤형태로 정리되어야 하는지 알아내는 확실한 방법은 없음 \n",
    "\n",
    "- 방법1,2,3 은 다른사람의 호의에 기대해야함.\n",
    "- 방법4는 사실상 불가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제1` -- 텍스트분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model1 = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*모델의 기본정보(config)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"distilbert/distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.46.2\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- max_position_embeddings: 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*모델의 입력파악*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mattention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhead_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minputs_embeds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequenceClassifierOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "The [`DistilBertForSequenceClassification`] forward method, overrides the `__call__` special method.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "the latter silently ignores them.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "Args:\n",
      "    input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "        Indices of input sequence tokens in the vocabulary.\n",
      "\n",
      "        Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "        [`PreTrainedTokenizer.__call__`] for details.\n",
      "\n",
      "        [What are input IDs?](../glossary#input-ids)\n",
      "    attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "        Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
      "\n",
      "        - 1 for tokens that are **not masked**,\n",
      "        - 0 for tokens that are **masked**.\n",
      "\n",
      "        [What are attention masks?](../glossary#attention-mask)\n",
      "    head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
      "        Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
      "\n",
      "        - 1 indicates the head is **not masked**,\n",
      "        - 0 indicates the head is **masked**.\n",
      "\n",
      "    inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
      "        Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
      "        is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
      "        model's internal embedding lookup matrix.\n",
      "    output_attentions (`bool`, *optional*):\n",
      "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "        tensors for more detail.\n",
      "    output_hidden_states (`bool`, *optional*):\n",
      "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "        more detail.\n",
      "    return_dict (`bool`, *optional*):\n",
      "        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "\n",
      "    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
      "        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
      "        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
      "        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
      "    \n",
      "Returns:\n",
      "    [`transformers.modeling_outputs.SequenceClassifierOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.SequenceClassifierOutput`] or a tuple of\n",
      "    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      "    elements depending on the configuration ([`DistilBertConfig`]) and inputs.\n",
      "\n",
      "    - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Classification (or regression if config.num_labels==1) loss.\n",
      "    - **logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) -- Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
      "    - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "      one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "\n",
      "      Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
      "    - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "      sequence_length)`.\n",
      "\n",
      "      Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
      "      heads.\n",
      "\n",
      "Example of single-label classification:\n",
      "\n",
      "```python\n",
      ">>> import torch\n",
      ">>> from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
      "\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
      ">>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
      "\n",
      ">>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      "\n",
      ">>> with torch.no_grad():\n",
      "...     logits = model(**inputs).logits\n",
      "\n",
      ">>> predicted_class_id = logits.argmax().item()\n",
      "\n",
      ">>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
      ">>> num_labels = len(model.config.id2label)\n",
      ">>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
      "\n",
      ">>> labels = torch.tensor([1])\n",
      ">>> loss = model(**inputs, labels=labels).loss\n",
      "```\n",
      "\n",
      "Example of multi-label classification:\n",
      "\n",
      "```python\n",
      ">>> import torch\n",
      ">>> from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
      "\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
      ">>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", problem_type=\"multi_label_classification\")\n",
      "\n",
      ">>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      "\n",
      ">>> with torch.no_grad():\n",
      "...     logits = model(**inputs).logits\n",
      "\n",
      ">>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
      "\n",
      ">>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
      ">>> num_labels = len(model.config.id2label)\n",
      ">>> model = DistilBertForSequenceClassification.from_pretrained(\n",
      "...     \"distilbert-base-uncased\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
      "... )\n",
      "\n",
      ">>> labels = torch.sum(\n",
      "...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n",
      "... ).to(torch.float)\n",
      ">>> loss = model(**inputs, labels=labels).loss\n",
      "```\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "model1.forward? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시1 -- 입력나열, loss O*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6543, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0386, -0.1142],\n",
       "        [-0.0372, -0.1201]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(\n",
    "    input_ids = torch.tensor([[1,2,3,4], [2,3,4,5]]),\n",
    "    labels = torch.tensor([0,0])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시2 -- `**딕셔너리`, loss O*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6543, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0386, -0.1142],\n",
       "        [-0.0372, -0.1201]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_input = dict(\n",
    "    input_ids = torch.tensor([[1,2,3,4], [2,3,4,5]]),\n",
    "    labels = torch.tensor([0,0])\n",
    ")\n",
    "model1(**model1_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시3 -- 입력나열, loss X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0386, -0.1142],\n",
       "        [-0.0372, -0.1201]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(\n",
    "    input_ids = torch.tensor([[1,2,3,4], [2,3,4,5]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시4 -- `**딕셔너리`, loss X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0386, -0.1142],\n",
       "        [-0.0372, -0.1201]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_input = dict(\n",
    "    input_ids = torch.tensor([[1,2,3,4], [2,3,4,5]])\n",
    ")\n",
    "model1(**model1_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시5 -- 초간단, loss X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0386, -0.1142],\n",
       "        [-0.0372, -0.1201]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(torch.tensor([[1,2,3,4], [2,3,4,5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 사용예시1~5에서 `model1()` 대신에 `model1.forward()`를 사용해도 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제2` -- 이미지분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model2 = transformers.AutoModelForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=3 # 그냥 대충 3이라고 했음.. 별 이유는 없음\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*모델의 기본정보(config)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
       "  \"architectures\": [\n",
       "    \"ViTModel\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"encoder_stride\": 16,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"vit\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"qkv_bias\": true,\n",
       "  \"transformers_version\": \"4.46.2\"\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- image_size: 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear</th>\n",
       "      <th>classifier</th>\n",
       "      <td>Applies an affine linear transformation to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">NoneType</th>\n",
       "      <th>generation_config</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_tags</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TypeVar</th>\n",
       "      <th>T_destination</th>\n",
       "      <td>Type variable.\\n\\nThe preferred way to constru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ViTConfig</th>\n",
       "      <th>config</th>\n",
       "      <td>This is the configuration class to store the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ViTModel</th>\n",
       "      <th>base_model</th>\n",
       "      <td>The bare ViT Model transformer outputting raw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vit</th>\n",
       "      <td>The bare ViT Model transformer outputting raw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">bool</th>\n",
       "      <th>call_super_init</th>\n",
       "      <td>bool(x) -&gt; bool\\n\\nReturns True when the argum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dump_patches</th>\n",
       "      <td>bool(x) -&gt; bool\\n\\nReturns True when the argum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_gradient_checkpointing</th>\n",
       "      <td>bool(x) -&gt; bool\\n\\nReturns True when the argum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_parallelizable</th>\n",
       "      <td>bool(x) -&gt; bool\\n\\nReturns True when the argum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supports_gradient_checkpointing</th>\n",
       "      <td>bool(x) -&gt; bool\\n\\nReturns True when the argum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>training</th>\n",
       "      <td>bool(x) -&gt; bool\\n\\nReturns True when the argum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>device</th>\n",
       "      <th>device</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">dict</th>\n",
       "      <th>dummy_inputs</th>\n",
       "      <td>dict() -&gt; new empty dictionary\\ndict(mapping) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warnings_issued</th>\n",
       "      <td>dict() -&gt; new empty dictionary\\ndict(mapping) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dtype</th>\n",
       "      <th>dtype</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">function</th>\n",
       "      <th>create_extended_attention_mask_for_decoder</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss_function</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int</th>\n",
       "      <th>num_labels</th>\n",
       "      <td>int([x]) -&gt; integer\\nint(x, base=10) -&gt; intege...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"92\" valign=\"top\">method</th>\n",
       "      <th>active_adapters</th>\n",
       "      <td>If you are not familiar with adapters and PEFT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>add_adapter</th>\n",
       "      <td>If you are not familiar with adapters and PEFT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>add_memory_hooks</th>\n",
       "      <td>Add a memory hook before and after each sub-mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>add_model_tags</th>\n",
       "      <td>Add custom tags into the model that gets pushe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>add_module</th>\n",
       "      <td>Add a child module to the current module.\\n\\nT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apply</th>\n",
       "      <td>Apply ``fn`` recursively to every submodule (a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bfloat16</th>\n",
       "      <td>Casts all floating point parameters and buffer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buffers</th>\n",
       "      <td>Return an iterator over module buffers.\\n\\nArg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can_generate</th>\n",
       "      <td>Returns whether this model can generate sequen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>children</th>\n",
       "      <td>Return an iterator over immediate children mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compile</th>\n",
       "      <td>Compile this Module's forward using :func:`tor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compute_transition_scores</th>\n",
       "      <td>Computes the transition scores of sequences gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu</th>\n",
       "      <td>Move all model parameters and buffers to the C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cuda</th>\n",
       "      <td>Move all model parameters and buffers to the G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dequantize</th>\n",
       "      <td>Potentially dequantize the model in case it ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disable_adapters</th>\n",
       "      <td>If you are not familiar with adapters and PEFT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disable_input_require_grads</th>\n",
       "      <td>Removes the `_require_grads_hook`.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>double</th>\n",
       "      <td>Casts all floating point parameters and buffer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enable_adapters</th>\n",
       "      <td>If you are not familiar with adapters and PEFT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enable_input_require_grads</th>\n",
       "      <td>Enables the gradients for the input embeddings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estimate_tokens</th>\n",
       "      <td>Helper function to estimate the total number o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval</th>\n",
       "      <td>Set the module in evaluation mode.\\n\\nThis has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extra_repr</th>\n",
       "      <td>Set the extra representation of the module.\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>float</th>\n",
       "      <td>Casts all floating point parameters and buffer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>floating_point_ops</th>\n",
       "      <td>Get number of (optionally, non-embeddings) flo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forward</th>\n",
       "      <td>The [`ViTForImageClassification`] forward meth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_pretrained</th>\n",
       "      <td>Instantiate a pretrained pytorch model from a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generate</th>\n",
       "      <td>Generates sequences of token ids for models wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get_adapter_state_dict</th>\n",
       "      <td>If you are not familiar with adapters and PEFT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get_buffer</th>\n",
       "      <td>Return the buffer given by ``target`` if it ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get_extended_attention_mask</th>\n",
       "      <td>Makes broadcastable attention and causal masks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get_extra_state</th>\n",
       "      <td>Return any extra state to include in the modul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get_head_mask</th>\n",
       "      <td>Prepare the head mask if needed.\\n\\nArgs:\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get_input_embeddings</th>\n",
       "      <td>Returns the model's input embeddings.\\n\\nRetur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get_memory_footprint</th>\n",
       "      <td>Get the memory footprint of a model. This will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get_output_embeddings</th>\n",
       "      <td>Returns the model's output embeddings.\\n\\nRetu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get_parameter</th>\n",
       "      <td>Return the parameter given by ``target`` if it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get_submodule</th>\n",
       "      <td>Return the submodule given by ``target`` if it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient_checkpointing_disable</th>\n",
       "      <td>Deactivates gradient checkpointing for the cur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient_checkpointing_enable</th>\n",
       "      <td>Activates gradient checkpointing for the curre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>half</th>\n",
       "      <td>Casts all floating point parameters and buffer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heal_tokens</th>\n",
       "      <td>Generates sequences of token ids for models wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>init_weights</th>\n",
       "      <td>If needed prunes and maybe initializes weights...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invert_attention_mask</th>\n",
       "      <td>Invert an attention mask (e.g., switches 0. an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ipu</th>\n",
       "      <td>Move all model parameters and buffers to the I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>load_adapter</th>\n",
       "      <td>Load adapter weights from file or remote Hub f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>load_state_dict</th>\n",
       "      <td>Copy parameters and buffers from :attr:`state_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modules</th>\n",
       "      <td>Return an iterator over all modules in the net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>named_buffers</th>\n",
       "      <td>Return an iterator over module buffers, yieldi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>named_children</th>\n",
       "      <td>Return an iterator over immediate children mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>named_modules</th>\n",
       "      <td>Return an iterator over all modules in the net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>named_parameters</th>\n",
       "      <td>Return an iterator over module parameters, yie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_parameters</th>\n",
       "      <td>Get number of (optionally, trainable or non-em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parameters</th>\n",
       "      <td>Return an iterator over module parameters.\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post_init</th>\n",
       "      <td>A method executed at the end of each Transform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prepare_inputs_for_generation</th>\n",
       "      <td>Prepare the model inputs for generation. In in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prune_heads</th>\n",
       "      <td>Prunes heads of the base model.\\n\\nArguments:\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>push_to_hub</th>\n",
       "      <td>Upload the model file to the 🤗 Model Hub.\\n\\nP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>register_backward_hook</th>\n",
       "      <td>Register a backward hook on the module.\\n\\nThi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>register_buffer</th>\n",
       "      <td>Add a buffer to the module.\\n\\nThis is typical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>register_for_auto_class</th>\n",
       "      <td>Register this class with a given auto class. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>register_forward_hook</th>\n",
       "      <td>Register a forward hook on the module.\\n\\nThe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>register_forward_pre_hook</th>\n",
       "      <td>Register a forward pre-hook on the module.\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>register_full_backward_hook</th>\n",
       "      <td>Register a backward hook on the module.\\n\\nThe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>register_full_backward_pre_hook</th>\n",
       "      <td>Register a backward pre-hook on the module.\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>register_load_state_dict_post_hook</th>\n",
       "      <td>Register a post hook to be run after module's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>register_module</th>\n",
       "      <td>Alias for :func:`add_module`.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>register_parameter</th>\n",
       "      <td>Add a parameter to the module.\\n\\nThe paramete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>register_state_dict_pre_hook</th>\n",
       "      <td>Register a pre-hook for the :meth:`~torch.nn.M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>requires_grad_</th>\n",
       "      <td>Change if autograd should record operations on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reset_memory_hooks_state</th>\n",
       "      <td>Reset the `mem_rss_diff` attribute of each mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resize_token_embeddings</th>\n",
       "      <td>Resizes input token embeddings matrix of the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reverse_bettertransformer</th>\n",
       "      <td>Reverts the transformation from [`~PreTrainedM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>save_pretrained</th>\n",
       "      <td>Save a model and its configuration file to a d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set_adapter</th>\n",
       "      <td>If you are not familiar with adapters and PEFT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set_extra_state</th>\n",
       "      <td>Set extra state contained in the loaded `state...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set_input_embeddings</th>\n",
       "      <td>Set model's input embeddings.\\n\\nArgs:\\n    va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>share_memory</th>\n",
       "      <td>See :meth:`torch.Tensor.share_memory_`.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_dict</th>\n",
       "      <td>Return a dictionary containing references to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tie_weights</th>\n",
       "      <td>Tie the weights between the input embeddings a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>Move and/or cast the parameters and buffers.\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to_bettertransformer</th>\n",
       "      <td>Converts the model to use [PyTorch's native at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to_empty</th>\n",
       "      <td>Move the parameters and buffers to the specifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>Set the module in training mode.\\n\\nThis has a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>Casts all parameters and buffers to :attr:`dst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warn_if_padding_and_no_attention_mask</th>\n",
       "      <td>Shows a one-time warning if the input_ids appe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xpu</th>\n",
       "      <td>Move all model parameters and buffers to the X...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero_grad</th>\n",
       "      <td>Reset gradients of all model parameters.\\n\\nSe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>active_adapter</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get_position_embeddings</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resize_position_embeddings</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retrieve_modules_from_names</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">str</th>\n",
       "      <th>base_model_prefix</th>\n",
       "      <td>str(object='') -&gt; str\\nstr(bytes_or_buffer[, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>framework</th>\n",
       "      <td>str(object='') -&gt; str\\nstr(bytes_or_buffer[, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_input_name</th>\n",
       "      <td>str(object='') -&gt; str\\nstr(bytes_or_buffer[, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name_or_path</th>\n",
       "      <td>str(object='') -&gt; str\\nstr(bytes_or_buffer[, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th>config_class</th>\n",
       "      <td>This is the configuration class to store the c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                            Description\n",
       "Type      Name                                                                                         \n",
       "Linear    classifier                                  Applies an affine linear transformation to the...\n",
       "NoneType  generation_config                                                                        None\n",
       "          model_tags                                                                               None\n",
       "TypeVar   T_destination                               Type variable.\\n\\nThe preferred way to constru...\n",
       "ViTConfig config                                      This is the configuration class to store the c...\n",
       "ViTModel  base_model                                  The bare ViT Model transformer outputting raw ...\n",
       "          vit                                         The bare ViT Model transformer outputting raw ...\n",
       "bool      call_super_init                             bool(x) -> bool\\n\\nReturns True when the argum...\n",
       "          dump_patches                                bool(x) -> bool\\n\\nReturns True when the argum...\n",
       "          is_gradient_checkpointing                   bool(x) -> bool\\n\\nReturns True when the argum...\n",
       "          is_parallelizable                           bool(x) -> bool\\n\\nReturns True when the argum...\n",
       "          supports_gradient_checkpointing             bool(x) -> bool\\n\\nReturns True when the argum...\n",
       "          training                                    bool(x) -> bool\\n\\nReturns True when the argum...\n",
       "device    device                                                                                   None\n",
       "dict      dummy_inputs                                dict() -> new empty dictionary\\ndict(mapping) ...\n",
       "          warnings_issued                             dict() -> new empty dictionary\\ndict(mapping) ...\n",
       "dtype     dtype                                                                                    None\n",
       "function  create_extended_attention_mask_for_decoder                                               None\n",
       "          loss_function                                                                            None\n",
       "int       num_labels                                  int([x]) -> integer\\nint(x, base=10) -> intege...\n",
       "method    active_adapters                             If you are not familiar with adapters and PEFT...\n",
       "          add_adapter                                 If you are not familiar with adapters and PEFT...\n",
       "          add_memory_hooks                            Add a memory hook before and after each sub-mo...\n",
       "          add_model_tags                              Add custom tags into the model that gets pushe...\n",
       "          add_module                                  Add a child module to the current module.\\n\\nT...\n",
       "          apply                                       Apply ``fn`` recursively to every submodule (a...\n",
       "          bfloat16                                    Casts all floating point parameters and buffer...\n",
       "          buffers                                     Return an iterator over module buffers.\\n\\nArg...\n",
       "          can_generate                                Returns whether this model can generate sequen...\n",
       "          children                                    Return an iterator over immediate children mod...\n",
       "          compile                                     Compile this Module's forward using :func:`tor...\n",
       "          compute_transition_scores                   Computes the transition scores of sequences gi...\n",
       "          cpu                                         Move all model parameters and buffers to the C...\n",
       "          cuda                                        Move all model parameters and buffers to the G...\n",
       "          dequantize                                  Potentially dequantize the model in case it ha...\n",
       "          disable_adapters                            If you are not familiar with adapters and PEFT...\n",
       "          disable_input_require_grads                                Removes the `_require_grads_hook`.\n",
       "          double                                      Casts all floating point parameters and buffer...\n",
       "          enable_adapters                             If you are not familiar with adapters and PEFT...\n",
       "          enable_input_require_grads                  Enables the gradients for the input embeddings...\n",
       "          estimate_tokens                             Helper function to estimate the total number o...\n",
       "          eval                                        Set the module in evaluation mode.\\n\\nThis has...\n",
       "          extra_repr                                  Set the extra representation of the module.\\n\\...\n",
       "          float                                       Casts all floating point parameters and buffer...\n",
       "          floating_point_ops                          Get number of (optionally, non-embeddings) flo...\n",
       "          forward                                     The [`ViTForImageClassification`] forward meth...\n",
       "          from_pretrained                             Instantiate a pretrained pytorch model from a ...\n",
       "          generate                                    Generates sequences of token ids for models wi...\n",
       "          get_adapter_state_dict                      If you are not familiar with adapters and PEFT...\n",
       "          get_buffer                                  Return the buffer given by ``target`` if it ex...\n",
       "          get_extended_attention_mask                 Makes broadcastable attention and causal masks...\n",
       "          get_extra_state                             Return any extra state to include in the modul...\n",
       "          get_head_mask                               Prepare the head mask if needed.\\n\\nArgs:\\n   ...\n",
       "          get_input_embeddings                        Returns the model's input embeddings.\\n\\nRetur...\n",
       "          get_memory_footprint                        Get the memory footprint of a model. This will...\n",
       "          get_output_embeddings                       Returns the model's output embeddings.\\n\\nRetu...\n",
       "          get_parameter                               Return the parameter given by ``target`` if it...\n",
       "          get_submodule                               Return the submodule given by ``target`` if it...\n",
       "          gradient_checkpointing_disable              Deactivates gradient checkpointing for the cur...\n",
       "          gradient_checkpointing_enable               Activates gradient checkpointing for the curre...\n",
       "          half                                        Casts all floating point parameters and buffer...\n",
       "          heal_tokens                                 Generates sequences of token ids for models wi...\n",
       "          init_weights                                If needed prunes and maybe initializes weights...\n",
       "          invert_attention_mask                       Invert an attention mask (e.g., switches 0. an...\n",
       "          ipu                                         Move all model parameters and buffers to the I...\n",
       "          load_adapter                                Load adapter weights from file or remote Hub f...\n",
       "          load_state_dict                             Copy parameters and buffers from :attr:`state_...\n",
       "          modules                                     Return an iterator over all modules in the net...\n",
       "          named_buffers                               Return an iterator over module buffers, yieldi...\n",
       "          named_children                              Return an iterator over immediate children mod...\n",
       "          named_modules                               Return an iterator over all modules in the net...\n",
       "          named_parameters                            Return an iterator over module parameters, yie...\n",
       "          num_parameters                              Get number of (optionally, trainable or non-em...\n",
       "          parameters                                  Return an iterator over module parameters.\\n\\n...\n",
       "          post_init                                   A method executed at the end of each Transform...\n",
       "          prepare_inputs_for_generation               Prepare the model inputs for generation. In in...\n",
       "          prune_heads                                 Prunes heads of the base model.\\n\\nArguments:\\...\n",
       "          push_to_hub                                 Upload the model file to the 🤗 Model Hub.\\n\\nP...\n",
       "          register_backward_hook                      Register a backward hook on the module.\\n\\nThi...\n",
       "          register_buffer                             Add a buffer to the module.\\n\\nThis is typical...\n",
       "          register_for_auto_class                     Register this class with a given auto class. T...\n",
       "          register_forward_hook                       Register a forward hook on the module.\\n\\nThe ...\n",
       "          register_forward_pre_hook                   Register a forward pre-hook on the module.\\n\\n...\n",
       "          register_full_backward_hook                 Register a backward hook on the module.\\n\\nThe...\n",
       "          register_full_backward_pre_hook             Register a backward pre-hook on the module.\\n\\...\n",
       "          register_load_state_dict_post_hook          Register a post hook to be run after module's ...\n",
       "          register_module                                                 Alias for :func:`add_module`.\n",
       "          register_parameter                          Add a parameter to the module.\\n\\nThe paramete...\n",
       "          register_state_dict_pre_hook                Register a pre-hook for the :meth:`~torch.nn.M...\n",
       "          requires_grad_                              Change if autograd should record operations on...\n",
       "          reset_memory_hooks_state                    Reset the `mem_rss_diff` attribute of each mod...\n",
       "          resize_token_embeddings                     Resizes input token embeddings matrix of the m...\n",
       "          reverse_bettertransformer                   Reverts the transformation from [`~PreTrainedM...\n",
       "          save_pretrained                             Save a model and its configuration file to a d...\n",
       "          set_adapter                                 If you are not familiar with adapters and PEFT...\n",
       "          set_extra_state                             Set extra state contained in the loaded `state...\n",
       "          set_input_embeddings                        Set model's input embeddings.\\n\\nArgs:\\n    va...\n",
       "          share_memory                                          See :meth:`torch.Tensor.share_memory_`.\n",
       "          state_dict                                  Return a dictionary containing references to t...\n",
       "          tie_weights                                 Tie the weights between the input embeddings a...\n",
       "          to                                          Move and/or cast the parameters and buffers.\\n...\n",
       "          to_bettertransformer                        Converts the model to use [PyTorch's native at...\n",
       "          to_empty                                    Move the parameters and buffers to the specifi...\n",
       "          train                                       Set the module in training mode.\\n\\nThis has a...\n",
       "          type                                        Casts all parameters and buffers to :attr:`dst...\n",
       "          warn_if_padding_and_no_attention_mask       Shows a one-time warning if the input_ids appe...\n",
       "          xpu                                         Move all model parameters and buffers to the X...\n",
       "          zero_grad                                   Reset gradients of all model parameters.\\n\\nSe...\n",
       "          active_adapter                                                                           None\n",
       "          get_position_embeddings                                                                  None\n",
       "          resize_position_embeddings                                                               None\n",
       "          retrieve_modules_from_names                                                              None\n",
       "str       base_model_prefix                           str(object='') -> str\\nstr(bytes_or_buffer[, e...\n",
       "          framework                                   str(object='') -> str\\nstr(bytes_or_buffer[, e...\n",
       "          main_input_name                             str(object='') -> str\\nstr(bytes_or_buffer[, e...\n",
       "          name_or_path                                str(object='') -> str\\nstr(bytes_or_buffer[, e...\n",
       "type      config_class                                This is the configuration class to store the c..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mp.tab(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.config.num_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*모델의 입력파악*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.5745e+00, -6.5144e-01,  1.3978e+00,  ...,  4.6716e-01,\n",
       "            1.0718e+00,  1.5891e+00],\n",
       "          [ 7.2066e-01, -1.2123e-01, -7.4478e-01,  ...,  2.1659e-01,\n",
       "           -1.5586e+00,  6.1032e-01],\n",
       "          [ 5.7600e-01,  1.2782e+00, -1.6226e+00,  ...,  5.7004e-01,\n",
       "           -9.3501e-01, -1.4276e+00],\n",
       "          ...,\n",
       "          [-6.1726e-01, -2.3000e+00,  1.0953e+00,  ...,  2.8432e-01,\n",
       "            1.4926e+00,  1.9293e-01],\n",
       "          [ 3.1345e-01, -1.6033e+00, -5.3968e-01,  ...,  3.2452e-01,\n",
       "            2.0841e-02, -5.9209e-01],\n",
       "          [ 1.5393e+00,  1.9803e+00,  3.8718e-01,  ..., -4.9967e-01,\n",
       "            3.2762e-01, -9.5884e-02]],\n",
       "\n",
       "         [[ 1.5481e-01, -2.1678e+00, -9.8535e-02,  ..., -8.5066e-01,\n",
       "           -1.6522e+00, -1.1021e+00],\n",
       "          [-6.1898e-02, -1.5264e+00, -4.2154e-01,  ..., -1.2401e+00,\n",
       "           -8.2305e-01, -9.2082e-01],\n",
       "          [-1.2751e+00, -6.3566e-01,  1.5400e+00,  ...,  9.1556e-01,\n",
       "            3.8755e-01,  1.2098e+00],\n",
       "          ...,\n",
       "          [ 3.1346e-01,  4.4149e-01, -2.5716e-01,  ..., -2.2885e-01,\n",
       "            3.9848e-01, -5.4158e-04],\n",
       "          [-1.2899e+00,  9.7720e-02, -9.6740e-01,  ..., -8.0419e-01,\n",
       "           -1.2512e-01, -8.4751e-01],\n",
       "          [ 2.2317e-01,  1.1580e+00, -1.7458e+00,  ..., -1.2634e+00,\n",
       "            1.0704e-01,  1.1791e+00]],\n",
       "\n",
       "         [[ 5.3254e-01,  6.3082e-01, -9.9093e-01,  ...,  6.4301e-01,\n",
       "           -7.2970e-01,  8.1526e-01],\n",
       "          [ 4.3581e-01,  1.6700e+00, -1.5576e+00,  ..., -6.1614e-01,\n",
       "            1.6733e-01, -6.0567e-01],\n",
       "          [ 3.6077e-02,  1.0616e+00, -1.2944e-01,  ..., -1.2316e+00,\n",
       "           -1.2281e+00, -5.6705e-01],\n",
       "          ...,\n",
       "          [-2.0337e+00, -1.2634e-01,  4.4070e-02,  ..., -5.7722e-02,\n",
       "            4.1883e-01, -9.8459e-01],\n",
       "          [ 2.1882e+00, -2.4888e-02,  5.3597e-02,  ..., -2.3750e+00,\n",
       "           -1.8387e-01, -1.6128e-01],\n",
       "          [-4.0576e-01, -1.8897e+00,  1.1782e+00,  ...,  5.1731e-01,\n",
       "            4.5297e-01, -5.3125e-01]]],\n",
       "\n",
       "\n",
       "        [[[-9.2747e-01, -7.5570e-01, -1.4661e+00,  ...,  1.1517e+00,\n",
       "           -2.2112e+00,  1.4106e+00],\n",
       "          [ 9.5970e-01,  6.6660e-01,  9.3574e-01,  ...,  1.5504e+00,\n",
       "           -1.1212e+00,  1.0728e+00],\n",
       "          [ 1.0182e+00, -1.3789e+00, -7.2933e-01,  ...,  5.9614e-01,\n",
       "           -1.2301e-01,  4.0164e-01],\n",
       "          ...,\n",
       "          [-1.0014e+00, -7.1850e-01, -7.8420e-01,  ..., -3.3162e-01,\n",
       "            4.8575e-01,  3.1998e-01],\n",
       "          [-1.7896e+00, -2.3898e+00, -8.0165e-01,  ..., -1.6916e+00,\n",
       "           -4.1522e-01,  1.4758e+00],\n",
       "          [ 5.5693e-01,  8.0012e-01,  9.9807e-01,  ...,  1.3519e+00,\n",
       "           -1.3475e+00,  2.3388e-01]],\n",
       "\n",
       "         [[-7.4014e-01,  3.4345e-01, -1.5668e+00,  ..., -3.0768e-02,\n",
       "           -1.7804e+00, -4.9629e-01],\n",
       "          [ 1.4107e+00,  8.5074e-01, -7.8545e-01,  ...,  7.8777e-02,\n",
       "            1.3570e+00,  1.1842e+00],\n",
       "          [ 5.8394e-01, -6.8746e-01,  5.0234e-01,  ..., -2.5901e-01,\n",
       "           -8.3004e-01, -6.7281e-01],\n",
       "          ...,\n",
       "          [-3.8982e-01, -2.0339e-01, -1.4185e+00,  ...,  7.1824e-01,\n",
       "            1.1272e-01, -5.5666e-01],\n",
       "          [-1.2153e+00,  1.1162e-01, -2.0197e+00,  ..., -4.1950e-01,\n",
       "           -7.8650e-01,  3.4739e-01],\n",
       "          [ 9.8186e-01, -2.8893e-01,  1.4978e+00,  ..., -7.9580e-01,\n",
       "            2.2957e-01, -1.0904e-01]],\n",
       "\n",
       "         [[ 1.9655e+00,  2.2175e-01, -1.1318e+00,  ..., -6.2302e-01,\n",
       "           -3.6953e-03,  7.1338e-01],\n",
       "          [-8.4720e-01, -6.9941e-01, -9.5270e-03,  ...,  2.9664e-01,\n",
       "            1.2582e+00,  3.9544e-01],\n",
       "          [-1.9157e-01,  1.9280e-01,  7.6348e-01,  ...,  2.1723e-01,\n",
       "           -1.3277e-01, -4.8275e-01],\n",
       "          ...,\n",
       "          [-2.0642e+00, -5.2406e-01, -1.1908e+00,  ...,  1.7075e+00,\n",
       "           -9.8756e-01,  5.1894e-02],\n",
       "          [-1.4027e+00, -1.7799e-01, -3.1410e-01,  ...,  2.6471e-01,\n",
       "            2.2696e-01,  1.2731e+00],\n",
       "          [ 4.0601e-01, -7.4519e-02, -6.2866e-02,  ..., -3.4719e-01,\n",
       "            1.6628e+00, -5.0701e-01]]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2,3,64,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시1 -- 입력나열, loss O*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=tensor(1.0646, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0740, -0.0991, -0.0717],\n",
       "        [ 0.0718, -0.0768, -0.1305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "model2(\n",
    "    pixel_values = torch.randn(2,3,224,224),\n",
    "    labels = torch.tensor([0,1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시2 -- `**딕셔너리`, loss O*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=tensor(1.0646, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0740, -0.0991, -0.0717],\n",
       "        [ 0.0718, -0.0768, -0.1305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "model2_input = dict(\n",
    "    pixel_values = torch.randn(2,3,224,224),\n",
    "    labels = torch.tensor([0,1])\n",
    ")\n",
    "model2(**model2_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시3 -- 입력나열, loss X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[ 0.0740, -0.0991, -0.0717],\n",
       "        [ 0.0718, -0.0768, -0.1305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "model2(\n",
    "    pixel_values = torch.randn(2,3,224,224)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시4 -- `**딕셔너리`, loss X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[ 0.0740, -0.0991, -0.0717],\n",
       "        [ 0.0718, -0.0768, -0.1305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "model2_input = dict(\n",
    "    pixel_values = torch.randn(2,3,224,224),\n",
    ")\n",
    "model2(**model2_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시5 -- 초간단, loss X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[ 0.0740, -0.0991, -0.0717],\n",
       "        [ 0.0718, -0.0768, -0.1305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "model2(torch.randn(2,3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제3` -- 동영상분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model3 = transformers.VideoMAEForVideoClassification.from_pretrained(\n",
    "    \"MCG-NJU/videomae-base\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*모델의 기본정보(config)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoMAEConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"MCG-NJU/videomae-base\",\n",
       "  \"architectures\": [\n",
       "    \"VideoMAEForPreTraining\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"decoder_hidden_size\": 384,\n",
       "  \"decoder_intermediate_size\": 1536,\n",
       "  \"decoder_num_attention_heads\": 6,\n",
       "  \"decoder_num_hidden_layers\": 4,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"videomae\",\n",
       "  \"norm_pix_loss\": true,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_frames\": 16,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"qkv_bias\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.46.2\",\n",
       "  \"tubelet_size\": 2,\n",
       "  \"use_mean_pooling\": false\n",
       "}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- image_size: 224\n",
    "- num_frames: 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*모델의 입력파악*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpixel_values\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhead_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageClassifierOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "The [`VideoMAEForVideoClassification`] forward method, overrides the `__call__` special method.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "the latter silently ignores them.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "Args:\n",
      "    pixel_values (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, height, width)`):\n",
      "        Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n",
      "        [`VideoMAEImageProcessor.__call__`] for details.\n",
      "\n",
      "    head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
      "        Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
      "\n",
      "        - 1 indicates the head is **not masked**,\n",
      "        - 0 indicates the head is **masked**.\n",
      "\n",
      "    output_attentions (`bool`, *optional*):\n",
      "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "        tensors for more detail.\n",
      "    output_hidden_states (`bool`, *optional*):\n",
      "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "        more detail.\n",
      "    return_dict (`bool`, *optional*):\n",
      "        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "\n",
      "    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
      "        Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n",
      "        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
      "        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
      "\n",
      "\n",
      "    Returns:\n",
      "        [`transformers.modeling_outputs.ImageClassifierOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.ImageClassifierOutput`] or a tuple of\n",
      "        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      "        elements depending on the configuration ([`VideoMAEConfig`]) and inputs.\n",
      "\n",
      "        - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Classification (or regression if config.num_labels==1) loss.\n",
      "        - **logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) -- Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
      "        - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "          one for the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states\n",
      "          (also called feature maps) of the model at the output of each stage.\n",
      "        - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, patch_size,\n",
      "          sequence_length)`.\n",
      "\n",
      "          Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
      "          heads.\n",
      "  \n",
      "\n",
      "    Examples:\n",
      "\n",
      "    ```python\n",
      "    >>> import av\n",
      "    >>> import torch\n",
      "    >>> import numpy as np\n",
      "\n",
      "    >>> from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n",
      "    >>> from huggingface_hub import hf_hub_download\n",
      "\n",
      "    >>> np.random.seed(0)\n",
      "\n",
      "\n",
      "    >>> def read_video_pyav(container, indices):\n",
      "    ...     '''\n",
      "    ...     Decode the video with PyAV decoder.\n",
      "    ...     Args:\n",
      "    ...         container (`av.container.input.InputContainer`): PyAV container.\n",
      "    ...         indices (`List[int]`): List of frame indices to decode.\n",
      "    ...     Returns:\n",
      "    ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
      "    ...     '''\n",
      "    ...     frames = []\n",
      "    ...     container.seek(0)\n",
      "    ...     start_index = indices[0]\n",
      "    ...     end_index = indices[-1]\n",
      "    ...     for i, frame in enumerate(container.decode(video=0)):\n",
      "    ...         if i > end_index:\n",
      "    ...             break\n",
      "    ...         if i >= start_index and i in indices:\n",
      "    ...             frames.append(frame)\n",
      "    ...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
      "\n",
      "\n",
      "    >>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
      "    ...     '''\n",
      "    ...     Sample a given number of frame indices from the video.\n",
      "    ...     Args:\n",
      "    ...         clip_len (`int`): Total number of frames to sample.\n",
      "    ...         frame_sample_rate (`int`): Sample every n-th frame.\n",
      "    ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n",
      "    ...     Returns:\n",
      "    ...         indices (`List[int]`): List of sampled frame indices\n",
      "    ...     '''\n",
      "    ...     converted_len = int(clip_len * frame_sample_rate)\n",
      "    ...     end_idx = np.random.randint(converted_len, seg_len)\n",
      "    ...     start_idx = end_idx - converted_len\n",
      "    ...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
      "    ...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
      "    ...     return indices\n",
      "\n",
      "\n",
      "    >>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n",
      "    >>> file_path = hf_hub_download(\n",
      "    ...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
      "    ... )\n",
      "    >>> container = av.open(file_path)\n",
      "\n",
      "    >>> # sample 16 frames\n",
      "    >>> indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
      "    >>> video = read_video_pyav(container, indices)\n",
      "\n",
      "    >>> image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n",
      "    >>> model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n",
      "\n",
      "    >>> inputs = image_processor(list(video), return_tensors=\"pt\")\n",
      "\n",
      "    >>> with torch.no_grad():\n",
      "    ...     outputs = model(**inputs)\n",
      "    ...     logits = outputs.logits\n",
      "\n",
      "    >>> # model predicts one of the 400 Kinetics-400 classes\n",
      "    >>> predicted_label = logits.argmax(-1).item()\n",
      "    >>> print(model.config.id2label[predicted_label])\n",
      "    eating spaghetti\n",
      "    ```\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/videomae/modeling_videomae.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "model3.forward?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시1 -- 입력나열, loss O*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=tensor(0.7254, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7084, -0.2919],\n",
       "        [-0.5846, -0.1854],\n",
       "        [-0.6635, -0.2402],\n",
       "        [-0.7193, -0.3811]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "model3(\n",
    "    pixel_values = torch.randn(4,16,3,224,224),\n",
    "    labels = torch.tensor([0,1,0,1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시2 -- `**딕셔너리`, loss O*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=tensor(0.7254, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7084, -0.2919],\n",
       "        [-0.5846, -0.1854],\n",
       "        [-0.6635, -0.2402],\n",
       "        [-0.7193, -0.3811]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "model3_input = dict(\n",
    "    pixel_values = torch.randn(4,16,3,224,224),\n",
    "    labels = torch.tensor([0,1,0,1])\n",
    ")\n",
    "model3(**model3_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시3 -- 입력나열, loss X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[-0.7084, -0.2919],\n",
       "        [-0.5846, -0.1854],\n",
       "        [-0.6635, -0.2402],\n",
       "        [-0.7193, -0.3811]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "model3(\n",
    "    pixel_values = torch.randn(4,16,3,224,224)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시4 -- `**딕셔너리`, loss X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[-0.7084, -0.2919],\n",
       "        [-0.5846, -0.1854],\n",
       "        [-0.6635, -0.2402],\n",
       "        [-0.7193, -0.3811]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "model3_input = dict(\n",
    "    pixel_values = torch.randn(4,16,3,224,224),\n",
    ")\n",
    "model3(**model3_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시5 -- 초간단, loss X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[-0.7084, -0.2919],\n",
       "        [-0.5846, -0.1854],\n",
       "        [-0.6635, -0.2402],\n",
       "        [-0.7193, -0.3811]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "model3(torch.randn(4,16,3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model 사용 연습 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. 텍스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model1 = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제1` -- imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = datasets.load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = imdb['train'].select(range(3))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*실패*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 363 at dim 1 (got 304)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model1\u001b[38;5;241m.\u001b[39mforward(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 363 at dim 1 (got 304)"
     ]
    }
   ],
   "source": [
    "model1.forward(torch.tensor(tokenizer(d['text'])['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*원인분석*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 - Type: list, Length: 3, Content: [[101, 1045, 12524, 1045, ... // ... , 7987, 1013, 1028, 102]]\n",
      "     Level 2 - Type: list, Length: 363, Content: [101, 1045, 12524, 1045,  ... // ... 7, 1037, 5436, 1012, 102]\n",
      "     Level 2 - Type: list, Length: 304, Content: [101, 1000, 1045, 2572, 8 ... // ... 5, 1055, 4230, 1012, 102]\n",
      "     Level 2 - Type: list, Length: 133, Content: [101, 2065, 2069, 2000, 4 ... // ... 6, 7987, 1013, 1028, 102]\n"
     ]
    }
   ],
   "source": [
    "mp.show_list(\n",
    "    tokenizer(d['text'])['input_ids']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 - Type: list, Length: 3, Content: [[101, 1045, 12524, 1045, ... // ...  0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "     Level 2 - Type: list, Length: 363, Content: [101, 1045, 12524, 1045,  ... // ... 7, 1037, 5436, 1012, 102]\n",
      "     Level 2 - Type: list, Length: 363, Content: [101, 1000, 1045, 2572, 8 ... // ... , 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "     Level 2 - Type: list, Length: 363, Content: [101, 2065, 2069, 2000, 4 ... // ... , 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "mp.show_list(\n",
    "    tokenizer(d['text'], padding=True)['input_ids']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*성공*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0165, -0.0270],\n",
       "        [ 0.0295, -0.0017],\n",
       "        [-0.0267, -0.0590]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(torch.tensor(tokenizer(d['text'], padding=True)['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0165, -0.0270],\n",
       "        [ 0.0295, -0.0017],\n",
       "        [-0.0267, -0.0590]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(tokenizer(d['text'], padding=True, return_tensors=\"pt\")['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제2` -- emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = datasets.load_dataset('emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = emotion['train'].select(range(3))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0104, -0.0463],\n",
       "        [-0.0066,  0.0257],\n",
       "        [-0.0160, -0.0326]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(torch.tensor(tokenizer(d['text'],padding=True)['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제3` -- MBTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-10 01:07:26--  https://raw.githubusercontent.com/guebin/MP2024/refs/heads/main/posts/mbti_1.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 62856486 (60M) [text/plain]\n",
      "Saving to: ‘mbti_1.csv’\n",
      "\n",
      "mbti_1.csv          100%[===================>]  59.94M   108MB/s    in 0.6s    \n",
      "\n",
      "2024-11-10 01:07:31 (108 MB/s) - ‘mbti_1.csv’ saved [62856486/62856486]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mbti1.csv 파일 다운로드 \n",
    "!wget https://raw.githubusercontent.com/guebin/MP2024/refs/heads/main/posts/mbti_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['type', 'posts'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.Dataset.from_csv(\"mbti_1.csv\").select(range(3))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*실패*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2102) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mposts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:978\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    976\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 978\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    988\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:785\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    783\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 785\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_flash_attention_2:\n\u001b[1;32m    788\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:131\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[0;34m(self, input_ids, input_embeds)\u001b[0m\n\u001b[1;32m    127\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand_as(input_ids)  \u001b[38;5;66;03m# (bs, max_seq_length)\u001b[39;00m\n\u001b[1;32m    129\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43minput_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    132\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2102) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "model1(torch.tensor(tokenizer(d['posts'],padding=True)['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*원인분석*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 - Type: list, Length: 3, Content: [[101, 1005, 8299, 1024,  ... // ...  0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "     Level 2 - Type: list, Length: 2102, Content: [101, 1005, 8299, 1024, 1 ... // ... , 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "     Level 2 - Type: list, Length: 2102, Content: [101, 1005, 1045, 1005, 1 ... // ... 2, 1012, 1012, 1005, 102]\n",
      "     Level 2 - Type: list, Length: 2102, Content: [101, 1005, 2204, 2028, 1 ... // ... , 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "mp.show_list(\n",
    "    tokenizer(d['posts'],padding=True)['input_ids']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 - Type: list, Length: 3, Content: [[101, 1005, 8299, 1024,  ... // ... , 7834, 1012, 2077, 102]]\n",
      "     Level 2 - Type: list, Length: 512, Content: [101, 1005, 8299, 1024, 1 ... // ... 1, 4127, 2017, 2215, 102]\n",
      "     Level 2 - Type: list, Length: 512, Content: [101, 1005, 1045, 1005, 1 ... // ... 6, 2600, 3259, 2028, 102]\n",
      "     Level 2 - Type: list, Length: 512, Content: [101, 1005, 2204, 2028, 1 ... // ... 0, 7834, 1012, 2077, 102]\n"
     ]
    }
   ],
   "source": [
    "mp.show_list(\n",
    "    tokenizer(d['posts'],truncation=True)['input_ids']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*성공*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3229, -0.0525],\n",
       "        [ 0.3190, -0.1215],\n",
       "        [ 0.3195, -0.0921]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(torch.tensor(tokenizer(d['posts'],truncation=True)['input_ids']))\n",
    "#model1(tokenizer(d['posts'],truncation=True,return_tensors=\"pt\")['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이2)` *--모델설정변경 (퀴즈5, 모델의 프레임수를 4로 바꾸는 예제에서 사용한 테크닉)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*distilbert/distilbert-base-uncased 설정값 부르기*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert/distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.46.2\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\"\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*설정값변경*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.max_position_embeddings = 2200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*설정값으로 모델불러오기*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_large = transformers.AutoModelForSequenceClassification.from_config(\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*모델사용*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[0.1088, 0.3406],\n",
       "        [0.0145, 0.2405],\n",
       "        [0.2319, 0.4239]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_large(torch.tensor(tokenizer(d['posts'],padding=True)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0095,  0.4671],\n",
       "        [ 0.0105,  0.3673],\n",
       "        [-0.0640,  0.4079]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_large(**tokenizer(d['posts'],padding=True,return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제4` -- sms_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sms', 'label'],\n",
       "        num_rows: 4459\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sms', 'label'],\n",
       "        num_rows: 1115\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_spam = datasets.load_dataset('sms_spam')['train'].train_test_split(test_size=0.2, seed=42)\n",
    "sms_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sms', 'label'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = sms_spam['train'].select(range(3))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.2472, -0.0833],\n",
       "        [ 0.2985, -0.1250],\n",
       "        [ 0.2760, -0.1416]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(**tokenizer(d['sms'],padding=True,return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. 이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model2 = transformers.AutoModelForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=3 # 그냥 대충 3이라고 했음.. 별 이유는 없음\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제1` -- food101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset(\"food101\", split=\"train[:4]\")\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(예비학습)` -- `torchvision.transforms` 에서 제공하는 기능들은 배치처리가 가능한가? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1216, 0.1137, 0.1098,  ..., 0.0039, 0.0039, 0.0000],\n",
       "         [0.1255, 0.1216, 0.1176,  ..., 0.0039, 0.0039, 0.0000],\n",
       "         [0.1294, 0.1255, 0.1255,  ..., 0.0039, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.2588, 0.2745, 0.2863,  ..., 0.3765, 0.3882, 0.3922],\n",
       "         [0.2353, 0.2471, 0.2667,  ..., 0.3373, 0.3373, 0.3373],\n",
       "         [0.2235, 0.2275, 0.2471,  ..., 0.3333, 0.3176, 0.3059]],\n",
       "\n",
       "        [[0.1373, 0.1294, 0.1255,  ..., 0.1020, 0.1020, 0.0980],\n",
       "         [0.1412, 0.1373, 0.1333,  ..., 0.1020, 0.1020, 0.0980],\n",
       "         [0.1451, 0.1412, 0.1412,  ..., 0.1020, 0.0980, 0.0980],\n",
       "         ...,\n",
       "         [0.2471, 0.2627, 0.2745,  ..., 0.3647, 0.3765, 0.3882],\n",
       "         [0.2235, 0.2353, 0.2549,  ..., 0.3255, 0.3333, 0.3333],\n",
       "         [0.2118, 0.2157, 0.2353,  ..., 0.3216, 0.3137, 0.3020]],\n",
       "\n",
       "        [[0.1412, 0.1333, 0.1294,  ..., 0.0902, 0.0902, 0.0863],\n",
       "         [0.1451, 0.1412, 0.1451,  ..., 0.0902, 0.0902, 0.0863],\n",
       "         [0.1490, 0.1451, 0.1529,  ..., 0.0902, 0.0863, 0.0863],\n",
       "         ...,\n",
       "         [0.1725, 0.1882, 0.2000,  ..., 0.2431, 0.2549, 0.2667],\n",
       "         [0.1490, 0.1608, 0.1804,  ..., 0.2039, 0.2118, 0.2118],\n",
       "         [0.1373, 0.1412, 0.1608,  ..., 0.2000, 0.1922, 0.1804]]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_tensor(d['image'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[176], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torchvision/transforms/functional.py:142\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    140\u001b[0m     _log_api_usage_once(to_tensor)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (F_pil\u001b[38;5;241m.\u001b[39m_is_pil_image(pic) \u001b[38;5;129;01mor\u001b[39;00m _is_numpy(pic)):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pic)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy(pic) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_numpy_image(pic):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be 2/3 dimensional. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'list'>"
     ]
    }
   ],
   "source": [
    "to_tensor(d['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "compose = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((224,224))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 224, 224])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(list(map(compose,d['image'])),axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[ 0.0918, -0.0196,  0.1804],\n",
       "        [ 0.1143, -0.1224, -0.0767],\n",
       "        [ 0.0384, -0.0947,  0.1103],\n",
       "        [ 0.1829, -0.1174, -0.0410]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.forward(\n",
    "    torch.stack(list(map(compose,d['image'])),axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_file_path', 'image', 'labels'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beans = datasets.load_dataset('beans')\n",
    "d = beans['train'].select(range(4))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[ 0.0715,  0.1286, -0.0608],\n",
       "        [-0.1150, -0.0106,  0.0222],\n",
       "        [-0.0856,  0.0813,  0.0233],\n",
       "        [ 0.0065,  0.1047,  0.0695]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(torch.stack(list(map(compose,d['image'])),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. 동영상 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model3 = transformers.VideoMAEForVideoClassification.from_pretrained(\n",
    "    \"MCG-NJU/videomae-base\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제1` -- UCF101_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = huggingface_hub.hf_hub_download(\n",
    "    repo_id=\"sayakpaul/ucf101-subset\",\n",
    "    filename=\"UCF101_subset.tar.gz\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "# file_path는 다운로드한 압축파일이 존재하는 경로와 파일명이 string으로 저장되어있음.\n",
    "with tarfile.open(file_path) as t:\n",
    "     t.extractall(\"./data\") # 여기에서 \".\"은 현재폴더라는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "└── UCF101_subset\n",
      "    ├── test\n",
      "    │   ├── ApplyEyeMakeup\n",
      "    │   │   ├── UCF101\n",
      "    │   │   ├── v_ApplyEyeMakeup_g03_c01.avi\n",
      "    │   │   └── ...\n",
      "    │   │   └── v_ApplyEyeMakeup_g23_c06.avi\n",
      "    │   ├── ApplyLipstick\n",
      "    │   │   ├── UCF101\n",
      "    │   │   ├── v_ApplyLipstick_g14_c01.avi\n",
      "    │   │   └── ...\n",
      "    │   │   └── v_ApplyLipstick_g16_c04.avi\n",
      "    │   └── ...\n",
      "    │   └── BenchPress\n",
      "    │       ├── UCF101\n",
      "    │       ├── v_BenchPress_g05_c02.avi\n",
      "    │       └── ...\n",
      "    │       └── v_BenchPress_g25_c06.avi\n",
      "    ├── train\n",
      "    │   ├── ApplyEyeMakeup\n",
      "    │   │   ├── UCF101\n",
      "    │   │   ├── v_ApplyEyeMakeup_g02_c03.avi\n",
      "    │   │   └── ...\n",
      "    │   │   └── v_ApplyEyeMakeup_g25_c07.avi\n",
      "    │   ├── ApplyLipstick\n",
      "    │   │   ├── UCF101\n",
      "    │   │   ├── v_ApplyLipstick_g01_c02.avi\n",
      "    │   │   └── ...\n",
      "    │   │   └── v_ApplyLipstick_g24_c05.avi\n",
      "    │   └── ...\n",
      "    │   └── BenchPress\n",
      "    │       ├── UCF101\n",
      "    │       ├── v_BenchPress_g01_c05.avi\n",
      "    │       └── ...\n",
      "    │       └── v_BenchPress_g24_c05.avi\n",
      "    └── val\n",
      "        ├── ApplyEyeMakeup\n",
      "        │   ├── UCF101\n",
      "        │   ├── v_ApplyEyeMakeup_g01_c01.avi\n",
      "        │   ├── v_ApplyEyeMakeup_g14_c05.avi\n",
      "        │   └── v_ApplyEyeMakeup_g20_c04.avi\n",
      "        ├── ApplyLipstick\n",
      "        │   ├── UCF101\n",
      "        │   ├── v_ApplyLipstick_g10_c04.avi\n",
      "        │   ├── v_ApplyLipstick_g20_c04.avi\n",
      "        │   └── v_ApplyLipstick_g25_c02.avi\n",
      "        └── ...\n",
      "        └── BenchPress\n",
      "            ├── UCF101\n",
      "            ├── v_BenchPress_g11_c05.avi\n",
      "            ├── v_BenchPress_g17_c02.avi\n",
      "            └── v_BenchPress_g17_c06.avi\n"
     ]
    }
   ],
   "source": [
    "mp.tree(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 67, 240, 320])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_path = \"./data/UCF101_subset/test/BenchPress/v_BenchPress_g05_c02.avi\"\n",
    "video = pytorchvideo.data.encoded_video.EncodedVideo.from_path(video_path).get_clip(0, float('inf'))['video']\n",
    "video.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[ 0.2642, -0.1763]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3(\n",
    "    #video.permute(1,0,2,3)[:16,:,:224,:224].unsqueeze(0)\n",
    "    #video.permute(1,0,2,3)[:16,:,:224,:224].reshape(1,16,3,224,224)\n",
    "    torch.stack([video.permute(1,0,2,3)[:16,:,:224,:224]])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
