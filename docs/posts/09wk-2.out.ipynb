{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 09wk-2: `model`의 입력파악, `model`의 사용연습\n",
        "\n",
        "최규빈  \n",
        "2024-11-09\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/guebin/MP2024/blob/main/posts/09wk-2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>\n",
        "\n",
        "# 1. 강의영상\n",
        "\n",
        "<https://youtu.be/playlist?list=PLQqh36zP38-xsRgsXpLEUNWClxLi0N9Mk&si=Lssxzw70RRT1adiJ>\n",
        "\n",
        "# 2. Imports"
      ],
      "id": "d0629c12-8d4a-4be9-a9c3-3bc006a484ee"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "import datasets\n",
        "import huggingface_hub\n",
        "import torch\n",
        "import torchvision\n",
        "import pytorchvideo.data\n",
        "import PIL\n",
        "import tarfile\n",
        "import mp2024pkg as mp"
      ],
      "id": "cell-5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Model 입력파악\n",
        "\n",
        "`-` 아래중 하나의 방법순으로..\n",
        "\n",
        "-   방법1: `model.forward?` 에서 시그니처를 확인\n",
        "-   방법2: `model.forward?` 에서 사용예제를 확인\n",
        "-   방법3: 인터넷을 활용한 외부 자료 확인 (공식문서, 공식튜토리얼,\n",
        "    신뢰할만한 블로그, ChatGPT등)\n",
        "-   방법4: `model.forward??` 를 보고 모든 코드를 뜯어봄 \\<— 하지마세요\n",
        "\n",
        "`-` 모델의 입력이 어떤형태로 정리되어야 하는지 알아내는 확실한 방법은\n",
        "없음\n",
        "\n",
        "-   방법1,2,3 은 다른사람의 호의에 기대해야함.\n",
        "-   방법4는 사실상 불가능\n",
        "\n",
        "`# 예제1` – 텍스트분류"
      ],
      "id": "418e77b8-1c6b-4636-b478-8dbb7698bf6d"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "model1 = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\", num_labels=2\n",
        ")"
      ],
      "id": "cell-10"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*모델의 기본정보(config)*"
      ],
      "id": "625a0127-6446-424d-b6b3-ba3661f64fe9"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1.config"
      ],
      "id": "cell-12"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   max_position_embeddings: 512\n",
        "\n",
        "*모델의 입력파악*"
      ],
      "id": "563e63bd-cb3d-4e6a-a619-7cd7b14fa319"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Signature:\n",
            "model1.forward(\n",
            "    input_ids: Optional[torch.Tensor] = None,\n",
            "    attention_mask: Optional[torch.Tensor] = None,\n",
            "    head_mask: Optional[torch.Tensor] = None,\n",
            "    inputs_embeds: Optional[torch.Tensor] = None,\n",
            "    labels: Optional[torch.LongTensor] = None,\n",
            "    output_attentions: Optional[bool] = None,\n",
            "    output_hidden_states: Optional[bool] = None,\n",
            "    return_dict: Optional[bool] = None,\n",
            ") -> Union[transformers.modeling_outputs.SequenceClassifierOutput, Tuple[torch.Tensor, ...]]\n",
            "Docstring:\n",
            "The [`DistilBertForSequenceClassification`] forward method, overrides the `__call__` special method.\n",
            "\n",
            "<Tip>\n",
            "\n",
            "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
            "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
            "the latter silently ignores them.\n",
            "\n",
            "</Tip>\n",
            "\n",
            "Args:\n",
            "    input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
            "        Indices of input sequence tokens in the vocabulary.\n",
            "\n",
            "        Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
            "        [`PreTrainedTokenizer.__call__`] for details.\n",
            "\n",
            "        [What are input IDs?](../glossary#input-ids)\n",
            "    attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
            "        Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
            "\n",
            "        - 1 for tokens that are **not masked**,\n",
            "        - 0 for tokens that are **masked**.\n",
            "\n",
            "        [What are attention masks?](../glossary#attention-mask)\n",
            "    head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
            "        Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
            "\n",
            "        - 1 indicates the head is **not masked**,\n",
            "        - 0 indicates the head is **masked**.\n",
            "\n",
            "    inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
            "        Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
            "        is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
            "        model's internal embedding lookup matrix.\n",
            "    output_attentions (`bool`, *optional*):\n",
            "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
            "        tensors for more detail.\n",
            "    output_hidden_states (`bool`, *optional*):\n",
            "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
            "        more detail.\n",
            "    return_dict (`bool`, *optional*):\n",
            "        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            "\n",
            "    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
            "        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
            "        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
            "        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
            "    \n",
            "Returns:\n",
            "    [`transformers.modeling_outputs.SequenceClassifierOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.SequenceClassifierOutput`] or a tuple of\n",
            "    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
            "    elements depending on the configuration ([`DistilBertConfig`]) and inputs.\n",
            "\n",
            "    - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Classification (or regression if config.num_labels==1) loss.\n",
            "    - **logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) -- Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
            "    - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
            "      one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
            "\n",
            "      Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
            "    - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
            "      sequence_length)`.\n",
            "\n",
            "      Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
            "      heads.\n",
            "\n",
            "Example of single-label classification:\n",
            "\n",
            "```python\n",
            ">>> import torch\n",
            ">>> from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
            "\n",
            ">>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
            ">>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
            "\n",
            ">>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
            "\n",
            ">>> with torch.no_grad():\n",
            "...     logits = model(**inputs).logits\n",
            "\n",
            ">>> predicted_class_id = logits.argmax().item()\n",
            "\n",
            ">>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
            ">>> num_labels = len(model.config.id2label)\n",
            ">>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
            "\n",
            ">>> labels = torch.tensor([1])\n",
            ">>> loss = model(**inputs, labels=labels).loss\n",
            "```\n",
            "\n",
            "Example of multi-label classification:\n",
            "\n",
            "```python\n",
            ">>> import torch\n",
            ">>> from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
            "\n",
            ">>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
            ">>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", problem_type=\"multi_label_classification\")\n",
            "\n",
            ">>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
            "\n",
            ">>> with torch.no_grad():\n",
            "...     logits = model(**inputs).logits\n",
            "\n",
            ">>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
            "\n",
            ">>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
            ">>> num_labels = len(model.config.id2label)\n",
            ">>> model = DistilBertForSequenceClassification.from_pretrained(\n",
            "...     \"distilbert-base-uncased\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
            "... )\n",
            "\n",
            ">>> labels = torch.sum(\n",
            "...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n",
            "... ).to(torch.float)\n",
            ">>> loss = model(**inputs, labels=labels).loss\n",
            "```\n",
            "File:      ~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py\n",
            "Type:      method"
          ]
        }
      ],
      "source": [
        "model1.forward? "
      ],
      "id": "cell-15"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시1 – 입력나열, loss O*"
      ],
      "id": "b683a6a3-8084-4327-8b80-490dba097344"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(\n",
        "    input_ids = torch.tensor([[1,2,3,4], [2,3,4,5]]),\n",
        "    labels = torch.tensor([0,0])\n",
        ")"
      ],
      "id": "cell-17"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시2 – `**딕셔너리`, loss O*"
      ],
      "id": "5f20a37a-f46d-4617-8c7e-2534075d6d55"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1_input = dict(\n",
        "    input_ids = torch.tensor([[1,2,3,4], [2,3,4,5]]),\n",
        "    labels = torch.tensor([0,0])\n",
        ")\n",
        "model1(**model1_input)"
      ],
      "id": "cell-19"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시3 – 입력나열, loss X*"
      ],
      "id": "57e1f5db-2648-49ec-836f-1337efeb576b"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(\n",
        "    input_ids = torch.tensor([[1,2,3,4], [2,3,4,5]])\n",
        ")"
      ],
      "id": "cell-21"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시4 – `**딕셔너리`, loss X*"
      ],
      "id": "72c3db24-6a7b-44e5-a095-fe058ffd7f97"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1_input = dict(\n",
        "    input_ids = torch.tensor([[1,2,3,4], [2,3,4,5]])\n",
        ")\n",
        "model1(**model1_input)"
      ],
      "id": "cell-23"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시5 – 초간단, loss X*"
      ],
      "id": "e02c830f-d132-4ec0-bb14-2eb4e601337b"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(torch.tensor([[1,2,3,4], [2,3,4,5]]))"
      ],
      "id": "cell-25"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> 사용예시1~5에서 `model1()` 대신에 `model1.forward()`를 사용해도 된다.\n",
        "\n",
        "`#`\n",
        "\n",
        "`# 예제2` – 이미지분류"
      ],
      "id": "6fef29d0-6c3f-47f9-aa73-2a60b606ede6"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "model2 = transformers.AutoModelForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\",\n",
        "    num_labels=3 # 그냥 대충 3이라고 했음.. 별 이유는 없음\n",
        ")"
      ],
      "id": "cell-29"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*모델의 기본정보(config)*"
      ],
      "id": "c5f80deb-9fe1-43a4-ad17-c32443508eb9"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2.config"
      ],
      "id": "cell-31"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   image_size: 224"
      ],
      "id": "14f53c98-ce34-4e4b-b363-727207be255b"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "mp.tab(model2.config)"
      ],
      "id": "cell-33"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2.config.num_channels"
      ],
      "id": "cell-34"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*모델의 입력파악*"
      ],
      "id": "2056481a-d3a1-4421-aa34-d4a31255c763"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.randn(2,3,64,64)"
      ],
      "id": "cell-36"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시1 – 입력나열, loss O*"
      ],
      "id": "9b9c27be-828d-4399-a09e-5a8376cec421"
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model2(\n",
        "    pixel_values = torch.randn(2,3,224,224),\n",
        "    labels = torch.tensor([0,1])\n",
        ")"
      ],
      "id": "cell-38"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시2 – `**딕셔너리`, loss O*"
      ],
      "id": "b0f58844-636e-46c2-96d1-56ac08eb09c8"
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model2_input = dict(\n",
        "    pixel_values = torch.randn(2,3,224,224),\n",
        "    labels = torch.tensor([0,1])\n",
        ")\n",
        "model2(**model2_input)"
      ],
      "id": "cell-40"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시3 – 입력나열, loss X*"
      ],
      "id": "9c36a382-1d83-473c-9b84-501dae0df12c"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model2(\n",
        "    pixel_values = torch.randn(2,3,224,224)\n",
        ")"
      ],
      "id": "cell-42"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시4 – `**딕셔너리`, loss X*"
      ],
      "id": "f645e000-52f8-4d94-8b83-d9755dcd667d"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model2_input = dict(\n",
        "    pixel_values = torch.randn(2,3,224,224),\n",
        ")\n",
        "model2(**model2_input)"
      ],
      "id": "cell-44"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시5 – 초간단, loss X*"
      ],
      "id": "e0e567f6-4a2b-441e-ad42-7c6d0a4cd7bf"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model2(torch.randn(2,3,224,224))"
      ],
      "id": "cell-46"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "`# 예제3` – 동영상분류"
      ],
      "id": "31b15ca5-b832-4d86-84eb-5b3d2e27fa66"
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "model3 = transformers.VideoMAEForVideoClassification.from_pretrained(\n",
        "    \"MCG-NJU/videomae-base\",\n",
        ")"
      ],
      "id": "cell-49"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*모델의 기본정보(config)*"
      ],
      "id": "c7609e03-65ef-4c72-a4a1-3b8e423c41e0"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "model3.config"
      ],
      "id": "cell-51"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   image_size: 224\n",
        "-   num_frames: 16\n",
        "\n",
        "*모델의 입력파악*"
      ],
      "id": "5efe5015-5f65-4798-b1bc-6850f25cf833"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Signature:\n",
            "model3.forward(\n",
            "    pixel_values: Optional[torch.Tensor] = None,\n",
            "    head_mask: Optional[torch.Tensor] = None,\n",
            "    labels: Optional[torch.Tensor] = None,\n",
            "    output_attentions: Optional[bool] = None,\n",
            "    output_hidden_states: Optional[bool] = None,\n",
            "    return_dict: Optional[bool] = None,\n",
            ") -> Union[Tuple, transformers.modeling_outputs.ImageClassifierOutput]\n",
            "Docstring:\n",
            "The [`VideoMAEForVideoClassification`] forward method, overrides the `__call__` special method.\n",
            "\n",
            "<Tip>\n",
            "\n",
            "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
            "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
            "the latter silently ignores them.\n",
            "\n",
            "</Tip>\n",
            "\n",
            "Args:\n",
            "    pixel_values (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, height, width)`):\n",
            "        Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n",
            "        [`VideoMAEImageProcessor.__call__`] for details.\n",
            "\n",
            "    head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
            "        Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
            "\n",
            "        - 1 indicates the head is **not masked**,\n",
            "        - 0 indicates the head is **masked**.\n",
            "\n",
            "    output_attentions (`bool`, *optional*):\n",
            "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
            "        tensors for more detail.\n",
            "    output_hidden_states (`bool`, *optional*):\n",
            "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
            "        more detail.\n",
            "    return_dict (`bool`, *optional*):\n",
            "        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            "\n",
            "    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
            "        Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n",
            "        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
            "        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
            "\n",
            "\n",
            "    Returns:\n",
            "        [`transformers.modeling_outputs.ImageClassifierOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.ImageClassifierOutput`] or a tuple of\n",
            "        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
            "        elements depending on the configuration ([`VideoMAEConfig`]) and inputs.\n",
            "\n",
            "        - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Classification (or regression if config.num_labels==1) loss.\n",
            "        - **logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) -- Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
            "        - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
            "          one for the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states\n",
            "          (also called feature maps) of the model at the output of each stage.\n",
            "        - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, patch_size,\n",
            "          sequence_length)`.\n",
            "\n",
            "          Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
            "          heads.\n",
            "  \n",
            "\n",
            "    Examples:\n",
            "\n",
            "    ```python\n",
            "    >>> import av\n",
            "    >>> import torch\n",
            "    >>> import numpy as np\n",
            "\n",
            "    >>> from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n",
            "    >>> from huggingface_hub import hf_hub_download\n",
            "\n",
            "    >>> np.random.seed(0)\n",
            "\n",
            "\n",
            "    >>> def read_video_pyav(container, indices):\n",
            "    ...     '''\n",
            "    ...     Decode the video with PyAV decoder.\n",
            "    ...     Args:\n",
            "    ...         container (`av.container.input.InputContainer`): PyAV container.\n",
            "    ...         indices (`List[int]`): List of frame indices to decode.\n",
            "    ...     Returns:\n",
            "    ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
            "    ...     '''\n",
            "    ...     frames = []\n",
            "    ...     container.seek(0)\n",
            "    ...     start_index = indices[0]\n",
            "    ...     end_index = indices[-1]\n",
            "    ...     for i, frame in enumerate(container.decode(video=0)):\n",
            "    ...         if i > end_index:\n",
            "    ...             break\n",
            "    ...         if i >= start_index and i in indices:\n",
            "    ...             frames.append(frame)\n",
            "    ...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
            "\n",
            "\n",
            "    >>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
            "    ...     '''\n",
            "    ...     Sample a given number of frame indices from the video.\n",
            "    ...     Args:\n",
            "    ...         clip_len (`int`): Total number of frames to sample.\n",
            "    ...         frame_sample_rate (`int`): Sample every n-th frame.\n",
            "    ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n",
            "    ...     Returns:\n",
            "    ...         indices (`List[int]`): List of sampled frame indices\n",
            "    ...     '''\n",
            "    ...     converted_len = int(clip_len * frame_sample_rate)\n",
            "    ...     end_idx = np.random.randint(converted_len, seg_len)\n",
            "    ...     start_idx = end_idx - converted_len\n",
            "    ...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
            "    ...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
            "    ...     return indices\n",
            "\n",
            "\n",
            "    >>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n",
            "    >>> file_path = hf_hub_download(\n",
            "    ...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
            "    ... )\n",
            "    >>> container = av.open(file_path)\n",
            "\n",
            "    >>> # sample 16 frames\n",
            "    >>> indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
            "    >>> video = read_video_pyav(container, indices)\n",
            "\n",
            "    >>> image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n",
            "    >>> model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n",
            "\n",
            "    >>> inputs = image_processor(list(video), return_tensors=\"pt\")\n",
            "\n",
            "    >>> with torch.no_grad():\n",
            "    ...     outputs = model(**inputs)\n",
            "    ...     logits = outputs.logits\n",
            "\n",
            "    >>> # model predicts one of the 400 Kinetics-400 classes\n",
            "    >>> predicted_label = logits.argmax(-1).item()\n",
            "    >>> print(model.config.id2label[predicted_label])\n",
            "    eating spaghetti\n",
            "    ```\n",
            "File:      ~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/videomae/modeling_videomae.py\n",
            "Type:      method"
          ]
        }
      ],
      "source": [
        "model3.forward?"
      ],
      "id": "cell-54"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시1 – 입력나열, loss O*"
      ],
      "id": "d24cf507-58af-4f04-90c9-6378513b78b6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model3(\n",
        "    pixel_values = torch.randn(4,16,3,224,224),\n",
        "    labels = torch.tensor([0,1,0,1])\n",
        ")"
      ],
      "id": "cell-56"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시2 – `**딕셔너리`, loss O*"
      ],
      "id": "4ec1b288-e5bb-4ae5-a9d5-36fec49bfca4"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model3_input = dict(\n",
        "    pixel_values = torch.randn(4,16,3,224,224),\n",
        "    labels = torch.tensor([0,1,0,1])\n",
        ")\n",
        "model3(**model3_input)"
      ],
      "id": "cell-58"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시3 – 입력나열, loss X*"
      ],
      "id": "c2921ad7-4ade-4390-a0ff-9955ac6f6a52"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model3(\n",
        "    pixel_values = torch.randn(4,16,3,224,224)\n",
        ")"
      ],
      "id": "cell-60"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시4 – `**딕셔너리`, loss X*"
      ],
      "id": "b86a7e17-b1ff-4ff3-b55e-077b88e47674"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model3_input = dict(\n",
        "    pixel_values = torch.randn(4,16,3,224,224),\n",
        ")\n",
        "model3(**model3_input)"
      ],
      "id": "cell-62"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시5 – 초간단, loss X*"
      ],
      "id": "2488a21d-4396-4e4b-93cb-b855f6c65f90"
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model3(torch.randn(4,16,3,224,224))"
      ],
      "id": "cell-64"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "# 4. Model 사용 연습\n",
        "\n",
        "## A. 텍스트"
      ],
      "id": "e110bc48-946c-47d4-9854-a0c216ebecf6"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "model1 = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\", num_labels=2\n",
        ")\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
      ],
      "id": "cell-68"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`# 예제1` – imdb"
      ],
      "id": "2c1e5d8b-f21e-43c0-bb7b-d2ad35f3dfc4"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "imdb = datasets.load_dataset('imdb')"
      ],
      "id": "cell-70"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = imdb['train'].select(range(3))\n",
        "d"
      ],
      "id": "cell-71"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이1)`\n",
        "\n",
        "*실패*"
      ],
      "id": "17a14cc8-3cc0-44b2-95a4-de442293b8a5"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1.forward(torch.tensor(tokenizer(d['text'])['input_ids']))"
      ],
      "id": "cell-74"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*원인분석*"
      ],
      "id": "02b2d5fd-9ec7-42f9-acfd-7e3e921e4bc9"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level 1 - Type: list, Length: 3, Content: [[101, 1045, 12524, 1045, ... // ... , 7987, 1013, 1028, 102]]\n",
            "     Level 2 - Type: list, Length: 363, Content: [101, 1045, 12524, 1045,  ... // ... 7, 1037, 5436, 1012, 102]\n",
            "     Level 2 - Type: list, Length: 304, Content: [101, 1000, 1045, 2572, 8 ... // ... 5, 1055, 4230, 1012, 102]\n",
            "     Level 2 - Type: list, Length: 133, Content: [101, 2065, 2069, 2000, 4 ... // ... 6, 7987, 1013, 1028, 102]"
          ]
        }
      ],
      "source": [
        "mp.show_list(\n",
        "    tokenizer(d['text'])['input_ids']\n",
        ")"
      ],
      "id": "cell-76"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level 1 - Type: list, Length: 3, Content: [[101, 1045, 12524, 1045, ... // ...  0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "     Level 2 - Type: list, Length: 363, Content: [101, 1045, 12524, 1045,  ... // ... 7, 1037, 5436, 1012, 102]\n",
            "     Level 2 - Type: list, Length: 363, Content: [101, 1000, 1045, 2572, 8 ... // ... , 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "     Level 2 - Type: list, Length: 363, Content: [101, 2065, 2069, 2000, 4 ... // ... , 0, 0, 0, 0, 0, 0, 0, 0]"
          ]
        }
      ],
      "source": [
        "mp.show_list(\n",
        "    tokenizer(d['text'], padding=True)['input_ids']\n",
        ")"
      ],
      "id": "cell-77"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*성공*"
      ],
      "id": "e37cf9d7-4ae5-4228-8c34-5e2c586989fe"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(torch.tensor(tokenizer(d['text'], padding=True)['input_ids']))"
      ],
      "id": "cell-79"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이2)`"
      ],
      "id": "1c3067c1-0b1a-4ea0-9256-17978b59dc73"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(tokenizer(d['text'], padding=True, return_tensors=\"pt\")['input_ids'])"
      ],
      "id": "cell-81"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "`# 예제2` – emotion"
      ],
      "id": "72089953-adec-4a2c-bb0b-a00706cf9aa9"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "emotion = datasets.load_dataset('emotion')"
      ],
      "id": "cell-84"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = emotion['train'].select(range(3))\n",
        "d"
      ],
      "id": "cell-85"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이)`"
      ],
      "id": "a131fd73-ed68-4ea3-8649-bee7433ec027"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(torch.tensor(tokenizer(d['text'],padding=True)['input_ids']))"
      ],
      "id": "cell-87"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "`# 예제3` – MBTI"
      ],
      "id": "e5584c39-540c-4c02-a286-f5fadd0ed191"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "    - Avoid using `tokenizers` before the fork if possible\n",
            "    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-10 01:07:26--  https://raw.githubusercontent.com/guebin/MP2024/refs/heads/main/posts/mbti_1.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62856486 (60M) [text/plain]\n",
            "Saving to: ‘mbti_1.csv’\n",
            "\n",
            "mbti_1.csv          100%[===================>]  59.94M   108MB/s    in 0.6s    \n",
            "\n",
            "2024-11-10 01:07:31 (108 MB/s) - ‘mbti_1.csv’ saved [62856486/62856486]\n"
          ]
        }
      ],
      "source": [
        "# mbti1.csv 파일 다운로드 \n",
        "!wget https://raw.githubusercontent.com/guebin/MP2024/refs/heads/main/posts/mbti_1.csv"
      ],
      "id": "cell-90"
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = datasets.Dataset.from_csv(\"mbti_1.csv\").select(range(3))\n",
        "d"
      ],
      "id": "cell-91"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이1)`\n",
        "\n",
        "*실패*"
      ],
      "id": "f4ad3ddd-26b6-4555-9e42-8545edae3fbd"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(torch.tensor(tokenizer(d['posts'],padding=True)['input_ids']))"
      ],
      "id": "cell-94"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*원인분석*"
      ],
      "id": "1c74e19d-0961-4309-a255-6ce24badce01"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level 1 - Type: list, Length: 3, Content: [[101, 1005, 8299, 1024,  ... // ...  0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "     Level 2 - Type: list, Length: 2102, Content: [101, 1005, 8299, 1024, 1 ... // ... , 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "     Level 2 - Type: list, Length: 2102, Content: [101, 1005, 1045, 1005, 1 ... // ... 2, 1012, 1012, 1005, 102]\n",
            "     Level 2 - Type: list, Length: 2102, Content: [101, 1005, 2204, 2028, 1 ... // ... , 0, 0, 0, 0, 0, 0, 0, 0]"
          ]
        }
      ],
      "source": [
        "mp.show_list(\n",
        "    tokenizer(d['posts'],padding=True)['input_ids']\n",
        ")"
      ],
      "id": "cell-96"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level 1 - Type: list, Length: 3, Content: [[101, 1005, 8299, 1024,  ... // ... , 7834, 1012, 2077, 102]]\n",
            "     Level 2 - Type: list, Length: 512, Content: [101, 1005, 8299, 1024, 1 ... // ... 1, 4127, 2017, 2215, 102]\n",
            "     Level 2 - Type: list, Length: 512, Content: [101, 1005, 1045, 1005, 1 ... // ... 6, 2600, 3259, 2028, 102]\n",
            "     Level 2 - Type: list, Length: 512, Content: [101, 1005, 2204, 2028, 1 ... // ... 0, 7834, 1012, 2077, 102]"
          ]
        }
      ],
      "source": [
        "mp.show_list(\n",
        "    tokenizer(d['posts'],truncation=True)['input_ids']\n",
        ")"
      ],
      "id": "cell-97"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*성공*"
      ],
      "id": "f729c059-b64c-49b2-93e6-fd1297cec5ce"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(torch.tensor(tokenizer(d['posts'],truncation=True)['input_ids']))\n",
        "#model1(tokenizer(d['posts'],truncation=True,return_tensors=\"pt\")['input_ids'])"
      ],
      "id": "cell-99"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이2)` *–모델설정변경 (퀴즈5, 모델의 프레임수를 4로 바꾸는 예제에서\n",
        "사용한 테크닉)*\n",
        "\n",
        "*distilbert/distilbert-base-uncased 설정값 부르기*"
      ],
      "id": "62fe2b6a-7c4c-44d2-8d59-a907eb2bceb0"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = transformers.AutoConfig.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\"\n",
        ")\n",
        "config"
      ],
      "id": "cell-102"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*설정값변경*"
      ],
      "id": "a37b4012-f662-42fe-8c9f-47a6417257a1"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "config.max_position_embeddings = 2200"
      ],
      "id": "cell-104"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*설정값으로 모델불러오기*"
      ],
      "id": "532c9848-f5d8-4afc-9973-906c7a0a35ad"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1_large = transformers.AutoModelForSequenceClassification.from_config(\n",
        "    config=config\n",
        ")"
      ],
      "id": "cell-106"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*모델사용*"
      ],
      "id": "c6d3e9f7-39a6-43dc-9693-c1d78245a95d"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1_large(torch.tensor(tokenizer(d['posts'],padding=True)['input_ids']))"
      ],
      "id": "cell-108"
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1_large(**tokenizer(d['posts'],padding=True,return_tensors=\"pt\"))"
      ],
      "id": "cell-109"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "`# 예제4` – sms_spam"
      ],
      "id": "d956d642-652e-4659-8438-a094bfce4953"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "sms_spam = datasets.load_dataset('sms_spam')['train'].train_test_split(test_size=0.2, seed=42)\n",
        "sms_spam"
      ],
      "id": "cell-112"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = sms_spam['train'].select(range(3))\n",
        "d"
      ],
      "id": "cell-113"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이)`"
      ],
      "id": "948d7700-a3c8-48d8-8dd7-2ffd1ce2ead9"
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(**tokenizer(d['sms'],padding=True,return_tensors=\"pt\"))"
      ],
      "id": "cell-115"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "## B. 이미지"
      ],
      "id": "957a30eb-11db-40cf-a6bc-cbf8a6c0e7bd"
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "model2 = transformers.AutoModelForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\",\n",
        "    num_labels=3 # 그냥 대충 3이라고 했음.. 별 이유는 없음\n",
        ")"
      ],
      "id": "cell-118"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`# 예제1` – food101"
      ],
      "id": "3494ac79-f676-41c4-a8a7-400e96141620"
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = datasets.load_dataset(\"food101\", split=\"train[:4]\")\n",
        "d"
      ],
      "id": "cell-120"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(예비학습)` – `torchvision.transforms` 에서 제공하는 기능들은\n",
        "배치처리가 가능한가?"
      ],
      "id": "cd1d2df6-ca5c-492e-997b-795715801ad1"
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [],
      "source": [
        "to_tensor = torchvision.transforms.ToTensor()"
      ],
      "id": "cell-122"
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [],
      "source": [
        "to_tensor(d['image'][0])"
      ],
      "id": "cell-123"
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [],
      "source": [
        "to_tensor(d['image'])"
      ],
      "id": "cell-124"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이)`"
      ],
      "id": "f2810725-a4cf-4170-9e1f-85860e8c9b61"
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [],
      "source": [
        "compose = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Resize((224,224))\n",
        "])"
      ],
      "id": "cell-126"
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.stack(list(map(compose,d['image'])),axis=0).shape"
      ],
      "id": "cell-127"
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2.forward(\n",
        "    torch.stack(list(map(compose,d['image'])),axis=0)\n",
        ")"
      ],
      "id": "cell-128"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "`# 예제2`"
      ],
      "id": "de399a09-10e6-4a95-af99-19cadc4dca96"
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [],
      "source": [
        "beans = datasets.load_dataset('beans')\n",
        "d = beans['train'].select(range(4))\n",
        "d"
      ],
      "id": "cell-131"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이)`"
      ],
      "id": "323f260d-a34e-498e-bfd6-bd2db19054bc"
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2(torch.stack(list(map(compose,d['image'])),axis=0))"
      ],
      "id": "cell-133"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "## C. 동영상"
      ],
      "id": "2c24c0f1-1e55-4f09-b18b-cbb3e4006559"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "model3 = transformers.VideoMAEForVideoClassification.from_pretrained(\n",
        "    \"MCG-NJU/videomae-base\",\n",
        ")"
      ],
      "id": "cell-136"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`# 예제1` – UCF101_subset"
      ],
      "id": "6c74885f-4a9b-4449-8f4f-bd6c68b50f68"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = huggingface_hub.hf_hub_download(\n",
        "    repo_id=\"sayakpaul/ucf101-subset\",\n",
        "    filename=\"UCF101_subset.tar.gz\",\n",
        "    repo_type=\"dataset\"\n",
        ")\n",
        "# file_path는 다운로드한 압축파일이 존재하는 경로와 파일명이 string으로 저장되어있음.\n",
        "with tarfile.open(file_path) as t:\n",
        "     t.extractall(\"./data\") # 여기에서 \".\"은 현재폴더라는 의미"
      ],
      "id": "cell-138"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "└── UCF101_subset\n",
            "    ├── test\n",
            "    │   ├── ApplyEyeMakeup\n",
            "    │   │   ├── UCF101\n",
            "    │   │   ├── v_ApplyEyeMakeup_g03_c01.avi\n",
            "    │   │   └── ...\n",
            "    │   │   └── v_ApplyEyeMakeup_g23_c06.avi\n",
            "    │   ├── ApplyLipstick\n",
            "    │   │   ├── UCF101\n",
            "    │   │   ├── v_ApplyLipstick_g14_c01.avi\n",
            "    │   │   └── ...\n",
            "    │   │   └── v_ApplyLipstick_g16_c04.avi\n",
            "    │   └── ...\n",
            "    │   └── BenchPress\n",
            "    │       ├── UCF101\n",
            "    │       ├── v_BenchPress_g05_c02.avi\n",
            "    │       └── ...\n",
            "    │       └── v_BenchPress_g25_c06.avi\n",
            "    ├── train\n",
            "    │   ├── ApplyEyeMakeup\n",
            "    │   │   ├── UCF101\n",
            "    │   │   ├── v_ApplyEyeMakeup_g02_c03.avi\n",
            "    │   │   └── ...\n",
            "    │   │   └── v_ApplyEyeMakeup_g25_c07.avi\n",
            "    │   ├── ApplyLipstick\n",
            "    │   │   ├── UCF101\n",
            "    │   │   ├── v_ApplyLipstick_g01_c02.avi\n",
            "    │   │   └── ...\n",
            "    │   │   └── v_ApplyLipstick_g24_c05.avi\n",
            "    │   └── ...\n",
            "    │   └── BenchPress\n",
            "    │       ├── UCF101\n",
            "    │       ├── v_BenchPress_g01_c05.avi\n",
            "    │       └── ...\n",
            "    │       └── v_BenchPress_g24_c05.avi\n",
            "    └── val\n",
            "        ├── ApplyEyeMakeup\n",
            "        │   ├── UCF101\n",
            "        │   ├── v_ApplyEyeMakeup_g01_c01.avi\n",
            "        │   ├── v_ApplyEyeMakeup_g14_c05.avi\n",
            "        │   └── v_ApplyEyeMakeup_g20_c04.avi\n",
            "        ├── ApplyLipstick\n",
            "        │   ├── UCF101\n",
            "        │   ├── v_ApplyLipstick_g10_c04.avi\n",
            "        │   ├── v_ApplyLipstick_g20_c04.avi\n",
            "        │   └── v_ApplyLipstick_g25_c02.avi\n",
            "        └── ...\n",
            "        └── BenchPress\n",
            "            ├── UCF101\n",
            "            ├── v_BenchPress_g11_c05.avi\n",
            "            ├── v_BenchPress_g17_c02.avi\n",
            "            └── v_BenchPress_g17_c06.avi"
          ]
        }
      ],
      "source": [
        "mp.tree(\"./data\")"
      ],
      "id": "cell-139"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "video_path = \"./data/UCF101_subset/test/BenchPress/v_BenchPress_g05_c02.avi\"\n",
        "video = pytorchvideo.data.encoded_video.EncodedVideo.from_path(video_path).get_clip(0, float('inf'))['video']\n",
        "video.shape"
      ],
      "id": "cell-140"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이)`"
      ],
      "id": "3fde2a4b-b06f-40cc-855b-2b8245006037"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "model3(\n",
        "    #video.permute(1,0,2,3)[:16,:,:224,:224].unsqueeze(0)\n",
        "    #video.permute(1,0,2,3)[:16,:,:224,:224].reshape(1,16,3,224,224)\n",
        "    torch.stack([video.permute(1,0,2,3)[:16,:,:224,:224]])\n",
        ")"
      ],
      "id": "cell-142"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "hf",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  }
}