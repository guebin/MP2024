{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 09wk-2: `model`의 입력, `model`의 사용연습\n",
        "\n",
        "최규빈  \n",
        "2024-11-09\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/guebin/MP2024/blob/main/posts/09wk-2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>\n",
        "\n",
        "# 1. 강의영상\n",
        "\n",
        "# 2. Imports"
      ],
      "id": "2ca76e79-4778-4206-b756-e8c3456fbbcc"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "import datasets\n",
        "import huggingface_hub\n",
        "import torch\n",
        "import torchvision\n",
        "import pytorchvideo.data\n",
        "import PIL\n",
        "import tarfile\n",
        "import mp2024pkg as mp"
      ],
      "id": "cell-4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Model 입력\n",
        "\n",
        "`-` 아래중 하나의 방법순으로..\n",
        "\n",
        "-   방법1: `model.forward?` 에서 시그니처를 확인\n",
        "-   방법2: `model.forward?` 에서 사용예제를 확인\n",
        "-   방법3: 인터넷을 활용한 외부 자료 확인 (공식문서, 공식튜토리얼,\n",
        "    신뢰할만한 블로그, ChatGPT등)\n",
        "-   방법4: `model.forward??` 를 보고 모든 코드를 뜯어봄 \\<— 하지마세요\n",
        "\n",
        "`-` 모델의 입력이 어떤형태로 정리되어야 하는지 알아내는 확실한 방법은\n",
        "없음\n",
        "\n",
        "-   방법1,2,3 은 다른사람의 호의에 기대해야함.\n",
        "-   방법4는 사실상 불가능\n",
        "\n",
        "`# 예제1` – 텍스트분류"
      ],
      "id": "10218377-440d-47b0-b47a-84b5d7630b5d"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "model1 = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\", num_labels=2\n",
        ")"
      ],
      "id": "cell-9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*모델의 기본정보(config)*"
      ],
      "id": "15cdc8fb-e781-4e72-b482-60ef5bdae910"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1.config"
      ],
      "id": "cell-11"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   max_position_embeddings: 512\n",
        "\n",
        "*모델의 입력파악*"
      ],
      "id": "c30b87a9-9a1a-40b5-8755-318faa5bd87b"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Signature:\n",
            "model1.forward(\n",
            "    input_ids: Optional[torch.Tensor] = None,\n",
            "    attention_mask: Optional[torch.Tensor] = None,\n",
            "    head_mask: Optional[torch.Tensor] = None,\n",
            "    inputs_embeds: Optional[torch.Tensor] = None,\n",
            "    labels: Optional[torch.LongTensor] = None,\n",
            "    output_attentions: Optional[bool] = None,\n",
            "    output_hidden_states: Optional[bool] = None,\n",
            "    return_dict: Optional[bool] = None,\n",
            ") -> Union[transformers.modeling_outputs.SequenceClassifierOutput, Tuple[torch.Tensor, ...]]\n",
            "Docstring:\n",
            "The [`DistilBertForSequenceClassification`] forward method, overrides the `__call__` special method.\n",
            "\n",
            "<Tip>\n",
            "\n",
            "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
            "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
            "the latter silently ignores them.\n",
            "\n",
            "</Tip>\n",
            "\n",
            "Args:\n",
            "    input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
            "        Indices of input sequence tokens in the vocabulary.\n",
            "\n",
            "        Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
            "        [`PreTrainedTokenizer.__call__`] for details.\n",
            "\n",
            "        [What are input IDs?](../glossary#input-ids)\n",
            "    attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
            "        Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
            "\n",
            "        - 1 for tokens that are **not masked**,\n",
            "        - 0 for tokens that are **masked**.\n",
            "\n",
            "        [What are attention masks?](../glossary#attention-mask)\n",
            "    head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
            "        Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
            "\n",
            "        - 1 indicates the head is **not masked**,\n",
            "        - 0 indicates the head is **masked**.\n",
            "\n",
            "    inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
            "        Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
            "        is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
            "        model's internal embedding lookup matrix.\n",
            "    output_attentions (`bool`, *optional*):\n",
            "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
            "        tensors for more detail.\n",
            "    output_hidden_states (`bool`, *optional*):\n",
            "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
            "        more detail.\n",
            "    return_dict (`bool`, *optional*):\n",
            "        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            "\n",
            "    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
            "        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
            "        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
            "        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
            "    \n",
            "Returns:\n",
            "    [`transformers.modeling_outputs.SequenceClassifierOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.SequenceClassifierOutput`] or a tuple of\n",
            "    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
            "    elements depending on the configuration ([`DistilBertConfig`]) and inputs.\n",
            "\n",
            "    - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Classification (or regression if config.num_labels==1) loss.\n",
            "    - **logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) -- Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
            "    - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
            "      one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
            "\n",
            "      Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
            "    - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
            "      sequence_length)`.\n",
            "\n",
            "      Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
            "      heads.\n",
            "\n",
            "Example of single-label classification:\n",
            "\n",
            "```python\n",
            ">>> import torch\n",
            ">>> from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
            "\n",
            ">>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
            ">>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
            "\n",
            ">>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
            "\n",
            ">>> with torch.no_grad():\n",
            "...     logits = model(**inputs).logits\n",
            "\n",
            ">>> predicted_class_id = logits.argmax().item()\n",
            "\n",
            ">>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
            ">>> num_labels = len(model.config.id2label)\n",
            ">>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
            "\n",
            ">>> labels = torch.tensor([1])\n",
            ">>> loss = model(**inputs, labels=labels).loss\n",
            "```\n",
            "\n",
            "Example of multi-label classification:\n",
            "\n",
            "```python\n",
            ">>> import torch\n",
            ">>> from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
            "\n",
            ">>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
            ">>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", problem_type=\"multi_label_classification\")\n",
            "\n",
            ">>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
            "\n",
            ">>> with torch.no_grad():\n",
            "...     logits = model(**inputs).logits\n",
            "\n",
            ">>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
            "\n",
            ">>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
            ">>> num_labels = len(model.config.id2label)\n",
            ">>> model = DistilBertForSequenceClassification.from_pretrained(\n",
            "...     \"distilbert-base-uncased\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
            "... )\n",
            "\n",
            ">>> labels = torch.sum(\n",
            "...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n",
            "... ).to(torch.float)\n",
            ">>> loss = model(**inputs, labels=labels).loss\n",
            "```\n",
            "File:      ~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py\n",
            "Type:      method"
          ]
        }
      ],
      "source": [
        "model1.forward? "
      ],
      "id": "cell-14"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시1 – 입력나열, loss O*"
      ],
      "id": "5efbde9f-114d-414e-b706-84db671da84e"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(\n",
        "    input_ids = torch.tensor([[1,2,3,4], [2,3,4,5]]),\n",
        "    labels = torch.tensor([0,0])\n",
        ")"
      ],
      "id": "cell-16"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시2 – `**딕셔너리`, loss O*"
      ],
      "id": "df724839-7358-48ac-bb96-a2790d80ffff"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1_input = dict(\n",
        "    input_ids = torch.tensor([[1,2,3,4], [2,3,4,5]]),\n",
        "    labels = torch.tensor([0,0])\n",
        ")\n",
        "model1(**model1_input)"
      ],
      "id": "cell-18"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시3 – 입력나열, loss X*"
      ],
      "id": "1f664c4c-5a7e-4b4e-8247-745e91ce77a6"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(\n",
        "    input_ids = torch.tensor([[1,2,3,4], [2,3,4,5]])\n",
        ")"
      ],
      "id": "cell-20"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시4 – `**딕셔너리`, loss X*"
      ],
      "id": "b7ef0d56-2663-4ce6-a232-7b0bedf5b85d"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1_input = dict(\n",
        "    input_ids = torch.tensor([[1,2,3,4], [2,3,4,5]])\n",
        ")\n",
        "model1(**model1_input)"
      ],
      "id": "cell-22"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시5 – 초간단, loss X*"
      ],
      "id": "ea61184b-e5bd-424c-8093-34b08c2944fa"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(torch.tensor([[1,2,3,4], [2,3,4,5]]))"
      ],
      "id": "cell-24"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> 사용예시1~5에서 `model1()` 대신에 `model1.forward()`를 사용해도 된다.\n",
        "\n",
        "`#`\n",
        "\n",
        "`# 예제2` – 이미지분류"
      ],
      "id": "3fba560b-087e-4372-8d8e-b2b99d757c70"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "model2 = transformers.AutoModelForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\",\n",
        "    num_labels=3 # 그냥 대충 3이라고 했음.. 별 이유는 없음\n",
        ")"
      ],
      "id": "cell-28"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*모델의 기본정보(config)*"
      ],
      "id": "a2a4ce94-3ea8-4c62-b439-b6191696aae8"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2.config"
      ],
      "id": "cell-30"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   image_size: 224"
      ],
      "id": "b5d05166-1d30-4324-87d9-9d10629320d8"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "mp.tab(model2.config)"
      ],
      "id": "cell-32"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2.config.num_channels"
      ],
      "id": "cell-33"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*모델의 입력파악*"
      ],
      "id": "1a16d83d-f44d-4fe6-8518-09f0ece3f337"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.randn(2,3,64,64)"
      ],
      "id": "cell-35"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시1 – 입력나열, loss O*"
      ],
      "id": "b691ba3c-e14e-4146-8b85-e00c315b565b"
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model2(\n",
        "    pixel_values = torch.randn(2,3,224,224),\n",
        "    labels = torch.tensor([0,1])\n",
        ")"
      ],
      "id": "cell-37"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시2 – `**딕셔너리`, loss O*"
      ],
      "id": "52c3320b-64e4-483e-a7c1-fd2d9516863e"
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model2_input = dict(\n",
        "    pixel_values = torch.randn(2,3,224,224),\n",
        "    labels = torch.tensor([0,1])\n",
        ")\n",
        "model2(**model2_input)"
      ],
      "id": "cell-39"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시3 – 입력나열, loss X*"
      ],
      "id": "b738c78c-7113-4962-a039-47968ba9851c"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model2(\n",
        "    pixel_values = torch.randn(2,3,224,224)\n",
        ")"
      ],
      "id": "cell-41"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시4 – `**딕셔너리`, loss X*"
      ],
      "id": "95c1cda2-3a23-4b68-8803-612785c249fa"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model2_input = dict(\n",
        "    pixel_values = torch.randn(2,3,224,224),\n",
        ")\n",
        "model2(**model2_input)"
      ],
      "id": "cell-43"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시5 – 초간단, loss X*"
      ],
      "id": "5c716f0b-0bc2-4961-bc83-4a89a608ad57"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model2(torch.randn(2,3,224,224))"
      ],
      "id": "cell-45"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "`# 예제3` – 동영상분류"
      ],
      "id": "edb7f9ce-091e-46aa-81a6-6ccf64734666"
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "model3 = transformers.VideoMAEForVideoClassification.from_pretrained(\n",
        "    \"MCG-NJU/videomae-base\",\n",
        ")"
      ],
      "id": "cell-48"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*모델의 기본정보(config)*"
      ],
      "id": "f3bc0dc3-1b2b-4961-bb97-81221ce583f2"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "model3.config"
      ],
      "id": "cell-50"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   image_size: 224\n",
        "-   num_frames: 16\n",
        "\n",
        "*모델의 입력파악*"
      ],
      "id": "3a0807f0-1e20-4e30-b859-add41a4cdf00"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Signature:\n",
            "model3.forward(\n",
            "    pixel_values: Optional[torch.Tensor] = None,\n",
            "    head_mask: Optional[torch.Tensor] = None,\n",
            "    labels: Optional[torch.Tensor] = None,\n",
            "    output_attentions: Optional[bool] = None,\n",
            "    output_hidden_states: Optional[bool] = None,\n",
            "    return_dict: Optional[bool] = None,\n",
            ") -> Union[Tuple, transformers.modeling_outputs.ImageClassifierOutput]\n",
            "Docstring:\n",
            "The [`VideoMAEForVideoClassification`] forward method, overrides the `__call__` special method.\n",
            "\n",
            "<Tip>\n",
            "\n",
            "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
            "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
            "the latter silently ignores them.\n",
            "\n",
            "</Tip>\n",
            "\n",
            "Args:\n",
            "    pixel_values (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, height, width)`):\n",
            "        Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n",
            "        [`VideoMAEImageProcessor.__call__`] for details.\n",
            "\n",
            "    head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
            "        Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
            "\n",
            "        - 1 indicates the head is **not masked**,\n",
            "        - 0 indicates the head is **masked**.\n",
            "\n",
            "    output_attentions (`bool`, *optional*):\n",
            "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
            "        tensors for more detail.\n",
            "    output_hidden_states (`bool`, *optional*):\n",
            "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
            "        more detail.\n",
            "    return_dict (`bool`, *optional*):\n",
            "        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            "\n",
            "    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
            "        Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n",
            "        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
            "        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
            "\n",
            "\n",
            "    Returns:\n",
            "        [`transformers.modeling_outputs.ImageClassifierOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.ImageClassifierOutput`] or a tuple of\n",
            "        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
            "        elements depending on the configuration ([`VideoMAEConfig`]) and inputs.\n",
            "\n",
            "        - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Classification (or regression if config.num_labels==1) loss.\n",
            "        - **logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) -- Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
            "        - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
            "          one for the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states\n",
            "          (also called feature maps) of the model at the output of each stage.\n",
            "        - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, patch_size,\n",
            "          sequence_length)`.\n",
            "\n",
            "          Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
            "          heads.\n",
            "  \n",
            "\n",
            "    Examples:\n",
            "\n",
            "    ```python\n",
            "    >>> import av\n",
            "    >>> import torch\n",
            "    >>> import numpy as np\n",
            "\n",
            "    >>> from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n",
            "    >>> from huggingface_hub import hf_hub_download\n",
            "\n",
            "    >>> np.random.seed(0)\n",
            "\n",
            "\n",
            "    >>> def read_video_pyav(container, indices):\n",
            "    ...     '''\n",
            "    ...     Decode the video with PyAV decoder.\n",
            "    ...     Args:\n",
            "    ...         container (`av.container.input.InputContainer`): PyAV container.\n",
            "    ...         indices (`List[int]`): List of frame indices to decode.\n",
            "    ...     Returns:\n",
            "    ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
            "    ...     '''\n",
            "    ...     frames = []\n",
            "    ...     container.seek(0)\n",
            "    ...     start_index = indices[0]\n",
            "    ...     end_index = indices[-1]\n",
            "    ...     for i, frame in enumerate(container.decode(video=0)):\n",
            "    ...         if i > end_index:\n",
            "    ...             break\n",
            "    ...         if i >= start_index and i in indices:\n",
            "    ...             frames.append(frame)\n",
            "    ...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
            "\n",
            "\n",
            "    >>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
            "    ...     '''\n",
            "    ...     Sample a given number of frame indices from the video.\n",
            "    ...     Args:\n",
            "    ...         clip_len (`int`): Total number of frames to sample.\n",
            "    ...         frame_sample_rate (`int`): Sample every n-th frame.\n",
            "    ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n",
            "    ...     Returns:\n",
            "    ...         indices (`List[int]`): List of sampled frame indices\n",
            "    ...     '''\n",
            "    ...     converted_len = int(clip_len * frame_sample_rate)\n",
            "    ...     end_idx = np.random.randint(converted_len, seg_len)\n",
            "    ...     start_idx = end_idx - converted_len\n",
            "    ...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
            "    ...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
            "    ...     return indices\n",
            "\n",
            "\n",
            "    >>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n",
            "    >>> file_path = hf_hub_download(\n",
            "    ...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
            "    ... )\n",
            "    >>> container = av.open(file_path)\n",
            "\n",
            "    >>> # sample 16 frames\n",
            "    >>> indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
            "    >>> video = read_video_pyav(container, indices)\n",
            "\n",
            "    >>> image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n",
            "    >>> model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n",
            "\n",
            "    >>> inputs = image_processor(list(video), return_tensors=\"pt\")\n",
            "\n",
            "    >>> with torch.no_grad():\n",
            "    ...     outputs = model(**inputs)\n",
            "    ...     logits = outputs.logits\n",
            "\n",
            "    >>> # model predicts one of the 400 Kinetics-400 classes\n",
            "    >>> predicted_label = logits.argmax(-1).item()\n",
            "    >>> print(model.config.id2label[predicted_label])\n",
            "    eating spaghetti\n",
            "    ```\n",
            "File:      ~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/videomae/modeling_videomae.py\n",
            "Type:      method"
          ]
        }
      ],
      "source": [
        "model3.forward?"
      ],
      "id": "cell-53"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시1 – 입력나열, loss O*"
      ],
      "id": "ec4d30c4-cd99-4f46-ba7e-7df8a07891d1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model3(\n",
        "    pixel_values = torch.randn(4,16,3,224,224),\n",
        "    labels = torch.tensor([0,1,0,1])\n",
        ")"
      ],
      "id": "cell-55"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시2 – `**딕셔너리`, loss O*"
      ],
      "id": "f4dc1e6c-84ab-4d1b-a7a7-bd547b42a4b7"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model3_input = dict(\n",
        "    pixel_values = torch.randn(4,16,3,224,224),\n",
        "    labels = torch.tensor([0,1,0,1])\n",
        ")\n",
        "model3(**model3_input)"
      ],
      "id": "cell-57"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시3 – 입력나열, loss X*"
      ],
      "id": "5c1079da-4a57-4c09-883d-aade0ef23d66"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model3(\n",
        "    pixel_values = torch.randn(4,16,3,224,224)\n",
        ")"
      ],
      "id": "cell-59"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시4 – `**딕셔너리`, loss X*"
      ],
      "id": "135eb7d9-30bb-4e6d-93c4-bf54dfba4a44"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model3_input = dict(\n",
        "    pixel_values = torch.randn(4,16,3,224,224),\n",
        ")\n",
        "model3(**model3_input)"
      ],
      "id": "cell-61"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시5 – 초간단, loss X*"
      ],
      "id": "f280bfd9-ee06-4d92-99d1-e5aad6c1bc37"
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model3(torch.randn(4,16,3,224,224))"
      ],
      "id": "cell-63"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "# 4. Model 사용 연습\n",
        "\n",
        "## A. 텍스트"
      ],
      "id": "89915081-61f7-44e6-aff4-4bbf6c0e2be0"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "model1 = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\", num_labels=2\n",
        ")\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
      ],
      "id": "cell-67"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`# 예제1` – imdb"
      ],
      "id": "68017166-a101-43ee-9fab-682f05f9b152"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "imdb = datasets.load_dataset('imdb')"
      ],
      "id": "cell-69"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = imdb['train'].select(range(3))\n",
        "d"
      ],
      "id": "cell-70"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이1)`\n",
        "\n",
        "*실패*"
      ],
      "id": "f3a8279d-960c-43f5-826f-57c902149e3b"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1.forward(torch.tensor(tokenizer(d['text'])['input_ids']))"
      ],
      "id": "cell-73"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*원인분석*"
      ],
      "id": "cae06fe0-3bd9-4e4c-8e36-171a350d0fcb"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level 1 - Type: list, Length: 3, Content: [[101, 1045, 12524, 1045, ... // ... , 7987, 1013, 1028, 102]]\n",
            "     Level 2 - Type: list, Length: 363, Content: [101, 1045, 12524, 1045,  ... // ... 7, 1037, 5436, 1012, 102]\n",
            "     Level 2 - Type: list, Length: 304, Content: [101, 1000, 1045, 2572, 8 ... // ... 5, 1055, 4230, 1012, 102]\n",
            "     Level 2 - Type: list, Length: 133, Content: [101, 2065, 2069, 2000, 4 ... // ... 6, 7987, 1013, 1028, 102]"
          ]
        }
      ],
      "source": [
        "mp.show_list(\n",
        "    tokenizer(d['text'])['input_ids']\n",
        ")"
      ],
      "id": "cell-75"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level 1 - Type: list, Length: 3, Content: [[101, 1045, 12524, 1045, ... // ...  0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "     Level 2 - Type: list, Length: 363, Content: [101, 1045, 12524, 1045,  ... // ... 7, 1037, 5436, 1012, 102]\n",
            "     Level 2 - Type: list, Length: 363, Content: [101, 1000, 1045, 2572, 8 ... // ... , 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "     Level 2 - Type: list, Length: 363, Content: [101, 2065, 2069, 2000, 4 ... // ... , 0, 0, 0, 0, 0, 0, 0, 0]"
          ]
        }
      ],
      "source": [
        "mp.show_list(\n",
        "    tokenizer(d['text'], padding=True)['input_ids']\n",
        ")"
      ],
      "id": "cell-76"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*성공*"
      ],
      "id": "9ddaa33b-ec67-45d2-a182-955ece8edb5e"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(torch.tensor(tokenizer(d['text'], padding=True)['input_ids']))"
      ],
      "id": "cell-78"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이2)`"
      ],
      "id": "d8941978-bbf0-4b35-9928-d99fd7c391a3"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(tokenizer(d['text'], padding=True, return_tensors=\"pt\")['input_ids'])"
      ],
      "id": "cell-80"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "`# 예제2` – emotion"
      ],
      "id": "9b298a85-8b18-4fd2-b98e-9c6a755ef04f"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "emotion = datasets.load_dataset('emotion')"
      ],
      "id": "cell-83"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = emotion['train'].select(range(3))\n",
        "d"
      ],
      "id": "cell-84"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이)`"
      ],
      "id": "1faa3094-4f0b-4bca-915d-12bebbe90eee"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(torch.tensor(tokenizer(d['text'],padding=True)['input_ids']))"
      ],
      "id": "cell-86"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "`# 예제3` – MBTI"
      ],
      "id": "9739e79a-68bc-4887-89e5-adc694d17354"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "    - Avoid using `tokenizers` before the fork if possible\n",
            "    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-10 01:07:26--  https://raw.githubusercontent.com/guebin/MP2024/refs/heads/main/posts/mbti_1.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62856486 (60M) [text/plain]\n",
            "Saving to: ‘mbti_1.csv’\n",
            "\n",
            "mbti_1.csv          100%[===================>]  59.94M   108MB/s    in 0.6s    \n",
            "\n",
            "2024-11-10 01:07:31 (108 MB/s) - ‘mbti_1.csv’ saved [62856486/62856486]\n"
          ]
        }
      ],
      "source": [
        "# mbti1.csv 파일 다운로드 \n",
        "!wget https://raw.githubusercontent.com/guebin/MP2024/refs/heads/main/posts/mbti_1.csv"
      ],
      "id": "cell-89"
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = datasets.Dataset.from_csv(\"mbti_1.csv\").select(range(3))\n",
        "d"
      ],
      "id": "cell-90"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이1)`\n",
        "\n",
        "*실패*"
      ],
      "id": "9713d5a3-91ca-4226-a9fa-1d63f632b47f"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(torch.tensor(tokenizer(d['posts'],padding=True)['input_ids']))"
      ],
      "id": "cell-93"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*원인분석*"
      ],
      "id": "cc51f2c4-5213-4194-b6ac-330678ac34ce"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level 1 - Type: list, Length: 3, Content: [[101, 1005, 8299, 1024,  ... // ...  0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "     Level 2 - Type: list, Length: 2102, Content: [101, 1005, 8299, 1024, 1 ... // ... , 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "     Level 2 - Type: list, Length: 2102, Content: [101, 1005, 1045, 1005, 1 ... // ... 2, 1012, 1012, 1005, 102]\n",
            "     Level 2 - Type: list, Length: 2102, Content: [101, 1005, 2204, 2028, 1 ... // ... , 0, 0, 0, 0, 0, 0, 0, 0]"
          ]
        }
      ],
      "source": [
        "mp.show_list(\n",
        "    tokenizer(d['posts'],padding=True)['input_ids']\n",
        ")"
      ],
      "id": "cell-95"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level 1 - Type: list, Length: 3, Content: [[101, 1005, 8299, 1024,  ... // ... , 7834, 1012, 2077, 102]]\n",
            "     Level 2 - Type: list, Length: 512, Content: [101, 1005, 8299, 1024, 1 ... // ... 1, 4127, 2017, 2215, 102]\n",
            "     Level 2 - Type: list, Length: 512, Content: [101, 1005, 1045, 1005, 1 ... // ... 6, 2600, 3259, 2028, 102]\n",
            "     Level 2 - Type: list, Length: 512, Content: [101, 1005, 2204, 2028, 1 ... // ... 0, 7834, 1012, 2077, 102]"
          ]
        }
      ],
      "source": [
        "mp.show_list(\n",
        "    tokenizer(d['posts'],truncation=True)['input_ids']\n",
        ")"
      ],
      "id": "cell-96"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*성공*"
      ],
      "id": "ba57052f-389a-4152-aa05-0a66d2dffe8f"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(torch.tensor(tokenizer(d['posts'],truncation=True)['input_ids']))\n",
        "#model1(tokenizer(d['posts'],truncation=True,return_tensors=\"pt\")['input_ids'])"
      ],
      "id": "cell-98"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이2)` *–모델설정변경 (퀴즈5, 모델의 프레임수를 4로 바꾸는 예제에서\n",
        "사용한 테크닉)*\n",
        "\n",
        "*distilbert/distilbert-base-uncased 설정값 부르기*"
      ],
      "id": "c0cf54b4-8d0d-41f2-8d3b-a7a35f5207c3"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = transformers.AutoConfig.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\"\n",
        ")\n",
        "config"
      ],
      "id": "cell-101"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*설정값변경*"
      ],
      "id": "2542dbda-7e77-4305-b19f-b7e852ac32d8"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "config.max_position_embeddings = 2200"
      ],
      "id": "cell-103"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*설정값으로 모델불러오기*"
      ],
      "id": "ff8e2c51-49d3-4bb6-9422-a0bd6dad5c54"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1_large = transformers.AutoModelForSequenceClassification.from_config(\n",
        "    config=config\n",
        ")"
      ],
      "id": "cell-105"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*모델사용*"
      ],
      "id": "d41438a5-d915-42b0-b340-fdf880dfbb67"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1_large(torch.tensor(tokenizer(d['posts'],padding=True)['input_ids']))"
      ],
      "id": "cell-107"
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1_large(**tokenizer(d['posts'],padding=True,return_tensors=\"pt\"))"
      ],
      "id": "cell-108"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "`# 예제4` – sms_spam"
      ],
      "id": "134af468-95ae-4cd4-8df3-8f02a290d811"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "sms_spam = datasets.load_dataset('sms_spam')['train'].train_test_split(test_size=0.2, seed=42)\n",
        "sms_spam"
      ],
      "id": "cell-111"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = sms_spam['train'].select(range(3))\n",
        "d"
      ],
      "id": "cell-112"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이)`"
      ],
      "id": "a060a666-dfa8-473f-910a-d5cb0c8164d2"
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(**tokenizer(d['sms'],padding=True,return_tensors=\"pt\"))"
      ],
      "id": "cell-114"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "## B. 이미지"
      ],
      "id": "8bc3b4c7-36bc-41e8-8699-148eade04a62"
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "model2 = transformers.AutoModelForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\",\n",
        "    num_labels=3 # 그냥 대충 3이라고 했음.. 별 이유는 없음\n",
        ")"
      ],
      "id": "cell-117"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`# 예제1` – food101"
      ],
      "id": "12f2696d-115e-4ed0-bfe7-b31aed7c759c"
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = datasets.load_dataset(\"food101\", split=\"train[:4]\")\n",
        "d"
      ],
      "id": "cell-119"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(예비학습)` – `torchvision.transforms` 에서 제공하는 기능들은\n",
        "배치처리가 가능한가?"
      ],
      "id": "4ae8f6f5-7e45-414e-9dec-c23352f17693"
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [],
      "source": [
        "to_tensor = torchvision.transforms.ToTensor()"
      ],
      "id": "cell-121"
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [],
      "source": [
        "to_tensor(d['image'][0])"
      ],
      "id": "cell-122"
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [],
      "source": [
        "to_tensor(d['image'])"
      ],
      "id": "cell-123"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이)`"
      ],
      "id": "b1ed0270-3844-4124-a316-80e9cea72830"
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [],
      "source": [
        "compose = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Resize((224,224))\n",
        "])"
      ],
      "id": "cell-125"
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.stack(list(map(compose,d['image'])),axis=0).shape"
      ],
      "id": "cell-126"
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2.forward(\n",
        "    torch.stack(list(map(compose,d['image'])),axis=0)\n",
        ")"
      ],
      "id": "cell-127"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "`# 예제2`"
      ],
      "id": "458710dd-4427-4850-8c90-c99a46efdb8a"
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [],
      "source": [
        "beans = datasets.load_dataset('beans')\n",
        "d = beans['train'].select(range(4))\n",
        "d"
      ],
      "id": "cell-130"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이)`"
      ],
      "id": "3eb56d48-255e-4a42-a151-cc17e34a0e2e"
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2(torch.stack(list(map(compose,d['image'])),axis=0))"
      ],
      "id": "cell-132"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "## C. 동영상"
      ],
      "id": "bcc9318e-3eb7-4136-864a-c9e0868f69fa"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "model3 = transformers.VideoMAEForVideoClassification.from_pretrained(\n",
        "    \"MCG-NJU/videomae-base\",\n",
        ")"
      ],
      "id": "cell-135"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`# 예제1` – UCF101_subset"
      ],
      "id": "09bc7b7b-9713-4816-b9f0-8fe81f82a3bd"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = huggingface_hub.hf_hub_download(\n",
        "    repo_id=\"sayakpaul/ucf101-subset\",\n",
        "    filename=\"UCF101_subset.tar.gz\",\n",
        "    repo_type=\"dataset\"\n",
        ")\n",
        "# file_path는 다운로드한 압축파일이 존재하는 경로와 파일명이 string으로 저장되어있음.\n",
        "with tarfile.open(file_path) as t:\n",
        "     t.extractall(\"./data\") # 여기에서 \".\"은 현재폴더라는 의미"
      ],
      "id": "cell-137"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "└── UCF101_subset\n",
            "    ├── test\n",
            "    │   ├── ApplyEyeMakeup\n",
            "    │   │   ├── UCF101\n",
            "    │   │   ├── v_ApplyEyeMakeup_g03_c01.avi\n",
            "    │   │   └── ...\n",
            "    │   │   └── v_ApplyEyeMakeup_g23_c06.avi\n",
            "    │   ├── ApplyLipstick\n",
            "    │   │   ├── UCF101\n",
            "    │   │   ├── v_ApplyLipstick_g14_c01.avi\n",
            "    │   │   └── ...\n",
            "    │   │   └── v_ApplyLipstick_g16_c04.avi\n",
            "    │   └── ...\n",
            "    │   └── BenchPress\n",
            "    │       ├── UCF101\n",
            "    │       ├── v_BenchPress_g05_c02.avi\n",
            "    │       └── ...\n",
            "    │       └── v_BenchPress_g25_c06.avi\n",
            "    ├── train\n",
            "    │   ├── ApplyEyeMakeup\n",
            "    │   │   ├── UCF101\n",
            "    │   │   ├── v_ApplyEyeMakeup_g02_c03.avi\n",
            "    │   │   └── ...\n",
            "    │   │   └── v_ApplyEyeMakeup_g25_c07.avi\n",
            "    │   ├── ApplyLipstick\n",
            "    │   │   ├── UCF101\n",
            "    │   │   ├── v_ApplyLipstick_g01_c02.avi\n",
            "    │   │   └── ...\n",
            "    │   │   └── v_ApplyLipstick_g24_c05.avi\n",
            "    │   └── ...\n",
            "    │   └── BenchPress\n",
            "    │       ├── UCF101\n",
            "    │       ├── v_BenchPress_g01_c05.avi\n",
            "    │       └── ...\n",
            "    │       └── v_BenchPress_g24_c05.avi\n",
            "    └── val\n",
            "        ├── ApplyEyeMakeup\n",
            "        │   ├── UCF101\n",
            "        │   ├── v_ApplyEyeMakeup_g01_c01.avi\n",
            "        │   ├── v_ApplyEyeMakeup_g14_c05.avi\n",
            "        │   └── v_ApplyEyeMakeup_g20_c04.avi\n",
            "        ├── ApplyLipstick\n",
            "        │   ├── UCF101\n",
            "        │   ├── v_ApplyLipstick_g10_c04.avi\n",
            "        │   ├── v_ApplyLipstick_g20_c04.avi\n",
            "        │   └── v_ApplyLipstick_g25_c02.avi\n",
            "        └── ...\n",
            "        └── BenchPress\n",
            "            ├── UCF101\n",
            "            ├── v_BenchPress_g11_c05.avi\n",
            "            ├── v_BenchPress_g17_c02.avi\n",
            "            └── v_BenchPress_g17_c06.avi"
          ]
        }
      ],
      "source": [
        "mp.tree(\"./data\")"
      ],
      "id": "cell-138"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "video_path = \"./data/UCF101_subset/test/BenchPress/v_BenchPress_g05_c02.avi\"\n",
        "video = pytorchvideo.data.encoded_video.EncodedVideo.from_path(video_path).get_clip(0, float('inf'))['video']\n",
        "video.shape"
      ],
      "id": "cell-139"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이)`"
      ],
      "id": "3c058a84-d665-4cb0-8c72-85075a865e88"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "model3(\n",
        "    #video.permute(1,0,2,3)[:16,:,:224,:224].unsqueeze(0)\n",
        "    #video.permute(1,0,2,3)[:16,:,:224,:224].reshape(1,16,3,224,224)\n",
        "    torch.stack([video.permute(1,0,2,3)[:16,:,:224,:224]])\n",
        ")"
      ],
      "id": "cell-141"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "hf",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  }
}