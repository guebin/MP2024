{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 09wk-2: `model`의 입력, `model`의 사용연습\n",
        "\n",
        "최규빈  \n",
        "2024-11-09\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/guebin/MP2024/blob/main/posts/09wk-2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>\n",
        "\n",
        "# 1. 강의영상\n",
        "\n",
        "<https://youtu.be/playlist?list=PLQqh36zP38-xsRgsXpLEUNWClxLi0N9Mk&si=Lssxzw70RRT1adiJ>\n",
        "\n",
        "# 2. Imports"
      ],
      "id": "a725eec2-0e5f-4e4d-9eb4-ca8f0e35bedc"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "import datasets\n",
        "import huggingface_hub\n",
        "import torch\n",
        "import torchvision\n",
        "import pytorchvideo.data\n",
        "import PIL\n",
        "import tarfile\n",
        "import mp2024pkg as mp"
      ],
      "id": "cell-5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Model 입력\n",
        "\n",
        "`-` 아래중 하나의 방법순으로..\n",
        "\n",
        "-   방법1: `model.forward?` 에서 시그니처를 확인\n",
        "-   방법2: `model.forward?` 에서 사용예제를 확인\n",
        "-   방법3: 인터넷을 활용한 외부 자료 확인 (공식문서, 공식튜토리얼,\n",
        "    신뢰할만한 블로그, ChatGPT등)\n",
        "-   방법4: `model.forward??` 를 보고 모든 코드를 뜯어봄 \\<— 하지마세요\n",
        "\n",
        "`-` 모델의 입력이 어떤형태로 정리되어야 하는지 알아내는 확실한 방법은\n",
        "없음\n",
        "\n",
        "-   방법1,2,3 은 다른사람의 호의에 기대해야함.\n",
        "-   방법4는 사실상 불가능\n",
        "\n",
        "`# 예제1` – 텍스트분류"
      ],
      "id": "e902ebb7-1ab0-4fa4-b3a5-4e76a545b44e"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "model1 = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\", num_labels=2\n",
        ")"
      ],
      "id": "cell-10"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*모델의 기본정보(config)*"
      ],
      "id": "6343a182-d197-47ab-8126-63fbad3a591a"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1.config"
      ],
      "id": "cell-12"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   max_position_embeddings: 512\n",
        "\n",
        "*모델의 입력파악*"
      ],
      "id": "a9fa4970-ecb3-405a-9900-4322510df9b7"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Signature:\n",
            "model1.forward(\n",
            "    input_ids: Optional[torch.Tensor] = None,\n",
            "    attention_mask: Optional[torch.Tensor] = None,\n",
            "    head_mask: Optional[torch.Tensor] = None,\n",
            "    inputs_embeds: Optional[torch.Tensor] = None,\n",
            "    labels: Optional[torch.LongTensor] = None,\n",
            "    output_attentions: Optional[bool] = None,\n",
            "    output_hidden_states: Optional[bool] = None,\n",
            "    return_dict: Optional[bool] = None,\n",
            ") -> Union[transformers.modeling_outputs.SequenceClassifierOutput, Tuple[torch.Tensor, ...]]\n",
            "Docstring:\n",
            "The [`DistilBertForSequenceClassification`] forward method, overrides the `__call__` special method.\n",
            "\n",
            "<Tip>\n",
            "\n",
            "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
            "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
            "the latter silently ignores them.\n",
            "\n",
            "</Tip>\n",
            "\n",
            "Args:\n",
            "    input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
            "        Indices of input sequence tokens in the vocabulary.\n",
            "\n",
            "        Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
            "        [`PreTrainedTokenizer.__call__`] for details.\n",
            "\n",
            "        [What are input IDs?](../glossary#input-ids)\n",
            "    attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
            "        Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
            "\n",
            "        - 1 for tokens that are **not masked**,\n",
            "        - 0 for tokens that are **masked**.\n",
            "\n",
            "        [What are attention masks?](../glossary#attention-mask)\n",
            "    head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
            "        Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
            "\n",
            "        - 1 indicates the head is **not masked**,\n",
            "        - 0 indicates the head is **masked**.\n",
            "\n",
            "    inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
            "        Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
            "        is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
            "        model's internal embedding lookup matrix.\n",
            "    output_attentions (`bool`, *optional*):\n",
            "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
            "        tensors for more detail.\n",
            "    output_hidden_states (`bool`, *optional*):\n",
            "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
            "        more detail.\n",
            "    return_dict (`bool`, *optional*):\n",
            "        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            "\n",
            "    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
            "        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
            "        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
            "        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
            "    \n",
            "Returns:\n",
            "    [`transformers.modeling_outputs.SequenceClassifierOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.SequenceClassifierOutput`] or a tuple of\n",
            "    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
            "    elements depending on the configuration ([`DistilBertConfig`]) and inputs.\n",
            "\n",
            "    - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Classification (or regression if config.num_labels==1) loss.\n",
            "    - **logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) -- Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
            "    - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
            "      one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
            "\n",
            "      Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
            "    - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
            "      sequence_length)`.\n",
            "\n",
            "      Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
            "      heads.\n",
            "\n",
            "Example of single-label classification:\n",
            "\n",
            "```python\n",
            ">>> import torch\n",
            ">>> from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
            "\n",
            ">>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
            ">>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
            "\n",
            ">>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
            "\n",
            ">>> with torch.no_grad():\n",
            "...     logits = model(**inputs).logits\n",
            "\n",
            ">>> predicted_class_id = logits.argmax().item()\n",
            "\n",
            ">>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
            ">>> num_labels = len(model.config.id2label)\n",
            ">>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
            "\n",
            ">>> labels = torch.tensor([1])\n",
            ">>> loss = model(**inputs, labels=labels).loss\n",
            "```\n",
            "\n",
            "Example of multi-label classification:\n",
            "\n",
            "```python\n",
            ">>> import torch\n",
            ">>> from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
            "\n",
            ">>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
            ">>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", problem_type=\"multi_label_classification\")\n",
            "\n",
            ">>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
            "\n",
            ">>> with torch.no_grad():\n",
            "...     logits = model(**inputs).logits\n",
            "\n",
            ">>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
            "\n",
            ">>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
            ">>> num_labels = len(model.config.id2label)\n",
            ">>> model = DistilBertForSequenceClassification.from_pretrained(\n",
            "...     \"distilbert-base-uncased\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
            "... )\n",
            "\n",
            ">>> labels = torch.sum(\n",
            "...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n",
            "... ).to(torch.float)\n",
            ">>> loss = model(**inputs, labels=labels).loss\n",
            "```\n",
            "File:      ~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py\n",
            "Type:      method"
          ]
        }
      ],
      "source": [
        "model1.forward? "
      ],
      "id": "cell-15"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시1 – 입력나열, loss O*"
      ],
      "id": "10864f63-463e-4a8b-a20c-6547a49ff835"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(\n",
        "    input_ids = torch.tensor([[1,2,3,4], [2,3,4,5]]),\n",
        "    labels = torch.tensor([0,0])\n",
        ")"
      ],
      "id": "cell-17"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시2 – `**딕셔너리`, loss O*"
      ],
      "id": "77dba3c6-7c4b-490d-81b7-ab1329de6e9a"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1_input = dict(\n",
        "    input_ids = torch.tensor([[1,2,3,4], [2,3,4,5]]),\n",
        "    labels = torch.tensor([0,0])\n",
        ")\n",
        "model1(**model1_input)"
      ],
      "id": "cell-19"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시3 – 입력나열, loss X*"
      ],
      "id": "73b25305-13bd-4780-9f53-c9c5d133a560"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(\n",
        "    input_ids = torch.tensor([[1,2,3,4], [2,3,4,5]])\n",
        ")"
      ],
      "id": "cell-21"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시4 – `**딕셔너리`, loss X*"
      ],
      "id": "d977a0a6-cf11-46aa-a091-6375a8731db7"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1_input = dict(\n",
        "    input_ids = torch.tensor([[1,2,3,4], [2,3,4,5]])\n",
        ")\n",
        "model1(**model1_input)"
      ],
      "id": "cell-23"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시5 – 초간단, loss X*"
      ],
      "id": "808f2c70-2aae-448d-bc89-e9c654da3f6b"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(torch.tensor([[1,2,3,4], [2,3,4,5]]))"
      ],
      "id": "cell-25"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> 사용예시1~5에서 `model1()` 대신에 `model1.forward()`를 사용해도 된다.\n",
        "\n",
        "`#`\n",
        "\n",
        "`# 예제2` – 이미지분류"
      ],
      "id": "4740dd1b-3420-4c20-a656-1ab950c44863"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "model2 = transformers.AutoModelForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\",\n",
        "    num_labels=3 # 그냥 대충 3이라고 했음.. 별 이유는 없음\n",
        ")"
      ],
      "id": "cell-29"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*모델의 기본정보(config)*"
      ],
      "id": "1148f545-6bfc-4e9f-bfbf-737e237f0654"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2.config"
      ],
      "id": "cell-31"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   image_size: 224"
      ],
      "id": "f08d3a76-31c4-471d-ab1d-8f1dbec8f36f"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "mp.tab(model2.config)"
      ],
      "id": "cell-33"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2.config.num_channels"
      ],
      "id": "cell-34"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*모델의 입력파악*"
      ],
      "id": "a64818e0-0a8a-4bf9-9028-2fe75a60c36f"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.randn(2,3,64,64)"
      ],
      "id": "cell-36"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시1 – 입력나열, loss O*"
      ],
      "id": "fdad3008-da2f-46bf-b03d-fb49a0ab209b"
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model2(\n",
        "    pixel_values = torch.randn(2,3,224,224),\n",
        "    labels = torch.tensor([0,1])\n",
        ")"
      ],
      "id": "cell-38"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시2 – `**딕셔너리`, loss O*"
      ],
      "id": "76e77b1f-b6cb-4722-a317-7eacb1a87bb3"
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model2_input = dict(\n",
        "    pixel_values = torch.randn(2,3,224,224),\n",
        "    labels = torch.tensor([0,1])\n",
        ")\n",
        "model2(**model2_input)"
      ],
      "id": "cell-40"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시3 – 입력나열, loss X*"
      ],
      "id": "0b43efbb-6fb7-4527-864c-1d2c0f4d3012"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model2(\n",
        "    pixel_values = torch.randn(2,3,224,224)\n",
        ")"
      ],
      "id": "cell-42"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시4 – `**딕셔너리`, loss X*"
      ],
      "id": "9fbb9aa8-daad-4cf8-9168-a59d2938e067"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model2_input = dict(\n",
        "    pixel_values = torch.randn(2,3,224,224),\n",
        ")\n",
        "model2(**model2_input)"
      ],
      "id": "cell-44"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시5 – 초간단, loss X*"
      ],
      "id": "0995eb97-7f82-47d0-98a9-87c8455f9f4f"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model2(torch.randn(2,3,224,224))"
      ],
      "id": "cell-46"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "`# 예제3` – 동영상분류"
      ],
      "id": "4388fcb3-4fa8-4505-8ea1-bfadab55e754"
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "model3 = transformers.VideoMAEForVideoClassification.from_pretrained(\n",
        "    \"MCG-NJU/videomae-base\",\n",
        ")"
      ],
      "id": "cell-49"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*모델의 기본정보(config)*"
      ],
      "id": "06d337cb-35f9-4583-8ae3-d7bb05fee144"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "model3.config"
      ],
      "id": "cell-51"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   image_size: 224\n",
        "-   num_frames: 16\n",
        "\n",
        "*모델의 입력파악*"
      ],
      "id": "cc2bb67e-5890-44e3-80bc-46e795d1c714"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Signature:\n",
            "model3.forward(\n",
            "    pixel_values: Optional[torch.Tensor] = None,\n",
            "    head_mask: Optional[torch.Tensor] = None,\n",
            "    labels: Optional[torch.Tensor] = None,\n",
            "    output_attentions: Optional[bool] = None,\n",
            "    output_hidden_states: Optional[bool] = None,\n",
            "    return_dict: Optional[bool] = None,\n",
            ") -> Union[Tuple, transformers.modeling_outputs.ImageClassifierOutput]\n",
            "Docstring:\n",
            "The [`VideoMAEForVideoClassification`] forward method, overrides the `__call__` special method.\n",
            "\n",
            "<Tip>\n",
            "\n",
            "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
            "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
            "the latter silently ignores them.\n",
            "\n",
            "</Tip>\n",
            "\n",
            "Args:\n",
            "    pixel_values (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, height, width)`):\n",
            "        Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n",
            "        [`VideoMAEImageProcessor.__call__`] for details.\n",
            "\n",
            "    head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
            "        Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
            "\n",
            "        - 1 indicates the head is **not masked**,\n",
            "        - 0 indicates the head is **masked**.\n",
            "\n",
            "    output_attentions (`bool`, *optional*):\n",
            "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
            "        tensors for more detail.\n",
            "    output_hidden_states (`bool`, *optional*):\n",
            "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
            "        more detail.\n",
            "    return_dict (`bool`, *optional*):\n",
            "        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            "\n",
            "    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
            "        Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n",
            "        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
            "        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
            "\n",
            "\n",
            "    Returns:\n",
            "        [`transformers.modeling_outputs.ImageClassifierOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.ImageClassifierOutput`] or a tuple of\n",
            "        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
            "        elements depending on the configuration ([`VideoMAEConfig`]) and inputs.\n",
            "\n",
            "        - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Classification (or regression if config.num_labels==1) loss.\n",
            "        - **logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) -- Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
            "        - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
            "          one for the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states\n",
            "          (also called feature maps) of the model at the output of each stage.\n",
            "        - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, patch_size,\n",
            "          sequence_length)`.\n",
            "\n",
            "          Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
            "          heads.\n",
            "  \n",
            "\n",
            "    Examples:\n",
            "\n",
            "    ```python\n",
            "    >>> import av\n",
            "    >>> import torch\n",
            "    >>> import numpy as np\n",
            "\n",
            "    >>> from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n",
            "    >>> from huggingface_hub import hf_hub_download\n",
            "\n",
            "    >>> np.random.seed(0)\n",
            "\n",
            "\n",
            "    >>> def read_video_pyav(container, indices):\n",
            "    ...     '''\n",
            "    ...     Decode the video with PyAV decoder.\n",
            "    ...     Args:\n",
            "    ...         container (`av.container.input.InputContainer`): PyAV container.\n",
            "    ...         indices (`List[int]`): List of frame indices to decode.\n",
            "    ...     Returns:\n",
            "    ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
            "    ...     '''\n",
            "    ...     frames = []\n",
            "    ...     container.seek(0)\n",
            "    ...     start_index = indices[0]\n",
            "    ...     end_index = indices[-1]\n",
            "    ...     for i, frame in enumerate(container.decode(video=0)):\n",
            "    ...         if i > end_index:\n",
            "    ...             break\n",
            "    ...         if i >= start_index and i in indices:\n",
            "    ...             frames.append(frame)\n",
            "    ...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
            "\n",
            "\n",
            "    >>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
            "    ...     '''\n",
            "    ...     Sample a given number of frame indices from the video.\n",
            "    ...     Args:\n",
            "    ...         clip_len (`int`): Total number of frames to sample.\n",
            "    ...         frame_sample_rate (`int`): Sample every n-th frame.\n",
            "    ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n",
            "    ...     Returns:\n",
            "    ...         indices (`List[int]`): List of sampled frame indices\n",
            "    ...     '''\n",
            "    ...     converted_len = int(clip_len * frame_sample_rate)\n",
            "    ...     end_idx = np.random.randint(converted_len, seg_len)\n",
            "    ...     start_idx = end_idx - converted_len\n",
            "    ...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
            "    ...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
            "    ...     return indices\n",
            "\n",
            "\n",
            "    >>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n",
            "    >>> file_path = hf_hub_download(\n",
            "    ...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
            "    ... )\n",
            "    >>> container = av.open(file_path)\n",
            "\n",
            "    >>> # sample 16 frames\n",
            "    >>> indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
            "    >>> video = read_video_pyav(container, indices)\n",
            "\n",
            "    >>> image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n",
            "    >>> model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n",
            "\n",
            "    >>> inputs = image_processor(list(video), return_tensors=\"pt\")\n",
            "\n",
            "    >>> with torch.no_grad():\n",
            "    ...     outputs = model(**inputs)\n",
            "    ...     logits = outputs.logits\n",
            "\n",
            "    >>> # model predicts one of the 400 Kinetics-400 classes\n",
            "    >>> predicted_label = logits.argmax(-1).item()\n",
            "    >>> print(model.config.id2label[predicted_label])\n",
            "    eating spaghetti\n",
            "    ```\n",
            "File:      ~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/videomae/modeling_videomae.py\n",
            "Type:      method"
          ]
        }
      ],
      "source": [
        "model3.forward?"
      ],
      "id": "cell-54"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시1 – 입력나열, loss O*"
      ],
      "id": "663a2958-2d8c-42ac-bcd6-e591f28f9776"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model3(\n",
        "    pixel_values = torch.randn(4,16,3,224,224),\n",
        "    labels = torch.tensor([0,1,0,1])\n",
        ")"
      ],
      "id": "cell-56"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시2 – `**딕셔너리`, loss O*"
      ],
      "id": "04b5b0f3-9f8b-4fa8-b6a5-0f2653d23f7d"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model3_input = dict(\n",
        "    pixel_values = torch.randn(4,16,3,224,224),\n",
        "    labels = torch.tensor([0,1,0,1])\n",
        ")\n",
        "model3(**model3_input)"
      ],
      "id": "cell-58"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시3 – 입력나열, loss X*"
      ],
      "id": "9eb0b853-cd41-4025-a075-64ba750fecf0"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model3(\n",
        "    pixel_values = torch.randn(4,16,3,224,224)\n",
        ")"
      ],
      "id": "cell-60"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시4 – `**딕셔너리`, loss X*"
      ],
      "id": "c24bf3e3-ff63-4e2d-9349-09a882f5031d"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model3_input = dict(\n",
        "    pixel_values = torch.randn(4,16,3,224,224),\n",
        ")\n",
        "model3(**model3_input)"
      ],
      "id": "cell-62"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*사용예시5 – 초간단, loss X*"
      ],
      "id": "774896d9-190e-4eb4-8832-768525b56b73"
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.random.manual_seed(42)\n",
        "model3(torch.randn(4,16,3,224,224))"
      ],
      "id": "cell-64"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "# 4. Model 사용 연습\n",
        "\n",
        "## A. 텍스트"
      ],
      "id": "bfe47529-3f54-4313-a958-0409e95cc3e7"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "model1 = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\", num_labels=2\n",
        ")\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
      ],
      "id": "cell-68"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`# 예제1` – imdb"
      ],
      "id": "1d4d94c4-22a4-4b07-ae0c-38c71c1c5cc8"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "imdb = datasets.load_dataset('imdb')"
      ],
      "id": "cell-70"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = imdb['train'].select(range(3))\n",
        "d"
      ],
      "id": "cell-71"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이1)`\n",
        "\n",
        "*실패*"
      ],
      "id": "476c5444-83c9-4863-bb13-9bf5745301b8"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1.forward(torch.tensor(tokenizer(d['text'])['input_ids']))"
      ],
      "id": "cell-74"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*원인분석*"
      ],
      "id": "b078bb1d-8cd7-4566-a862-4185b2600c28"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level 1 - Type: list, Length: 3, Content: [[101, 1045, 12524, 1045, ... // ... , 7987, 1013, 1028, 102]]\n",
            "     Level 2 - Type: list, Length: 363, Content: [101, 1045, 12524, 1045,  ... // ... 7, 1037, 5436, 1012, 102]\n",
            "     Level 2 - Type: list, Length: 304, Content: [101, 1000, 1045, 2572, 8 ... // ... 5, 1055, 4230, 1012, 102]\n",
            "     Level 2 - Type: list, Length: 133, Content: [101, 2065, 2069, 2000, 4 ... // ... 6, 7987, 1013, 1028, 102]"
          ]
        }
      ],
      "source": [
        "mp.show_list(\n",
        "    tokenizer(d['text'])['input_ids']\n",
        ")"
      ],
      "id": "cell-76"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level 1 - Type: list, Length: 3, Content: [[101, 1045, 12524, 1045, ... // ...  0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "     Level 2 - Type: list, Length: 363, Content: [101, 1045, 12524, 1045,  ... // ... 7, 1037, 5436, 1012, 102]\n",
            "     Level 2 - Type: list, Length: 363, Content: [101, 1000, 1045, 2572, 8 ... // ... , 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "     Level 2 - Type: list, Length: 363, Content: [101, 2065, 2069, 2000, 4 ... // ... , 0, 0, 0, 0, 0, 0, 0, 0]"
          ]
        }
      ],
      "source": [
        "mp.show_list(\n",
        "    tokenizer(d['text'], padding=True)['input_ids']\n",
        ")"
      ],
      "id": "cell-77"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*성공*"
      ],
      "id": "6e14c67e-0d6c-41c2-b953-017fd5c335ae"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(torch.tensor(tokenizer(d['text'], padding=True)['input_ids']))"
      ],
      "id": "cell-79"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이2)`"
      ],
      "id": "c3da2612-5cba-4fa4-9914-e3d5a1c2fb27"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(tokenizer(d['text'], padding=True, return_tensors=\"pt\")['input_ids'])"
      ],
      "id": "cell-81"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "`# 예제2` – emotion"
      ],
      "id": "35352241-9935-4a80-a76d-eeed4363d41c"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "emotion = datasets.load_dataset('emotion')"
      ],
      "id": "cell-84"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = emotion['train'].select(range(3))\n",
        "d"
      ],
      "id": "cell-85"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이)`"
      ],
      "id": "208660c5-acc7-4d55-8a40-b83fc568c78e"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(torch.tensor(tokenizer(d['text'],padding=True)['input_ids']))"
      ],
      "id": "cell-87"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "`# 예제3` – MBTI"
      ],
      "id": "88618e94-fc61-4ba4-9f89-288b5debef73"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "    - Avoid using `tokenizers` before the fork if possible\n",
            "    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-10 01:07:26--  https://raw.githubusercontent.com/guebin/MP2024/refs/heads/main/posts/mbti_1.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62856486 (60M) [text/plain]\n",
            "Saving to: ‘mbti_1.csv’\n",
            "\n",
            "mbti_1.csv          100%[===================>]  59.94M   108MB/s    in 0.6s    \n",
            "\n",
            "2024-11-10 01:07:31 (108 MB/s) - ‘mbti_1.csv’ saved [62856486/62856486]\n"
          ]
        }
      ],
      "source": [
        "# mbti1.csv 파일 다운로드 \n",
        "!wget https://raw.githubusercontent.com/guebin/MP2024/refs/heads/main/posts/mbti_1.csv"
      ],
      "id": "cell-90"
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = datasets.Dataset.from_csv(\"mbti_1.csv\").select(range(3))\n",
        "d"
      ],
      "id": "cell-91"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이1)`\n",
        "\n",
        "*실패*"
      ],
      "id": "573f11b2-7148-4f7e-a36c-913c125916aa"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(torch.tensor(tokenizer(d['posts'],padding=True)['input_ids']))"
      ],
      "id": "cell-94"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*원인분석*"
      ],
      "id": "50387dad-0e52-479e-8ce1-f14bdff47796"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level 1 - Type: list, Length: 3, Content: [[101, 1005, 8299, 1024,  ... // ...  0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "     Level 2 - Type: list, Length: 2102, Content: [101, 1005, 8299, 1024, 1 ... // ... , 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "     Level 2 - Type: list, Length: 2102, Content: [101, 1005, 1045, 1005, 1 ... // ... 2, 1012, 1012, 1005, 102]\n",
            "     Level 2 - Type: list, Length: 2102, Content: [101, 1005, 2204, 2028, 1 ... // ... , 0, 0, 0, 0, 0, 0, 0, 0]"
          ]
        }
      ],
      "source": [
        "mp.show_list(\n",
        "    tokenizer(d['posts'],padding=True)['input_ids']\n",
        ")"
      ],
      "id": "cell-96"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level 1 - Type: list, Length: 3, Content: [[101, 1005, 8299, 1024,  ... // ... , 7834, 1012, 2077, 102]]\n",
            "     Level 2 - Type: list, Length: 512, Content: [101, 1005, 8299, 1024, 1 ... // ... 1, 4127, 2017, 2215, 102]\n",
            "     Level 2 - Type: list, Length: 512, Content: [101, 1005, 1045, 1005, 1 ... // ... 6, 2600, 3259, 2028, 102]\n",
            "     Level 2 - Type: list, Length: 512, Content: [101, 1005, 2204, 2028, 1 ... // ... 0, 7834, 1012, 2077, 102]"
          ]
        }
      ],
      "source": [
        "mp.show_list(\n",
        "    tokenizer(d['posts'],truncation=True)['input_ids']\n",
        ")"
      ],
      "id": "cell-97"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*성공*"
      ],
      "id": "6f38d8fa-c150-420e-849c-4e0fe006d0ef"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(torch.tensor(tokenizer(d['posts'],truncation=True)['input_ids']))\n",
        "#model1(tokenizer(d['posts'],truncation=True,return_tensors=\"pt\")['input_ids'])"
      ],
      "id": "cell-99"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이2)` *–모델설정변경 (퀴즈5, 모델의 프레임수를 4로 바꾸는 예제에서\n",
        "사용한 테크닉)*\n",
        "\n",
        "*distilbert/distilbert-base-uncased 설정값 부르기*"
      ],
      "id": "602a5f82-8e15-4fa4-80b3-a90a0ee2e446"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = transformers.AutoConfig.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\"\n",
        ")\n",
        "config"
      ],
      "id": "cell-102"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*설정값변경*"
      ],
      "id": "a365fe77-e807-454f-9673-8db68d3c7715"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "config.max_position_embeddings = 2200"
      ],
      "id": "cell-104"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*설정값으로 모델불러오기*"
      ],
      "id": "bd68f57f-7574-451e-a427-5e30de99d46f"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1_large = transformers.AutoModelForSequenceClassification.from_config(\n",
        "    config=config\n",
        ")"
      ],
      "id": "cell-106"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*모델사용*"
      ],
      "id": "96bc9df7-29de-4f71-b5cd-b3ba955503b8"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1_large(torch.tensor(tokenizer(d['posts'],padding=True)['input_ids']))"
      ],
      "id": "cell-108"
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1_large(**tokenizer(d['posts'],padding=True,return_tensors=\"pt\"))"
      ],
      "id": "cell-109"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "`# 예제4` – sms_spam"
      ],
      "id": "7878338c-0f2f-4c33-8d45-87692f313038"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "sms_spam = datasets.load_dataset('sms_spam')['train'].train_test_split(test_size=0.2, seed=42)\n",
        "sms_spam"
      ],
      "id": "cell-112"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = sms_spam['train'].select(range(3))\n",
        "d"
      ],
      "id": "cell-113"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이)`"
      ],
      "id": "a7187a92-b7c9-4d99-8750-90abc4731fef"
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1(**tokenizer(d['sms'],padding=True,return_tensors=\"pt\"))"
      ],
      "id": "cell-115"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "## B. 이미지"
      ],
      "id": "50ae9382-42d2-4b21-81ec-875fe3bcc6b5"
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "model2 = transformers.AutoModelForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\",\n",
        "    num_labels=3 # 그냥 대충 3이라고 했음.. 별 이유는 없음\n",
        ")"
      ],
      "id": "cell-118"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`# 예제1` – food101"
      ],
      "id": "db49dde5-327c-41a7-823a-d4dd1c714731"
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = datasets.load_dataset(\"food101\", split=\"train[:4]\")\n",
        "d"
      ],
      "id": "cell-120"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(예비학습)` – `torchvision.transforms` 에서 제공하는 기능들은\n",
        "배치처리가 가능한가?"
      ],
      "id": "3ad502bc-71bf-4244-9f32-5fa530593e36"
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [],
      "source": [
        "to_tensor = torchvision.transforms.ToTensor()"
      ],
      "id": "cell-122"
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [],
      "source": [
        "to_tensor(d['image'][0])"
      ],
      "id": "cell-123"
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [],
      "source": [
        "to_tensor(d['image'])"
      ],
      "id": "cell-124"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이)`"
      ],
      "id": "232b4923-0b20-4911-9c12-33fb9ecf4204"
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [],
      "source": [
        "compose = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Resize((224,224))\n",
        "])"
      ],
      "id": "cell-126"
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.stack(list(map(compose,d['image'])),axis=0).shape"
      ],
      "id": "cell-127"
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2.forward(\n",
        "    torch.stack(list(map(compose,d['image'])),axis=0)\n",
        ")"
      ],
      "id": "cell-128"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "`# 예제2`"
      ],
      "id": "6f25b6a9-64ca-4d98-8013-297261a470eb"
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [],
      "source": [
        "beans = datasets.load_dataset('beans')\n",
        "d = beans['train'].select(range(4))\n",
        "d"
      ],
      "id": "cell-131"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이)`"
      ],
      "id": "d8fd3077-4da9-41dc-9221-3ee00a417475"
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2(torch.stack(list(map(compose,d['image'])),axis=0))"
      ],
      "id": "cell-133"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`#`\n",
        "\n",
        "## C. 동영상"
      ],
      "id": "8325b399-e695-4ba6-a914-e4afb9dcd03f"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "model3 = transformers.VideoMAEForVideoClassification.from_pretrained(\n",
        "    \"MCG-NJU/videomae-base\",\n",
        ")"
      ],
      "id": "cell-136"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`# 예제1` – UCF101_subset"
      ],
      "id": "af13ae1f-f9af-4718-9545-9767da76e255"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = huggingface_hub.hf_hub_download(\n",
        "    repo_id=\"sayakpaul/ucf101-subset\",\n",
        "    filename=\"UCF101_subset.tar.gz\",\n",
        "    repo_type=\"dataset\"\n",
        ")\n",
        "# file_path는 다운로드한 압축파일이 존재하는 경로와 파일명이 string으로 저장되어있음.\n",
        "with tarfile.open(file_path) as t:\n",
        "     t.extractall(\"./data\") # 여기에서 \".\"은 현재폴더라는 의미"
      ],
      "id": "cell-138"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "└── UCF101_subset\n",
            "    ├── test\n",
            "    │   ├── ApplyEyeMakeup\n",
            "    │   │   ├── UCF101\n",
            "    │   │   ├── v_ApplyEyeMakeup_g03_c01.avi\n",
            "    │   │   └── ...\n",
            "    │   │   └── v_ApplyEyeMakeup_g23_c06.avi\n",
            "    │   ├── ApplyLipstick\n",
            "    │   │   ├── UCF101\n",
            "    │   │   ├── v_ApplyLipstick_g14_c01.avi\n",
            "    │   │   └── ...\n",
            "    │   │   └── v_ApplyLipstick_g16_c04.avi\n",
            "    │   └── ...\n",
            "    │   └── BenchPress\n",
            "    │       ├── UCF101\n",
            "    │       ├── v_BenchPress_g05_c02.avi\n",
            "    │       └── ...\n",
            "    │       └── v_BenchPress_g25_c06.avi\n",
            "    ├── train\n",
            "    │   ├── ApplyEyeMakeup\n",
            "    │   │   ├── UCF101\n",
            "    │   │   ├── v_ApplyEyeMakeup_g02_c03.avi\n",
            "    │   │   └── ...\n",
            "    │   │   └── v_ApplyEyeMakeup_g25_c07.avi\n",
            "    │   ├── ApplyLipstick\n",
            "    │   │   ├── UCF101\n",
            "    │   │   ├── v_ApplyLipstick_g01_c02.avi\n",
            "    │   │   └── ...\n",
            "    │   │   └── v_ApplyLipstick_g24_c05.avi\n",
            "    │   └── ...\n",
            "    │   └── BenchPress\n",
            "    │       ├── UCF101\n",
            "    │       ├── v_BenchPress_g01_c05.avi\n",
            "    │       └── ...\n",
            "    │       └── v_BenchPress_g24_c05.avi\n",
            "    └── val\n",
            "        ├── ApplyEyeMakeup\n",
            "        │   ├── UCF101\n",
            "        │   ├── v_ApplyEyeMakeup_g01_c01.avi\n",
            "        │   ├── v_ApplyEyeMakeup_g14_c05.avi\n",
            "        │   └── v_ApplyEyeMakeup_g20_c04.avi\n",
            "        ├── ApplyLipstick\n",
            "        │   ├── UCF101\n",
            "        │   ├── v_ApplyLipstick_g10_c04.avi\n",
            "        │   ├── v_ApplyLipstick_g20_c04.avi\n",
            "        │   └── v_ApplyLipstick_g25_c02.avi\n",
            "        └── ...\n",
            "        └── BenchPress\n",
            "            ├── UCF101\n",
            "            ├── v_BenchPress_g11_c05.avi\n",
            "            ├── v_BenchPress_g17_c02.avi\n",
            "            └── v_BenchPress_g17_c06.avi"
          ]
        }
      ],
      "source": [
        "mp.tree(\"./data\")"
      ],
      "id": "cell-139"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "video_path = \"./data/UCF101_subset/test/BenchPress/v_BenchPress_g05_c02.avi\"\n",
        "video = pytorchvideo.data.encoded_video.EncodedVideo.from_path(video_path).get_clip(0, float('inf'))['video']\n",
        "video.shape"
      ],
      "id": "cell-140"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`(풀이)`"
      ],
      "id": "a91b66e0-cbac-4deb-baa1-da4510193444"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "model3(\n",
        "    #video.permute(1,0,2,3)[:16,:,:224,:224].unsqueeze(0)\n",
        "    #video.permute(1,0,2,3)[:16,:,:224,:224].reshape(1,16,3,224,224)\n",
        "    torch.stack([video.permute(1,0,2,3)[:16,:,:224,:224]])\n",
        ")"
      ],
      "id": "cell-142"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  }
}