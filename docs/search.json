[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "기계학습활용 (2024)",
    "section": "",
    "text": "질문하는 방법\n\n카카오톡: 질문하러 가기\n이메일: guebin@jbnu.ac.kr\n직접방문: 자연과학대학 본관 205호\nZoom: 카카오톡이나 이메일로 미리 시간을 정할 것\nLMS쪽지: https://ieilms.jbnu.ac.kr/\n\nQuiz: https://guebin.github.io/MP2024/quiz.html\n강의노트\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nOct 4, 2024\n\n\n05wk-1: Food-101 이미지자료의 분류\n\n\n최규빈 \n\n\n\n\nSep 27, 2024\n\n\n04wk-1: 감성분석 파고들기 (2)\n\n\n최규빈 \n\n\n\n\nSep 19, 2024\n\n\n03wk-1: 감성분석 파고들기 (1)\n\n\n최규빈 \n\n\n\n\nSep 12, 2024\n\n\n02wk-1: IMDB 영화평 감성분석\n\n\n최규빈 \n\n\n\n\nSep 6, 2024\n\n\n01wk-2: IMDB 자료 살펴보기, 지도학습의 개념\n\n\n최규빈 \n\n\n\n\nSep 3, 2024\n\n\n01wk-1: 강의소개\n\n\n최규빈 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "quiz/Quiz-1.html",
    "href": "quiz/Quiz-1.html",
    "title": "Quiz-1 (2024.09.05) // 범위: 파이썬기본문법",
    "section": "",
    "text": "항목\n허용 여부\n비고\n\n\n\n\n강의노트 참고\n허용\n수업 중 제공된 강의노트나 본인이 정리한 자료를 참고 가능\n\n\n구글 검색\n허용\n인터넷을 통한 자료 검색 및 정보 확인 가능\n\n\n생성 모형 사용\n허용 안함\n인공지능 기반 도구(GPT 등) 사용 불가\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\n\n모든 문항은 부분점수 없음.\n일부문항은 부분문제를 모두 맞출경우만 정답으로 인정함.\n.ipynb 파일 형태로 LMS에 제출된 답안지만 채점하며 그 외의 형식 (.hwp등)은 채점하지 않음. 즉 0점 처리함. 제출방법 모르면 물어볼것!\n\n\n\n\n공지사항: (1) 부정행위 적발시 F학점 처리. 부정행위는 판단하는 기준은 교수주관임. (2) 지연제출은 별다른 감점이 없을 예정. 지속적인 지연제출은 감점 대상이 댈 수 있음. (3) 지각역시 별다른 패널티가 없을 예정이나 반복적인 지각은 지양할 것. (4) 모든 퀴즈를 응시하더라도 성적미달시 F가 나올 수 있음.\n\n\n1. 10점\n아래의 코드를 관찰하고 올바르게 해석한 학생을 모두 골라라.\n\n올바르게 해석한 학생을 모두 정확하게 맞출경우만 20점으로 인정하고 그 외의는 0점 처리함\n\n\ndef double_result(func):\n    def wrapper(*args, **kwargs):\n        result = func(*args, **kwargs)\n        return result * 2\n    return wrapper\n\ndef add(a, b):\n    return a + b\n\n@double_result\ndef add2(a, b):\n    return a + b\n\n민수: 데코레이터는 함수 결과를 두 배로 만들어 반환하므로 add2(4, 4)의 결과는 8 * 2 = 16이다. 출력 결과는 16이다.\n선철: add(-2, 2) 와 add2(-2, 2)의 출력결과는 같다.\n예지: 데코레이터는 매개변수(=함수의 입력값)를 두 배로 해서 함수에 전달한다. 그래서 add2(4, 4)는 (4 * 2) + (4 * 2)가 되어 출력 결과는 16이다.\n준현: 데코레이터는 add2 함수의 입력값을 곱해서 리턴하도록 동작한다. 따라서 add2(4, 4)의 결과는 (4 * 4) = 16 이 출력된다.\n(풀이)\n답: 민수, 선철\n\n\n2. 10점\n아래의 코드를 관찰하고 올바르게 해석한 학생을 모두 골라라.\n\n올바르게 해석한 학생을 모두 정확하게 맞출경우만 20점으로 인정하고 그 외의는 0점 처리함\n\n\ndef infinite_sequence():\n    num = 0\n    while True:\n        yield num\n        num += 1\n\ngen = infinite_sequence()\n\nfor _ in range(5):\n    print(next(gen))\n\n0\n1\n2\n3\n4\n\n\n\nset(dir(gen)) & {'__next__'}\n\n{'__next__'}\n\n\n토르: infinite_sequence()는 무한 루프를 돌며 num 값을 하나씩 yield로 반환하고, next()로 값을 호출할 때마다 그 값을 증가시킨다. 따라서 이 코드는 (0, 1, 2, 3, 4)를 순서대로 출력한다.\n캡틴: infinite_sequence()는 5개의 값을 반환한 후 더 이상 값을 생성하지 않고 종료된다. 즉, 그 이후에 next(gen)을 호출하면 더 이상 값이 반환되지 않고 StopIteration 예외가 발생한다.\n위도우: gen은 호출가능한 객체 (callable object) 이다.\n헐크: 이 코드에서 추가로 아래의 코드를 실행하면\nfor _ in range(10):\n    print(next(gen))\n다음 10개의 값(5, 6, 7, 8, 9, 10, 11, 12, 13, 14)이 순서대로 출력된다.\n(풀이)\n답: 토르, 헐크\n\n\n3. 20점\n아래의 코드를 관찰하고 올바르게 해석한 학생을 모두 골라라.\n\n올바르게 해석한 학생을 모두 정확하게 맞출경우만 20점으로 인정하고 그 외의는 0점 처리함\n\n\nclass Dummy:\n    def __init__(self, a):\n        self.a = a\n\nclass Dummy2(Dummy):\n    def __init__(self, a):\n        super().__init__(a)\n        self.a = self.a + 1\n\nclass Dummy3(Dummy):\n    def __init__(self, a):\n        super().__init__(a)\n        self.a = self.a + 2\n\nclass Dummy4(Dummy3, Dummy2):\n    def __init__(self, a):\n        super(Dummy2, self).__init__(a)\n        self.a = self.a + 3\n\n민지: Dummy4 인스턴스를 생성할 때 super(Dummy2, self).__init__(a)가 호출되어, MRO에 따라 먼저 Dummy3의 __init__ 메소드가 실행된다. 그 후 Dummy4의 __init__ 에서 self.a = self.a + 3 이 실행된다.\n시혁: Dummy4 인스턴스를 생성할 때 super(Dummy2, self).__init__(a)가 호출되면 Dummy2의 __init__이 실행된다. 그 후 Dummy4의 __init__ 에서 self.a = self.a + 3 이 실행된다.\n희진: Dummy4는 다중 상속을 받았으므로 Dummy2, 3의 __init__이 모두 실행된다. 그 후 Dummy4의 __init__ 에서 self.a = self.a + 3 이 실행된다.\n팜하니: Dummy4는 다중 상속을 사용하므로 Dummy, Dummy2, Dummy3 의 __init__ 순서대로 모두 호출한다. 그리고 마지막으로 Dummy4의 __init__ 에서 self.a = self.a + 3 이 실행된다.\n(풀이)\n답: 모두 틀렸음\n\n\n4. 20점\n\n!pip install datasets # 우선 이걸 실행해서 패키지 설치하세요, 못하겠으면 손들고 저를 부르세요\n\n아래의 코드를 관찰하고 올바르게 해석한 학생을 모두 골라라.\n\n올바르게 해석한 학생을 모두 정확하게 맞출경우만 20점으로 인정하고 그 외의는 0점 처리함\n\n\nfrom datasets import load_dataset\nimdb = load_dataset(\"imdb\")\n\n/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n동용: load_dataset은 function 클래스에서 생성된 인스턴스이다.\n태한: imdb 는 __len__ 메소드를 가지고 있다. 따라서 len(imdb) 는 실행가능한 코드임을 알 수 있다.\n대영: imdb['train'], imdb['test'] 역시 모두 __len__ 메소드를 가지고 있다. 그리고 len(imdb['train']) + len(imdb['test']) 의 값은 len(imbd)의 값과 같다.\n현수: imdb['train']는 __iter__ 메소드를 가지고 있다. 따라서 아래의 코드가 실행가능하다.\niterator = iter(imdb['train'])\nprint(next(iterator))\nprint(next(iterator))\n(풀이)\n답: 동용, 태한, 현수\n\n채점시 동용과 대영은 틀려도 봐줬습니다. 동용은 개념이 약하면 헷갈릴 수 있다고 생각했고, 대영은 본의아니게 낚시처럼 문제가 되어서요, 실수할수 있다고 생각했습니다.\n\n\n\n5. 40점\n주어진 load_dataset() 함수의 도움말에 대한 설명을 보고, 설명이 맞으면 O, 틀리면 X를 선택하라.\n\n모두 맞출 경우만 정답으로 인정\n\nload_dataset?\nSignature:\nload_dataset(\n    path: str,\n    name: Optional[str] = None,\n    data_dir: Optional[str] = None,\n    data_files: Union[str, Sequence[str], Mapping[str, Union[str, Sequence[str]]], NoneType] = None,\n    split: Union[str, datasets.splits.Split, NoneType] = None,\n    cache_dir: Optional[str] = None,\n    features: Optional[datasets.features.features.Features] = None,\n    download_config: Optional[datasets.download.download_config.DownloadConfig] = None,\n    download_mode: Union[datasets.download.download_manager.DownloadMode, str, NoneType] = None,\n    verification_mode: Union[datasets.utils.info_utils.VerificationMode, str, NoneType] = None,\n    ignore_verifications='deprecated',\n    keep_in_memory: Optional[bool] = None,\n    save_infos: bool = False,\n    revision: Union[str, datasets.utils.version.Version, NoneType] = None,\n    token: Union[bool, str, NoneType] = None,\n    use_auth_token='deprecated',\n    task='deprecated',\n    streaming: bool = False,\n    num_proc: Optional[int] = None,\n    storage_options: Optional[Dict] = None,\n    trust_remote_code: bool = None,\n    **config_kwargs,\n) -&gt; Union[datasets.dataset_dict.DatasetDict, datasets.arrow_dataset.Dataset, datasets.dataset_dict.IterableDatasetDict, datasets.iterable_dataset.IterableDataset]\nDocstring:\n여기서 부터는 너무 기니까 생략~\n1 path는 위치 인자로 전달될 수 있다.\n2. name 인자는 함수 호출 시 키워드 인자로 전달될 수 있으며, 기본값은 None이다.\n3. load_dataset() 함수는 가변 위치 인자를 허용하지 않는다.\n4. config_kwargs는 가변 키워드 인자이며, 추가적인 키워드 인자를 받기 위한 것이다.\n5. path의 타입 힌트는 str이다. 하지만 이것이path 인자의 값을 문자열 값만 전달될 수 있다는 의미는 아니다. Python에서 타입 힌트는 강제적인 제약이 아니라 개발자에게 코드의 의도를 알리기 위한 도구일 뿐이다. 따라서 Python에서는 타입 힌트를 무시하고 다른 타입의 값을 전달할 수 있다.\n6. split 인자의 타입 힌트는 Union[str, datasets.splits.Split, NoneType]이므로, 문자열(str), datasets.Split 객체, 또는 None의 형태를 입력으로 권장한다.\n7. download_config 인자는 가변 키워드 인자로 전달된다.\n8. data_dir은 위치 인자로도 전달될 수 있다.\n9. cache_dir 인자의 타입 힌트는 Optional[str]이므로, 문자열(str) 또는 None의 형태를 입력으로 권장한다.\n10. **config_kwargs는 함수 호출 시 몇 개의 키워드 인자를 받을 수 있는지 미리 알 필요가 없다.\n(풀이)\n7만 틀렸음.\n\n8번은 헷갈릴만 해서 틀려도 봐줬습니다. 8번이외에 한개정도 더 실수한 건 만점드렸습니다."
  },
  {
    "objectID": "posts/03wk-1.html#a.-1단계",
    "href": "posts/03wk-1.html#a.-1단계",
    "title": "03wk-1: 감성분석 파고들기 (1)",
    "section": "A. 1단계",
    "text": "A. 1단계\n\n인공지능에 대한 이해\n\n- 인공지능 불러오기\n\ntorch.manual_seed(43052)\n인공지능 = model = transformers.AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert/distilbert-base-uncased\", num_labels=2\n)\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n- 인공지능의 정체? 엄청나게 많은 숫자들이 포함된 어떠한 물체 (엄청나게 많은 파라메터들이 포함된 네트워크)\n\n인공지능\n\nDistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)\n\n\n\n인공지능.classifier.weight\n\nParameter containing:\ntensor([[-0.0234,  0.0279,  0.0242,  ...,  0.0091, -0.0063, -0.0133],\n        [ 0.0087,  0.0007, -0.0099,  ...,  0.0183, -0.0007,  0.0295]],\n       requires_grad=True)\n\n\n\n인공지능.pre_classifier.weight\n\nParameter containing:\ntensor([[-0.0122,  0.0030,  0.0301,  ...,  0.0003,  0.0327, -0.0123],\n        [-0.0041,  0.0008,  0.0169,  ..., -0.0163, -0.0117, -0.0135],\n        [ 0.0020, -0.0215, -0.0021,  ...,  0.0016,  0.0102, -0.0483],\n        ...,\n        [-0.0026, -0.0327,  0.0099,  ...,  0.0341, -0.0184, -0.0109],\n        [ 0.0088, -0.0345, -0.0011,  ...,  0.0018,  0.0172, -0.0122],\n        [-0.0148,  0.0147, -0.0184,  ..., -0.0166,  0.0241,  0.0201]],\n       requires_grad=True)\n\n\n- 인공지능? “입력정보 -&gt; 정리된숫자 -&gt; 계산 -&gt; 계산된숫자 -&gt; 출력정보” 의 과정에서 “계산”을 담당.\n\n인공지능이 가지고 있는 숫자들은 “계산”에 사용된다.\n\n- 입력정보에 영화에 대한 부정적 평가에 해당하는 텍스트를 넣는다면?\n\n입력정보_원시텍스트 = \"This movie was a huge disappointment.\"\n정리된숫자_토큰화된자료 = 토크나이저(입력정보_원시텍스트,return_tensors='pt')\n정리된숫자_토큰화된자료\n\n{'input_ids': tensor([[  101,  2023,  3185,  2001,  1037,  4121, 10520,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n\n\n\n인공지능(**정리된숫자_토큰화된자료)\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.1173,  0.0261]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n\n계산된숫자_로짓 = 인공지능(**정리된숫자_토큰화된자료).logits.detach().numpy()\n계산된숫자_로짓\n\narray([[-0.11731046,  0.02610319]], dtype=float32)\n\n\n\n출력정보_확률 = np.exp(계산된숫자_로짓)/np.exp(계산된숫자_로짓).sum() # 0일확률(=부정평가일확률), 1일확률(=긍정평가일확률)\n출력정보_확률\n\narray([[0.46420792, 0.53579205]], dtype=float32)\n\n\n\n출력정보_확률.argmax() # 부정적 영화평가에 대한 인공지능의 예측 \n\n1\n\n\n\n계산된숫자_로짓.argmax() # 부정적 영화평가에 대한 인공지능의 예측 &lt;-- 이렇게 구해도 됩니다.. 왜??\n\n1\n\n\n- 입력정보에 영화에 대한 긍정적 평가에 해당하는 텍스트를 넣는다면?\n\n입력정보_원시텍스트 = \"This was a masterpiece.\"\n정리된숫자_토큰화된자료 = 토크나이저(입력정보_원시텍스트,return_tensors='pt')\n정리된숫자_토큰화된자료\n\n{'input_ids': tensor([[  101,  2023,  2001,  1037, 17743,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n\n\n\n인공지능(**정리된숫자_토큰화된자료)\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.1080,  0.0264]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n\n계산된숫자_로짓 = 인공지능(**정리된숫자_토큰화된자료).logits.detach().numpy()\n계산된숫자_로짓\n\narray([[-0.10802954,  0.02638793]], dtype=float32)\n\n\n\n출력정보_확률 = np.exp(계산된숫자_로짓)/np.exp(계산된숫자_로짓).sum() # 0일확률(=부정평가일확률), 1일확률(=긍정평가일확률)\n출력정보_확률\n\narray([[0.46644613, 0.53355384]], dtype=float32)\n\n\n\n출력정보_확률.argmax() # 긍정적 영화평가에 대한 인공지능의 예측 \n\n1\n\n\n\n계산된숫자_로짓.argmax() # 긍정적 영화평가에 대한 인공지능의 예측\n\n1\n\n\n- 아무숫자나 뱉어내는 듯 \\(\\to\\) 멍청한 인공지능 (옹호: 멍청한건 당연함. 아직 학습전이니까)\n- 인공지능에 대한 이해1: 인공지능은 “정리된숫자”를 입력으로 하고 일련의 계산을 거쳐서 “계산된숫자”를 출력해주는 함수라 생각할 수 있음.\n- 인공지능에 대한 이해2: 인공지능은 (1) 많은숫자들과 (2) 고유의 계산방식을 가지고 있음.\n\n인공지능이 내부에 자체적으로 저장하고 있는 숫자들 “파라메터”라고 부름.\n인공지능은 나름의 법칙에 따라 “데이터”와 “파라메터”의 숫자들을 연산함. 즉 인공지능은 자체적으로 데이터와 파라메터를 어떻게 계산할지 알고있는데, 이러한 고유의 계산방식을 “아키텍처”라고 말함.\n\n- 인공지능에 대한 이해3: 두개의 인공지능이 서로 다른 고유의 계산방식을 가지고 있다면 두 인공지능은 “다른 모델” 임.\n- 인공지능에 대한 이해3’: 동일한 생성방식으로 만들어진 인공지능들은 모두 같은 모델임. 예를들면 아래의 인공지능1,2는 같은 모델임\n인공지능1 = transformers.AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert/distilbert-base-uncased\", num_labels=2\n)\n인공지능2 = transformers.AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert/distilbert-base-uncased\", num_labels=2\n)\n- 인공지능에 대한 이해4: 두 인공지능이 같은모델이라고 해도, 항상 같은 결과를 주는건 아님. 파라메터에 따라 다른 결과를 줄 수도 있음. (예를들면 위의 인공지능1,2는 같은 모델이지만 다른 파라메터를 가지므로 다른 결과를 줌)"
  },
  {
    "objectID": "posts/03wk-1.html#b.-2단계",
    "href": "posts/03wk-1.html#b.-2단계",
    "title": "03wk-1: 감성분석 파고들기 (1)",
    "section": "B. 2단계",
    "text": "B. 2단계\n\n미니배치의 이해\n\n- 예비학습\n\narr = np.array([[1,2],[2,3],[3,4]])\narr\n\narray([[1, 2],\n       [2, 3],\n       [3, 4]])\n\n\n\narr / arr.sum(axis=1).reshape(-1,1)\n\narray([[0.33333333, 0.66666667],\n       [0.4       , 0.6       ],\n       [0.42857143, 0.57142857]])\n\n\n- 예비개념1: 인공지능은 사실 영화평을 하나씩 하나씩 처리하지 않는다. 덩어리로 처리한다.\n- 예비개념2: 그렇다고 해서 인공지능이 25000개를 모두 덩어리로 처리하는건 아니다 \\(\\to\\) 16개씩 혹은 32개씩 묶어서 작은덩어리를 만든 후 처리한다.\n\n16, 32와 같은 숫자를 batch_size 라고 한다.\n16개, 32개로 모인 작은덩어리를 미니배치라고 한다.\n\n- 16개의 입력정보를 한번에 처리\n\n입력정보들_원시텍스트 = 데이터['train'][:16]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n출력정보들_확률 = np.exp(계산된숫자들_로짓) / np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\n출력정보들_확률\n\narray([[0.47823825, 0.5217617 ],\n       [0.47215328, 0.5278467 ],\n       [0.4804948 , 0.5195052 ],\n       [0.46609667, 0.5339033 ],\n       [0.48814487, 0.5118551 ],\n       [0.48004225, 0.5199578 ],\n       [0.49288097, 0.50711906],\n       [0.49470255, 0.5052974 ],\n       [0.4941453 , 0.50585467],\n       [0.49016201, 0.50983804],\n       [0.495789  , 0.504211  ],\n       [0.48260576, 0.51739424],\n       [0.49386355, 0.5061364 ],\n       [0.49013382, 0.5098662 ],\n       [0.48240176, 0.5175982 ],\n       [0.49301967, 0.5069803 ]], dtype=float32)\n\n\n\n계산된숫자들_로짓.argmax(axis=1) # 인공지능의 예측\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n\n- 기억할 것: 정리된숫자들_토큰화된자료 는 모두 길이가 512임. (그렇게 되도록 패딩함)\n\n정리된숫자들_토큰화된자료['input_ids'].shape, 정리된숫자들_토큰화된자료['attention_mask'].shape\n\n(torch.Size([16, 512]), torch.Size([16, 512]))\n\n\n- 실제 단어수\n\n정리된숫자들_토큰화된자료['attention_mask'].sum(axis=1)\n\ntensor([363, 304, 133, 185, 495, 154, 143, 388, 512, 297, 365, 171, 192, 173,\n        470, 263])\n\n\n- 패딩된단어수\n\n512-정리된숫자들_토큰화된자료['attention_mask'].sum(axis=1)\n\ntensor([149, 208, 379, 327,  17, 358, 369, 124,   0, 215, 147, 341, 320, 339,\n         42, 249])\n\n\n- 이러한 변환이 필요한 이유? 인공지능은 항상 (n,m) 차원으로 정리된 숫자들만 입력으로 받을 수 있음.\n\n왜? 사실 인공지능은 행렬계산을 하도록 설계되어있음.\n그래서 할수없이 padding을 하거나 truncation을 하는 것임. (실제로는 행렬이 아니지만 억지로 행렬을 만들기 위해서)"
  },
  {
    "objectID": "posts/03wk-1.html#c.-3단계",
    "href": "posts/03wk-1.html#c.-3단계",
    "title": "03wk-1: 감성분석 파고들기 (1)",
    "section": "C. 3단계",
    "text": "C. 3단계\n\n동적패딩을 이해하자.\n\n- 만약에 batch_size=4로 설정하여 처리한다면?\n\n입력정보들_원시텍스트 = 데이터['train'][:4]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n출력정보들_확률 = np.exp(계산된숫자들_로짓) / np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\n출력정보들_확률\n\narray([[0.47823825, 0.5217618 ],\n       [0.47215328, 0.5278467 ],\n       [0.4804948 , 0.5195052 ],\n       [0.46609667, 0.5339033 ]], dtype=float32)\n\n\n\n계산된숫자들_로짓.argmax(axis=1) # 인공지능의 예측 \n\narray([1, 1, 1, 1])\n\n\n- 정리된숫자들_토큰화된자료['input_ids'] 의 차원은 어떠할까? (4,512)\n\n정리된숫자들_토큰화된자료['input_ids'].shape, 정리된숫자들_토큰화된자료['attention_mask'].shape\n\n(torch.Size([4, 363]), torch.Size([4, 363]))\n\n\n\n끝의 차원이 512가 아니라 363이다.. 왜??\n\n- 덩어리의 상태에 따라서 유동적으로 패딩 \\(\\to\\) 이렇게 해도 잘 돌아감\n\n입력정보들_원시텍스트 = 데이터['train'][:4]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n인공지능(**정리된숫자들_토큰화된자료)\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.0847,  0.0024],\n        [-0.0830,  0.0285],\n        [-0.0600,  0.0180],\n        [-0.0919,  0.0440]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n\n입력정보들_원시텍스트 = 데이터['train'][4:8]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n인공지능(**정리된숫자들_토큰화된자료)\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.0422,  0.0053],\n        [-0.0646,  0.0152],\n        [-0.0172,  0.0112],\n        [-0.0283, -0.0072]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n\n입력정보들_원시텍스트 = 데이터['train'][8:12]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n인공지능(**정리된숫자들_토큰화된자료)\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.0246, -0.0012],\n        [-0.0594, -0.0200],\n        [-0.0240, -0.0071],\n        [-0.0836, -0.0140]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n\n입력정보들_원시텍스트 = 데이터['train'][12:16]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n인공지능(**정리된숫자들_토큰화된자료)\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.0440, -0.0195],\n        [-0.0469, -0.0074],\n        [-0.0542,  0.0162],\n        [-0.0316, -0.0037]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n- 싹다 maxlen=512로 가정하고 패딩해서 돌린결과와 비교 \\(\\to\\) 같음 \\(\\to\\) 동적패딩이 효율적\n\n입력정보들_원시텍스트 = 데이터['train'][:16]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n인공지능(**정리된숫자들_토큰화된자료)\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.0847,  0.0024],\n        [-0.0830,  0.0285],\n        [-0.0600,  0.0180],\n        [-0.0919,  0.0440],\n        [-0.0422,  0.0053],\n        [-0.0646,  0.0152],\n        [-0.0172,  0.0112],\n        [-0.0283, -0.0072],\n        [-0.0246, -0.0012],\n        [-0.0594, -0.0200],\n        [-0.0240, -0.0071],\n        [-0.0836, -0.0140],\n        [-0.0440, -0.0195],\n        [-0.0469, -0.0074],\n        [-0.0542,  0.0162],\n        [-0.0316, -0.0037]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)"
  },
  {
    "objectID": "posts/03wk-1.html#d.-4단계",
    "href": "posts/03wk-1.html#d.-4단계",
    "title": "03wk-1: 감성분석 파고들기 (1)",
    "section": "D. 4단계",
    "text": "D. 4단계\n\n손실(=loss)의 개념을 이해하자\n\n- 정리된숫자들_토큰화된자료에서 labels를 추가 전달하면, 인공지능(**정리된숫자들_토큰화된자료)의 결과로 loss가 추가계산됨.\n\n입력정보들_원시텍스트 = 데이터['train'][:4]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n#데이터['train'][:4]['label'] # 정답확인\n정리된숫자들_토큰화된자료['labels'] = torch.tensor([0,0,0,0]) # 정답입력\n인공지능(**정리된숫자들_토큰화된자료)\n\nSequenceClassifierOutput(loss=tensor(0.7461, grad_fn=&lt;NllLossBackward0&gt;), logits=tensor([[-0.0847,  0.0024],\n        [-0.0830,  0.0285],\n        [-0.0600,  0.0180],\n        [-0.0919,  0.0440]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n- 정리를 해보자.\n\n입력정보들_원시텍스트 = 데이터['train'][:4]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\nprint(f'실제정답: {데이터['train'][:4]['label']}') # 정답확인\n정리된숫자들_토큰화된자료['labels'] = torch.tensor([0,0,0,0]) # 정답입력\n계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n출력정보들_확률 = np.exp(계산된숫자들_로짓)/np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\nprint(f'인공지능의예측: {계산된숫자들_로짓.argmax(axis=1)}')\nprint(f'인공지능의확신정도: {출력정보들_확률.max(axis=1)}')\nprint(f'손실(loss): {인공지능(**정리된숫자들_토큰화된자료).loss.item():.6f}')\n\n실제정답: [0, 0, 0, 0]\n인공지능의예측: [1 1 1 1]\n인공지능의확신정도: [0.5217618 0.5278467 0.5195052 0.5339033]\n손실(loss): 0.746100\n\n\n- loss는 작을수록 주어진 데이터에 대한 정답을 잘 맞추고 있다고 볼 수 있음. (그렇지만 학습이 잘 되었다는걸 보장하지는 못함)\n텍스트0-텍스트3\n\nprint(\"텍스트0 -- 텍스트3\")\n입력정보들_원시텍스트 = 데이터['train'][:4]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\nprint(f'실제정답: {데이터['train'][:4]['label']}') # 정답확인\n정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][:4]['label']) # 정답입력\n계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n출력정보들_확률 = np.exp(계산된숫자들_로짓)/np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\nprint(f'인공지능의예측: {계산된숫자들_로짓.argmax(axis=1)}')\nprint(f'인공지능의확신정도: {출력정보들_확률.max(axis=1)}')\nprint(f'손실(loss): {인공지능(**정리된숫자들_토큰화된자료).loss.item():.6f}')\n\n텍스트0 -- 텍스트3\n실제정답: [0, 0, 0, 0]\n인공지능의예측: [1 1 1 1]\n인공지능의확신정도: [0.5217618 0.5278467 0.5195052 0.5339033]\n손실(loss): 0.746100\n\n\n텍스트12498-텍스트12501\n\nprint(\"텍스트12498 -- 텍스트12501\")\n입력정보들_원시텍스트 = 데이터['train'][12498:12502]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\nprint(f'실제정답: {데이터['train'][12498:12502]['label']}') # 정답확인\n정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][12498:12502]['label']) # 정답입력\n계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n출력정보들_확률 = np.exp(계산된숫자들_로짓)/np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\nprint(f'인공지능의예측: {계산된숫자들_로짓.argmax(axis=1)}')\nprint(f'인공지능의확신정도: {출력정보들_확률.max(axis=1)}')\nprint(f'손실(loss): {인공지능(**정리된숫자들_토큰화된자료).loss.item():.6f}')\n\n텍스트12498 -- 텍스트12501\n실제정답: [0, 0, 1, 1]\n인공지능의예측: [1 1 1 1]\n인공지능의확신정도: [0.52731967 0.5243837  0.51578873 0.5351162 ]\n손실(loss): 0.694952\n\n\n텍스트12502-텍스트12506\n\nprint(\"텍스트12502 -- 텍스트12506\")\n입력정보들_원시텍스트 = 데이터['train'][12502:12506]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\nprint(f'실제정답: {데이터['train'][12502:12506]['label']}') # 정답확인\n정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][12502:12506]['label']) # 정답입력\n계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n출력정보들_확률 = np.exp(계산된숫자들_로짓)/np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\nprint(f'인공지능의예측: {계산된숫자들_로짓.argmax(axis=1)}')\nprint(f'인공지능의확신정도: {출력정보들_확률.max(axis=1)}')\nprint(f'손실(loss): {인공지능(**정리된숫자들_토큰화된자료).loss.item():.6f}')\n\n텍스트12502 -- 텍스트12506\n실제정답: [1, 1, 1, 1]\n인공지능의예측: [1 1 1 1]\n인공지능의확신정도: [0.52180934 0.51908445 0.521884   0.5197189 ]\n손실(loss): 0.652730\n\n\n- 똑같이 틀려도 오답에 대한 확신이 강할수록 loss가 크다.\n\nprint(\"텍스트0 -- 텍스트1\")\n입력정보들_원시텍스트 = 데이터['train'][:2]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\nprint(f'실제정답: {데이터['train'][:2]['label']}') # 정답확인\n정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][:2]['label']) # 정답입력\n계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n출력정보들_확률 = np.exp(계산된숫자들_로짓)/np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\nprint(f'인공지능의예측: {계산된숫자들_로짓.argmax(axis=1)}')\nprint(f'인공지능의확신정도: {출력정보들_확률.max(axis=1)}')\nprint(f'손실(loss): {인공지능(**정리된숫자들_토큰화된자료).loss.item():.6f}')\n\n텍스트0 -- 텍스트1\n실제정답: [0, 0]\n인공지능의예측: [1 1]\n인공지능의확신정도: [0.5217617 0.5278467]\n손실(loss): 0.744049\n\n\n\nprint(\"텍스트1 -- 텍스트2\")\n입력정보들_원시텍스트 = 데이터['train'][1:3]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\nprint(f'실제정답: {데이터['train'][1:3]['label']}') # 정답확인\n정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][1:3]['label']) # 정답입력\n계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n출력정보들_확률 = np.exp(계산된숫자들_로짓)/np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\nprint(f'인공지능의예측: {계산된숫자들_로짓.argmax(axis=1)}')\nprint(f'인공지능의확신정도: {출력정보들_확률.max(axis=1)}')\nprint(f'손실(loss): {인공지능(**정리된숫자들_토큰화된자료).loss.item():.6f}')\n\n텍스트1 -- 텍스트2\n실제정답: [0, 0]\n인공지능의예측: [1 1]\n인공지능의확신정도: [0.5278467 0.5195052]\n손실(loss): 0.741695\n\n\n\n\n\n\n\n\n\\(\\star\\star\\star\\) 학습이 가능한 이유 (대충 아이디어만)\n\n\n\n1. 랜덤으로 하나의 인공지능을 생성한다. (아래의 코드로 가능)\n인공지능 = 인공지능생성기()\n2. 하나의 미니배치를 선택한다.\n3. 인공지능의 파라메터중 하나의 숫자를 선택한다. 예를들면 아래와 같은 상황이 있다고 하자.\n인공지능.classifier.weight\nParameter containing:\ntensor([[-0.0234,  0.0279,  0.0242,  ...,  0.0091, -0.0063, -0.0133],\n        [ 0.0087,  0.0007, -0.0099,  ...,  0.0183, -0.0007,  0.0295]],\n       requires_grad=True)\n하나의 숫자 -0.0234를 선택한다.\n4. -0.0234의 값을 아주 조금 변화시킨다. 예를들면 -0.0233, -0.0235 와 같은 숫자로 바꾼다. 2에서 고정된 미니배치에 대하여 -0.0234, -0.0233, -0.0235 에 대한 loss를 계산해보고 비교한다.\n\n-0.0234이 최저 loss라면? 값을 안바꾸는게 좋겠음.\n-0.0233이 최저 loss라면?? 값을 -0.0233으로 바꿈.\n-0.0235이 최저 loss라면?? 값을 -0.0235으로 바꿈.\n\n5. 다음은 다른 모든 파라메터에 대하여 3-4을 반복한다. (과정을 반복할수록 loss는 작아지겠죠, 즉 인공지능은 정답을 잘 맞추겠죠)\n6. 다른 미니배치에 대하여 2-5를 반복한다.\n\n\n- 인공지능의 학습은 마법같은 신비한 현상이 아니고, 극한의 노가다를 통해 얻어지는 산물일 뿐이다."
  },
  {
    "objectID": "posts/01wk-2.html#a.-imdb",
    "href": "posts/01wk-2.html#a.-imdb",
    "title": "01wk-2: IMDB 자료 살펴보기, 지도학습의 개념",
    "section": "A. imdb",
    "text": "A. imdb\n- 이제 imdb가 뭔지 살펴보자..\n\n아마 데이터가 있겠죠?\n그런데 이걸 어떻게 보죠??\n\n- 뜯어보자..\n\nimdb # 데이터인듯\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})\n\n\n\n딕셔너리 아니야?\n\n- 딕셔너리는 아님\n\ntype(imdb)\n\ndatasets.dataset_dict.DatasetDict\n\n\n- 그런데 거의 딕셔너리 비슷한 느낌으로 일단 사용되는것 같음.\n\nimdb.keys()\n\ndict_keys(['train', 'test', 'unsupervised'])\n\n\n\nimdb['train']\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 25000\n})\n\n\n\nimdb['test']\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 25000\n})\n\n\n\nimdb['unsupervised']\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 50000\n})\n\n\n- imdb는 딕셔너리 같은 것이고, imdb['train']와 같은 명령어로 세부항목에 접근가능함. 즉 아래의 구조임.\n\nimdb['train'] \\(\\subset\\) imdb\nimdb['test'] \\(\\subset\\) imdb\nimdb['unsupervised'] \\(\\subset\\) imdb"
  },
  {
    "objectID": "posts/01wk-2.html#b.-imdbtrain",
    "href": "posts/01wk-2.html#b.-imdbtrain",
    "title": "01wk-2: IMDB 자료 살펴보기, 지도학습의 개념",
    "section": "B. imdb['train']",
    "text": "B. imdb['train']\n- imdb['train']을 살펴보자..\n\nimdb['train']\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 25000\n})\n\n\n- 이건 딕셔너리 처럼 안되는 것 같네..\n\nimdb['train'].keys()\n\nAttributeError: 'Dataset' object has no attribute 'keys'\n\n\n\nimdb['train']['features']\n\nKeyError: \"Column features not in the dataset. Current columns in the dataset: ['text', 'label']\"\n\n\n- 그럼 imdb['train']는 어떻게 쓰라는 것이냐?? \\(\\to\\) 쓸만한 기능이 있을지 dir로 체크 \\(\\to\\) __getitem__이 있음.. \\(\\to\\) imdb['train'][0] 를 써볼 용기가 생김..\n\nset(dir(imdb['train'])) & {'__getitem__'}\n\n{'__getitem__'}\n\n\n\nimdb['train'][0]\n\n{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.&lt;br /&gt;&lt;br /&gt;The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.&lt;br /&gt;&lt;br /&gt;What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.&lt;br /&gt;&lt;br /&gt;I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n 'label': 0}\n\n\n\n실행이 되었음.\n기쁨.\n\n- imdb['train'][1], imdb['train'][-1], imdb['train'][:2] 등을 실행해보자..\n\nimdb['train'][:5]\n\n{'text': ['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.&lt;br /&gt;&lt;br /&gt;The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.&lt;br /&gt;&lt;br /&gt;What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.&lt;br /&gt;&lt;br /&gt;I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n  '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.',\n  \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.&lt;br /&gt;&lt;br /&gt;One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).&lt;br /&gt;&lt;br /&gt;One might better spend one's time staring out a window at a tree growing.&lt;br /&gt;&lt;br /&gt;\",\n  \"This film was probably inspired by Godard's Masculin, féminin and I urge you to see that film instead.&lt;br /&gt;&lt;br /&gt;The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it's unattractive. Comparing to Godard's film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.&lt;br /&gt;&lt;br /&gt;A movie of its time, and place. 2/10.\",\n  'Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..&lt;br /&gt;&lt;br /&gt;\"Is that all there is??\" ...I was just an early teen when this smoked fish hit the U.S. I was too young to get in the theater (although I did manage to sneak into \"Goodbye Columbus\"). Then a screening at a local film museum beckoned - Finally I could see this film, except now I was as old as my parents were when they schlepped to see it!!&lt;br /&gt;&lt;br /&gt;The ONLY reason this film was not condemned to the anonymous sands of time was because of the obscenity case sparked by its U.S. release. MILLIONS of people flocked to this stinker, thinking they were going to see a sex film...Instead, they got lots of closeups of gnarly, repulsive Swedes, on-street interviews in bland shopping malls, asinie political pretension...and feeble who-cares simulated sex scenes with saggy, pale actors.&lt;br /&gt;&lt;br /&gt;Cultural icon, holy grail, historic artifact..whatever this thing was, shred it, burn it, then stuff the ashes in a lead box!&lt;br /&gt;&lt;br /&gt;Elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings..But if it weren\\'t for the censorship scandal, it would have been ignored, then forgotten.&lt;br /&gt;&lt;br /&gt;Instead, the \"I Am Blank, Blank\" rhythymed title was repeated endlessly for years as a titilation for porno films (I am Curious, Lavender - for gay films, I Am Curious, Black - for blaxploitation films, etc..) and every ten years or so the thing rises from the dead, to be viewed by a new generation of suckers who want to see that \"naughty sex film\" that \"revolutionized the film industry\"...&lt;br /&gt;&lt;br /&gt;Yeesh, avoid like the plague..Or if you MUST see it - rent the video and fast forward to the \"dirty\" parts, just to get it over with.&lt;br /&gt;&lt;br /&gt;'],\n 'label': [0, 0, 0, 0, 0]}\n\n\n\nimdb['train'][-1]\n\n{'text': 'The story centers around Barry McKenzie who must go to England if he wishes to claim his inheritance. Being about the grossest Aussie shearer ever to set foot outside this great Nation of ours there is something of a culture clash and much fun and games ensue. The songs of Barry McKenzie(Barry Crocker) are highlights.',\n 'label': 1}\n\n\n- imdb['train'][0] 을 구체적으로 살펴보자.\n\nprint(imdb['train'][0]['text'])\nprint(imdb['train'][0]['label'])\n\nI rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.&lt;br /&gt;&lt;br /&gt;The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.&lt;br /&gt;&lt;br /&gt;What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.&lt;br /&gt;&lt;br /&gt;I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n0\n\n\n- imdb['train'][1] 을 구체적으로 살펴보자.\n\nprint(imdb['train'][1]['text'])\nprint(imdb['train'][1]['label'])\n\n\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we're treated to the site of Vincent Gallo's throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won't see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women's bodies.\n0\n\n\n- imdb['train'][100] 을 구체적으로 살펴보자.\n\nprint(imdb['train'][100]['text'])\nprint(imdb['train'][100]['label'])\n\nTerrible movie. Nuff Said.&lt;br /&gt;&lt;br /&gt;These Lines are Just Filler. The movie was bad. Why I have to expand on that I don't know. This is already a waste of my time. I just wanted to warn others. Avoid this movie. The acting sucks and the writing is just moronic. Bad in every way. The only nice thing about the movie are Deniz Akkaya's breasts. Even that was ruined though by a terrible and unneeded rape scene. The movie is a poorly contrived and totally unbelievable piece of garbage.&lt;br /&gt;&lt;br /&gt;OK now I am just going to rag on IMDb for this stupid rule of 10 lines of text minimum. First I waste my time watching this offal. Then feeling compelled to warn others I create an account with IMDb only to discover that I have to write a friggen essay on the film just to express how bad I think it is. Totally unnecessary.\n0\n\n\n\n영어: Terrible movie. Nuff Said.These Lines are Just Filler. The movie was bad. Why I have to expand on that I don’t know. This is already a waste of my time. I just wanted to warn others. Avoid this movie. The acting sucks and the writing is just moronic. Bad in every way. The only nice thing about the movie are Deniz Akkaya’s breasts. Even that was ruined though by a terrible and unneeded rape scene. The movie is a poorly contrived and totally unbelievable piece of garbage.OK now I am just going to rag on IMDb for this stupid rule of 10 lines of text minimum. First I waste my time watching this offal. Then feeling compelled to warn others I create an account with IMDb only to discover that I have to write a friggen essay on the film just to express how bad I think it is. Totally unnecessary.\n\n\n한글: 끔찍한 영화. 더 할 말 없음. 이 줄들은 그냥 채우기일 뿐이다. 영화가 나빴다. 내가 왜 이걸 더 길게 써야 하는지 모르겠다. 이건 이미 내 시간 낭비다. 나는 그저 다른 사람들에게 경고하고 싶었을 뿐이다. 이 영화를 피하라. 연기도 형편없고, 대본도 완전히 멍청하다. 모든 면에서 나쁜 영화다. 영화에서 유일하게 좋은 것은 데니즈 아카야의 가슴뿐이었다. 하지만 그것조차도 끔찍하고 불필요한 강간 장면 때문에 망쳤다. 이 영화는 조잡하게 만들어졌고, 전혀 믿을 수 없는 쓰레기다. 이제 IMDb에 대한 불만을 좀 말하겠다. 10줄 이상 써야 한다는 이 어리석은 규칙 때문에 말이다. 먼저 이 쓰레기를 보면서 내 시간을 낭비했다. 그리고 다른 사람들에게 경고하려고 IMDb 계정을 만들었더니, 영화에 대해 내가 얼마나 나쁘게 생각하는지를 표현하려면 이 따위 에세이를 써야 한다는 걸 알게 되었다. 완전히 불필요하다.\n\n\n영화평인듯..\n겁나 뭐라고함.. 부정적임..\n이 text에 대한 label은 0\n\n- imdb['train'][-1] 을 구체적으로 살펴보자.\n\nprint(imdb['train'][-1]['text'])\nprint(imdb['train'][-1]['label'])\n\nThe story centers around Barry McKenzie who must go to England if he wishes to claim his inheritance. Being about the grossest Aussie shearer ever to set foot outside this great Nation of ours there is something of a culture clash and much fun and games ensue. The songs of Barry McKenzie(Barry Crocker) are highlights.\n1\n\n\n\n영어: The story centers around Barry McKenzie who must go to England if he wishes to claim his inheritance. Being about the grossest Aussie shearer ever to set foot outside this great Nation of ours there is something of a culture clash and much fun and games ensue. The songs of Barry McKenzie(Barry Crocker) are highlights.\n\n\n한글: 이 이야기는 유산을 상속받기 위해 영국으로 가야 하는 배리 맥켄지에 초점을 맞추고 있습니다. 배리 맥켄지는 이 위대한 호주를 떠난 사람 중 가장 거친 호주 농부로, 문화적 충돌이 일어나고 다양한 재미와 사건들이 이어집니다. 배리 맥켄지(배리 크로커)가 부르는 노래들이 이 영화의 하이라이트입니다.\n\n- 요약: imdb['train'] 에는 여러개의 영화평이 있고, 각각 긍정평가와 부정평가를 담고 있음.\n\n몇개의 영화평이 있냐? 25000\n부정평가는 0, 긍정평가는 1로 라벨링\n\n\nlen(imdb['train'])\n\n25000\n\n\n\nimdb['train'][24999]\n\n{'text': 'The story centers around Barry McKenzie who must go to England if he wishes to claim his inheritance. Being about the grossest Aussie shearer ever to set foot outside this great Nation of ours there is something of a culture clash and much fun and games ensue. The songs of Barry McKenzie(Barry Crocker) are highlights.',\n 'label': 1}\n\n\n\nimdb['train'][-1]\n\n{'text': 'The story centers around Barry McKenzie who must go to England if he wishes to claim his inheritance. Being about the grossest Aussie shearer ever to set foot outside this great Nation of ours there is something of a culture clash and much fun and games ensue. The songs of Barry McKenzie(Barry Crocker) are highlights.',\n 'label': 1}"
  },
  {
    "objectID": "posts/01wk-2.html#c.-imdbtest",
    "href": "posts/01wk-2.html#c.-imdbtest",
    "title": "01wk-2: IMDB 자료 살펴보기, 지도학습의 개념",
    "section": "C. imdb['test']",
    "text": "C. imdb['test']\n- imdb['train'] 과 비슷함..\n\nimdb['test'][0]\n\n{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n 'label': 0}\n\n\n\nlen(imdb['test'])\n\n25000"
  },
  {
    "objectID": "posts/01wk-2.html#d.-imdbunsupervised",
    "href": "posts/01wk-2.html#d.-imdbunsupervised",
    "title": "01wk-2: IMDB 자료 살펴보기, 지도학습의 개념",
    "section": "D. imdb['unsupervised']",
    "text": "D. imdb['unsupervised']\n- imdb['unsupervised'] 를 살펴보자.\n\nimdb['unsupervised'][3]\n\n{'text': \"Being that the only foreign films I usually like star a Japanese person in a rubber suit who crushes little tiny buildings and tanks, I had high hopes for this movie. I thought that this was a movie that wouldn't put me to sleep. WRONG! Starts off with a bang, okay, now she's in training, alright, she's an assassin, I'm still with you, oh, now she's having this moral dilemma and she can't decide if she loves her boyfriend or her controller, zzzzz.... Oh well, back to Gamera!\",\n 'label': -1}\n\n\n\nimdb['train'], imdb['test'] 와 비슷해 보이지만 살짝 다름.\nlabel 값이 특이하게 -1\n\n- 혹시 imdb['unsupervised'][??] 의 모든 라벨값이 모두 -1인가?\n\nset([l['label'] for l in imdb['unsupervised']])\n\n{-1}\n\n\n\n일단 라벨은 -1 밖에 없음.\n\n\nset([l['label'] for l in imdb['train']])\n\n{0, 1}\n\n\n\nset([l['label'] for l in imdb['test']])\n\n{0, 1}\n\n\n\nimdb['train'],\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 25000\n})\n\n\n\nimdb['test']\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 25000\n})\n\n\n- 느낌상 imdb['unsupervised']는 imdb['train'] 와 imdb['test'] 에서 text를 합치고, label은 모두 -1 로 바꾼 자료가 아닐까? 하는 의심이 들었음. —&gt; 아니었음..\n\ntxt0 = imdb['unsupervised'][0]['text']\ntxt0\n\n'This is just a precious little diamond. The play, the script are excellent. I cant compare this movie with anything else, maybe except the movie \"Leon\" wonderfully played by Jean Reno and Natalie Portman. But... What can I say about this one? This is the best movie Anne Parillaud has ever played in (See please \"Frankie Starlight\", she\\'s speaking English there) to see what I mean. The story of young punk girl Nikita, taken into the depraved world of the secret government forces has been exceptionally over used by Americans. Never mind the \"Point of no return\" and especially the \"La femme Nikita\" TV series. They cannot compare the original believe me! Trash these videos. Buy this one, do not rent it, BUY it. BTW beware of the subtitles of the LA company which \"translate\" the US release. What a disgrace! If you cant understand French, get a dubbed version. But you\\'ll regret later :)'\n\n\n\n[l for l in imdb['train'] if l['text'] == txt0]\n\n[]\n\n\n\n[l for l in imdb['test'] if l['text'] == txt0]\n\n[]\n\n\n\n[l for l in imdb['unsupervised'] if l['text'] == txt0]\n\n[{'text': 'This is just a precious little diamond. The play, the script are excellent. I cant compare this movie with anything else, maybe except the movie \"Leon\" wonderfully played by Jean Reno and Natalie Portman. But... What can I say about this one? This is the best movie Anne Parillaud has ever played in (See please \"Frankie Starlight\", she\\'s speaking English there) to see what I mean. The story of young punk girl Nikita, taken into the depraved world of the secret government forces has been exceptionally over used by Americans. Never mind the \"Point of no return\" and especially the \"La femme Nikita\" TV series. They cannot compare the original believe me! Trash these videos. Buy this one, do not rent it, BUY it. BTW beware of the subtitles of the LA company which \"translate\" the US release. What a disgrace! If you cant understand French, get a dubbed version. But you\\'ll regret later :)',\n  'label': -1}]"
  },
  {
    "objectID": "posts/01wk-2.html#f.-정리",
    "href": "posts/01wk-2.html#f.-정리",
    "title": "01wk-2: IMDB 자료 살펴보기, 지도학습의 개념",
    "section": "F. 정리",
    "text": "F. 정리\n- 요약:\n\nimdb는 imdb['train'], imdb['test'], imdb['unsupervised'] 로 나누어져 있음.\nimdb['train'], imdb['test'] 에는 각각 (text,label)와 같은 형식으로 정보가 저장되어 있음. 여기에서 label은 0 혹은 1의 값을 가지는데 0은 부정, 1은 긍정을 의미함.\nimdb['unsupervised'] 에도 조사해보니 각각 (text,label)와 같은 형식으로 정보가 저장되어 있었지만, 여기에서 label값은 모두 -1의 값만 있었음. 따라서 사살상 imdb['unsupervised']는 text에 대한 정보만 있다고 생각해도 무방. 그 text가 영화에 대한 긍정평가인지 부정평가인지 분류가 되어있지 않은 상태.\n\n- 외우세요: train, test, unsupervised 와 같은 단어는 매우 중요한 단어니까 일단 눈여겨보세요"
  },
  {
    "objectID": "posts/01wk-2.html#a.-기계학습딥러닝-과업",
    "href": "posts/01wk-2.html#a.-기계학습딥러닝-과업",
    "title": "01wk-2: IMDB 자료 살펴보기, 지도학습의 개념",
    "section": "A. 기계학습/딥러닝 과업",
    "text": "A. 기계학습/딥러닝 과업\n- 왜 데이터가 imdb['train'], imdb['test'], imdb['unsupervised'] 와 같이 나누어져 있는가?\n\n개념: 기계학습/딥러닝은 과업은 크게 지도학습과 비지도학습이 있음.\n데이터에서 imdb['train'], imdb['test'] 는 지도학습 모델을 배우기 위한 예제데이터이고, imdb['unsupervised']는 비지도학습모델을 배우기 위한 예제데이터임.\n아무튼 우리가 하는 “감성분석”은 지도학습이고, 따라서 우리는 imdb['train'], imdb['test'] 에만 관심을 가질 예정임."
  },
  {
    "objectID": "posts/01wk-2.html#b.-지도학습",
    "href": "posts/01wk-2.html#b.-지도학습",
    "title": "01wk-2: IMDB 자료 살펴보기, 지도학습의 개념",
    "section": "B. 지도학습",
    "text": "B. 지도학습\n- 지도학습이란? (이 예제에 한정하여 설명)\n\n자료가 “(텍스트, 라벨)” 의 형태로 정리가 되어 있을때, “텍스트”를 입력으로 주면 “라벨”을 출력해주는 함수 f를 찾는 일\n코드로 예를들어 설명하면 적당히 f라는 함수가 존재하여 아래와 같은 동작이 가능하도록 해야함.\n\nf(\"영화가 너무 재미없어요\")\n&gt; \"부정평가입니다\"\n\nf(\"영화, 괜찮은데요??\")\n&gt; \"긍정평가입니다\"\n\nf(\"배우들 연기 진짜 잘함. 영상미도 있음. 그런데 스토리 때문에 망했네.\")\n&gt; \"부정평가입니다\"\n- 이러한 함수 f를 우리가 잘 정의한다면 좋겠음. (가능한가??)\n- 대충 아래와 같은 과정을 거친다고 생각하면 편리함.\n\n정보(숫자,텍스트,이미지,…) \\(\\to\\) 숫자 \\(\\to\\) 계산 \\(\\to\\) 계산된숫자 \\(\\to\\) 정보\n\n- 예를들면 아래와 같은 방식이 가능\n\n긍정단어 = {'좋아', '재미있었음', '잘생김', '예뻐', '연기훌륭함'}\n부정단어 = {'지루해', '재미없었음', '비추천'}\n\n\ntxt0 = \"남주가 너무 잘생김 여주도 예뻐 영화가 중간에 조금 지루해 그래도 아무튼 재미있었음\"\n\n\n긍정단어수 = sum([l in 긍정단어 for l in txt0.split(' ')])\n부정단어수 = sum([l in 부정단어 for l in txt0.split(' ')])\n긍정평가일확률 = 긍정단어수/(긍정단어수+부정단어수)\n부정평가일확률 = 부정단어수/(긍정단어수+부정단어수)\n긍정평가일확률,부정평가일확률\n\n(0.75, 0.25)\n\n\n\n긍정평가군!\n\n\ndef f(text):\n    긍정단어수 = sum([l in 긍정단어 for l in text.split(' ')])\n    부정단어수 = sum([l in 부정단어 for l in text.split(' ')])\n    긍정평가일확률 = 긍정단어수/(긍정단어수+부정단어수)\n    부정평가일확률 = 부정단어수/(긍정단어수+부정단어수)\n    if 긍정평가일확률 &gt; 0.5:\n        return 긍정평가일확률,\"긍정평가\"\n    else:\n        return 부정평가일확률,\"부정평가\"\n\n\nf(\"남주가 너무 잘생김 여주도 예뻐 영화가 중간에 조금 지루해 그래도 아무튼 재미있었음\")\n\n(0.75, '긍정평가')\n\n\n- 당연히 현재는 많은 문제점이 있음.\n\nf(\"남주가 너무 잘생김. 여주도 예뻐. 영화가 중간에 조금 지루해 그래도 아무튼 재미있었음.\")\n\n(1.0, '부정평가')\n\n\n\nf(\"캐스팅 좋아 여주인공 특히 예뻐 배우들 연기훌륭함 그렇지만 스토리때문에 나는 개인적으로 비추천\")\n\n(0.75, '긍정평가')\n\n\n- 다행인점: 좋은 f를 만들기 위해서 우리가 고민할 필요 없음. (똑똑한 사람들이 다 만들어 놓음. 그리고 만들고 있음.)\n\n옛날방식: f 를 한땀한땀 설계. 초고수가 밤새 코딩해서 진짜 잘 맞추는 f를 한번에 제시.\n최근방식: f 를 대충 설계. 거의 멍청이 수준의 f임. 그런데 데이터를 줄수록 f가 점점 똑똑해짐. 나중에는 다 맞춤. –&gt; 인공지능???"
  },
  {
    "objectID": "posts/01wk-2.html#c.-traintest-자료의-의미",
    "href": "posts/01wk-2.html#c.-traintest-자료의-의미",
    "title": "01wk-2: IMDB 자료 살펴보기, 지도학습의 개념",
    "section": "C. train/test 자료의 의미",
    "text": "C. train/test 자료의 의미\n- 초고수가 f를 직접 설계하던 시대에는 별로 문제가 없었음. 그런데 최근 컴퓨터가 데이터를 보고 f를 스스로 수정하기 시작하면서 이상한 방식으로 f가 수정되는 경우가 보고되고 있음. f가 똑똑해 보이는데, 사실 멍청한 상태..\n\n어떻게 이런일이 가능하지??\n\n- 최규빈 교수의 착각\n\n상상: 나는 학생들에게 파이썬프로그래밍을 잘 강의하고 싶었다. 나는 학생들에게 다양한 문제를 풀어줬으며, 문제를 풀면서 학생들이 스스로 개념을 깨우치길 원했다. 나는 다양한 예시를 통해서 이해하는 것이 좋다고 생각했기 때문이다. 예시는 많을수록 좋으니까 한 학기동안 총 1000개의 문제를 풀어줬다. 기말고사는 풀어준 문제중에서 약 20문항을 샘플링하여 출제했다. 놀랍게도 학생들이 모두 만점을 받았으며 나는 아주 만족스러웠다. 한 학기 동안 고생한 보람이 있어보였다. 눈물이 흘렀다.\n질문: 최규빈 교수는 잘 평가한 걸까요? 학생들이 진짜 파이썬프로그래밍을 잘 이해했을까요?? (학과교수님들께 자랑해도 될까요?)\n이렇게 질문하고 싶지 않아요?: 1000개의 문제에서 샘플링하여 출제하지 않고, 새로운 문항을 개발하여 학생들에게 제시했다면??\n요령이 있는 교수라면 이렇게 했을거에요: 50000개의 문제세트가 있다고 하자. 수업시간에는 학생들에게 예시로 25000개 정도의 문항을 풀이하며 설명. 기말고사는 수업시간에 풀이하지 않은 25000개의 문항을 출제함.\n만약에 학생들이 수업시간에 풀어준 25000개의 문제를 올바르게 이해했다면, 수업시간에 풀이하지 않은 문항 25000개 역시 잘 풀었을 것임.\n\n- 이 상황을 살짝 말만 바꿔볼게요.\n\n상상: 나는 인공지능에게 “영화평가 텍스트를 주면 그것이 긍정평가인지 부정평가인지 판단하는 능력”을 잘 학습시키고 싶었다. 나는 인공지능에게 다양한 데이터를 제공했으며, 데이터를 보고 인공지능이 스스로 원리를 깨우치길 원했다. 데이터는 많을수록 좋으니까 약 50000개의 “(텍스트,라벨)” 쌍을 제공했다. 그리고 50000개의 “(텍스트.라벨)” 쌍에서 약 20문항을 샘플링하여 테스트했다. 놀랍게도 인공지능은 20문항을 모두 맞추었다. 나는 아주 만족스러웠다. 눈물이 흘렀다.\n질문: 저는 인공지능을 잘 학습시켰을까요?\n이렇게 하고 싶지 않아요?: 50000개의 데이터중, 25000개의 “(텍스트,라벨)”만 인공지능에게 제공하여 학습시킴. 그리고 나머지 25000개는 평가용으로 테스트해봄.\n만약에 인공지능이 진짜 영화평가 텍스트를 바탕으로 그것이 긍정평가인지 부정평가인지 판단하는 능력을 길렀다면?? 내가 인공지능에게 제공하지 않은 25000개의 데이터에 대해서도 함수 f 가 올바르게 동작해야함.\n\n- train data / test data\n\ntrain data 는 인공지능에게 학습용으로 제공하는 데이터\ntest data 는 인공지능이 진짜 잘 학습했는지 평가하기 위해 학습시 제공하지 않는 자료\n\n\nimdb\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})"
  },
  {
    "objectID": "posts/01wk-2.html#d.-trainvaltest-자료의-의미",
    "href": "posts/01wk-2.html#d.-trainvaltest-자료의-의미",
    "title": "01wk-2: IMDB 자료 살펴보기, 지도학습의 개념",
    "section": "D. train/val/test 자료의 의미",
    "text": "D. train/val/test 자료의 의미\n- 학생들 입장에서 생각해본다면??\n\n소망: 내가 외우려고 한게 아니고, 하도 공부를 많이 하다보니 문제가 외워졌음. 나도 그러기 싫음. 나도 내가 올바르게 공부했는지 체크하고 싶어.\n아이디어: 교수님이 풀어준 25000개의 문항중, 15000개만 전략적으로 공부함. 그리고 나머지 10000개는 나 스스로 올바르게 공부했는지 체크하는 용도로 삼음.\n진행사항:\n\n1일차: {15000문항: 50%맞음, 10000문항: 45%맞음}\n\n2일차: {15000문항: 90%맞음, 10000문항: 85%맞음}\n3일차: {15000문항: 100%맞음, 10000문항: 70%맞음} &lt;— 이런 경험 있어요??\n\n판단: 이게 한 3일차쯤 공부하다보니까 문제를 내가 너무 외운것 같네? 오히려 2일차일때의 느낌이 더 좋음. 그냥 2일차의 느낌으로 시험보러 가자!!\n\n- 이럴경우 아래와 같이 상황이 정리된다.\n\n원래: 교수가 수업시간에 풀어준 25000문제 = 학생이 공부할 25000문제 = train // 교수가 기말시험으로 제출할 25000문제 = test\n바뀐상황: 학생이 공부할 15000문제 = train / 학생이 자가진단용으로 으로 뺀 10000문제 = test // 교수가 기말시험으로 제출할 25000문제 = 찐test\n\n- 학생이 자가진단용으로 빼둔 10000개의 문항을 보통 validation set 이라고 부른다. 따라서 아래와 같이 정리 가능하다.\n\ntrain = 15000문제 = 학생이 스스로 공부\nvalidation = 10000문제 = 학생이 공부할때 사용하지 않음. 자가진단용.\ntest = 25000문제 = 교수가 출제하는 시험\n\n- train / validation / test 에 대한 용어는 엄밀하지 않게 사용되는 경우가 많아 그때그때 상황에 맞게 알아서 해석해야 한다.\n\n억지상황1: 교수가 시험보지 않음. 그런데 내가 스스로 공부하면서 잘 공부하고 있는지 체크하고 싶어서 1000개의 문제를 구하고 그중 매일 800개만 학습하고 200개는 검증용으로 사용함. 이 경우 200개의 문항을 validation 이라고 부르기도 하고 test 라고 부르기도 함. (엄밀하게는 validation이 맞다고 생각하지만, 외부데이터가 없는 상황이므로 validation과 test의 경계가 흐릿해짐)\n억지상황2: 나 혼자 1000개의 문항을 800/200으로 나누어 매일 공부하고 있었음. 이때 나는 200개의 문항을 test라고 부르기도 하고, validation이라고 부르기도 했음. 그런데 갑자기 교수가 나보고 외부 코딩대회에 나가라고 함. 이 경우 200개의 문항은 validation 이 되고 외부코딩대회에서 출제된 문항이 test가 됨.\n\n- 느낌: 아래가 가장 정확한 설명임\n\ntrain: 학습에 사용하는 자료\nvalidation: 학습에 사용하지 않는 자료. 왜 안써? 더 좋은 훈련을 위한 목적.\ntest: 학습에 사용하지 않는 자료. 왜 안써? 올바르게 학습됨을 평가하기 위한 목적.\n\n\n헷갈리는 이유는 더 좋은 훈련을 위한 목적과 올바르게 학습됨을 평가하기 위한 목적이 무 자르듯이 구분되지 않기 때문.\n\n- 이상한 분류법\n\n데이터를 보통 2개의 셋으로 나누면 train/test 로 3개로 나누면 train/test/validation 으로 많이 표현.\n딱 맞는 정의는 아님. 의미상 구분해야함."
  },
  {
    "objectID": "posts/01wk-1-강의소개.html",
    "href": "posts/01wk-1-강의소개.html",
    "title": "01wk-1: 강의소개",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. 이 수업을 들어야 하는 이유\n\npass\n\n\n\n3. 이 수업을 듣지 말아야 하는 이유\n1. F학점을 줄 수 있음.\n\n모든 퀴즈를 보더라도 성적미달시 F학점 부여\n졸업이 임박한 경우 수강을 권장하지 않음\n\n2. 플립러닝\n\n수업은 비대면수업으로 진행하며, 수업 시간에는 퀴즈를 봄.\n대면 수업에 익숙하고 비대면 수업에 익숙하지 않은 학생들의 경우 수강을 권장하지 않음.\n매주 진행되는 퀴즈가 부담스러운 학생은 수강을 권장하지 않음.\n\n3. 기계학습, 기계학습활용\n\n기계학습활용: 이론적인 설명을 최소화하고, 실습 및 활용에 중점을 둔 수업\n기계학습: 이론과 실습을 같이 배우는 수업\n기계학습활용과 기계학습은 선수과목관계가 아니며, 기계학습활용을 듣지 않고도 기계학습을 이해하는데 문제가 없음.\n\n4. 잘하는 사람들이 많다.\n\n통계학과 고학년, 타학과 고수들\n\n5. 학점이 짜다.\n\n짜게 느껴진다가 더 정확한 표현같아요\n\n6. 파이썬문법이 선행되어야 함.\n\n리스트를 만드는 방법, numpy array가 무엇인지, colab 사용방법 등을 설명하지 않음.\n\n7. cost-effective 하지 않다.\n\n여러가지 이유로..\n이 교과목을 위해서 너무 많은 노력을 해야한다면 드랍하는 것이 좋다고 생각함.\n\n\n\n4. 학점산정방식\n- 강의계획서: 중간40, 기말40, 출석10, 과제10\n- 실제운영: 퀴즈90, 과제10\n\n매주 퀴즈를 보므로 출석은 퀴즈에 포함\n중간/기말 대신 매주 퀴즈로 평가하므로 중간/기말 점수도 퀴즈에 포함된다고 볼 수 있음."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/04wk-1.html#a.-트레이너의-제1역할-cpu에서-gpu로..",
    "href": "posts/04wk-1.html#a.-트레이너의-제1역할-cpu에서-gpu로..",
    "title": "04wk-1: 감성분석 파고들기 (2)",
    "section": "A. 트레이너의 제1역할 – CPU에서 GPU로..",
    "text": "A. 트레이너의 제1역할 – CPU에서 GPU로..\n\n# 트레이너 생성전\n- 인공지능의 파라메터 상태확인 1\n\n인공지능.classifier.weight\n\nParameter containing:\ntensor([[-0.0234,  0.0279,  0.0242,  ...,  0.0091, -0.0063, -0.0133],\n        [ 0.0087,  0.0007, -0.0099,  ...,  0.0183, -0.0007,  0.0295]],\n       requires_grad=True)\n\n\n\n중요한내용1: 숫자들 = 초기숫자들\n중요한내용2: 숫자들이 CPU에 존재한다는 의미\n\n- 인공지능을 이용한 예측 1\n\n확률계산하기(인공지능(**데이터콜렉터([토크나이저(\"This movie was a huge disappointment.\")])))\n\ntensor([[0.4642, 0.5358]], grad_fn=&lt;DivBackward0&gt;)\n\n\n\n확률계산하기(인공지능(**데이터콜렉터([토크나이저(\"This was a masterpiece.\")])))\n\ntensor([[0.4664, 0.5336]], grad_fn=&lt;DivBackward0&gt;)\n\n\n\n\n# 트레이너 생성후\n- 트레이너생성\n\n## Step3 \n트레이너세부지침 = 트레이너세부지침생성기(\n    output_dir=\"my_awesome_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2, # 전체문제세트를 2번 공부하라..\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n)\n트레이너 = 트레이너생성기(\n    model=인공지능,\n    args=트레이너세부지침,\n    train_dataset=전처리된훈련자료,\n    eval_dataset=전처리된검증자료,\n    tokenizer=토크나이저,\n    data_collator=데이터콜렉터,\n    compute_metrics=평가하기,\n)\n\n- 인공지능의 파라메터 상태확인 2\n\n인공지능.classifier.weight\n\nParameter containing:\ntensor([[-0.0234,  0.0279,  0.0242,  ...,  0.0091, -0.0063, -0.0133],\n        [ 0.0087,  0.0007, -0.0099,  ...,  0.0183, -0.0007,  0.0295]],\n       device='cuda:0', requires_grad=True)\n\n\n\n중요한내용1: 숫자들 = 초기숫자들\n중요한내용2: device=“cuda:0” // 숫자들이 GPU에 있다는 의미\n\n- 인공지능을 이용한 예측 2\n\n확률계산하기(인공지능(**데이터콜렉터([토크나이저(\"This movie was a huge disappointment.\")]).to(\"cuda:0\")))\n\ntensor([[0.4642, 0.5358]], device='cuda:0', grad_fn=&lt;DivBackward0&gt;)\n\n\n\n확률계산하기(인공지능(**데이터콜렉터([토크나이저(\"This was a masterpiece.\")]).to(\"cuda:0\")))\n\ntensor([[0.4664, 0.5336]], device='cuda:0', grad_fn=&lt;DivBackward0&gt;)\n\n\n\n트레이너의 제1역할: 트레이너는 생성과 동시에 하는역할이 있는데, 바로 인공지능의 파라메터를 GPU에 올리는 것이다."
  },
  {
    "objectID": "posts/04wk-1.html#b.-트레이너의-제2역할-예측하기",
    "href": "posts/04wk-1.html#b.-트레이너의-제2역할-예측하기",
    "title": "04wk-1: 감성분석 파고들기 (2)",
    "section": "B. 트레이너의 제2역할 – 예측하기",
    "text": "B. 트레이너의 제2역할 – 예측하기\n\n트레이너의 제2역할: 트레이너.predict() 사용가능. 트레이너.predict()의 입력형태는 input_ids, attention_mask, label 이 존재하는 Dataset\n\n# 예제1 트레이너를 이용한 예측\n\nsample_dict = {\n    'text': [\"This movie was a huge disappointment.\"],\n    'label': [0],\n    'input_ids': [[101, 2023, 3185, 2001, 1037, 4121, 10520, 1012, 102]],\n    'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]\n}\nsample_dataset = datasets.Dataset.from_dict(sample_dict)\nsample_dataset\n\nDataset({\n    features: ['text', 'label', 'input_ids', 'attention_mask'],\n    num_rows: 1\n})\n\n\n\n트레이너.predict(sample_dataset)\n\n\n\n\nPredictionOutput(predictions=array([[-0.11731032,  0.02610314]], dtype=float32), label_ids=array([0]), metrics={'test_loss': 0.7674226760864258, 'test_model_preparation_time': 0.0009, 'test_accuracy': 0.0, 'test_runtime': 1.1868, 'test_samples_per_second': 0.843, 'test_steps_per_second': 0.843})\n\n\n\nlogits = np.array([[-0.11731032,  0.02610314]])\nnp.exp(logits)/ np.exp(logits).sum(axis=1)\n\narray([[0.46420796, 0.53579204]])\n\n\n#\n# 예제2 – 트레이너를 이용하여 train_data, test_data 의 prediction 값을 구하라.\n\n트레이너.predict(전처리된데이터['train'])\n\n\n\n\nPredictionOutput(predictions=array([[-0.08470809,  0.0023939 ],\n       [-0.08299972,  0.02850237],\n       [-0.06004279,  0.01801764],\n       ...,\n       [-0.02317078, -0.01451463],\n       [-0.00802051, -0.02698467],\n       [-0.03900156, -0.02573229]], dtype=float32), label_ids=array([0, 0, 0, ..., 1, 1, 1]), metrics={'test_loss': 0.6949957609176636, 'test_model_preparation_time': 0.0009, 'test_accuracy': 0.4916, 'test_runtime': 89.9353, 'test_samples_per_second': 277.978, 'test_steps_per_second': 17.379})\n\n\n\n트레이너.predict(전처리된데이터['test'])\n\n\n\n\nPredictionOutput(predictions=array([[-0.03815563,  0.00212397],\n       [-0.08166712, -0.00432102],\n       [-0.10371644,  0.03148611],\n       ...,\n       [-0.08171435, -0.00681646],\n       [-0.09139054,  0.01050513],\n       [-0.06704493,  0.0221498 ]], dtype=float32), label_ids=array([0, 0, 0, ..., 1, 1, 1]), metrics={'test_loss': 0.6949934363365173, 'test_model_preparation_time': 0.0009, 'test_accuracy': 0.4912, 'test_runtime': 89.8578, 'test_samples_per_second': 278.217, 'test_steps_per_second': 17.394})\n\n\n#"
  },
  {
    "objectID": "posts/04wk-1.html#c.-트레이너의-제3역할-학습-및-결과저장",
    "href": "posts/04wk-1.html#c.-트레이너의-제3역할-학습-및-결과저장",
    "title": "04wk-1: 감성분석 파고들기 (2)",
    "section": "C. 트레이너의 제3역할 – 학습 및 결과저장",
    "text": "C. 트레이너의 제3역할 – 학습 및 결과저장\n\n# 학습\n\n트레이너.train()\n\n\n    \n      \n      \n      [3126/3126 11:57, Epoch 2/2]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nModel Preparation Time\nAccuracy\n\n\n\n\n1\n0.221900\n0.199376\n0.001900\n0.923000\n\n\n2\n0.145700\n0.233206\n0.001900\n0.931000\n\n\n\n\n\n\nTrainOutput(global_step=3126, training_loss=0.20330691871472223, metrics={'train_runtime': 718.1146, 'train_samples_per_second': 69.627, 'train_steps_per_second': 4.353, 'total_flos': 6556904415524352.0, 'train_loss': 0.20330691871472223, 'epoch': 2.0})\n\n\n\n25000 / 16 \n\n1562.5\n\n\n\n1563 * 2 \n\n3126\n\n\n\n\n# 학습후\n- 인공지능이 똑똑해졌을까?\n- 인공지능의 파라메터 상태확인 3\n\n인공지능.classifier.weight\n\nParameter containing:\ntensor([[-0.0230,  0.0279,  0.0239,  ...,  0.0085, -0.0062, -0.0143],\n        [ 0.0084,  0.0007, -0.0097,  ...,  0.0189, -0.0008,  0.0304]],\n       device='cuda:0', requires_grad=True)\n\n\n인공지능의 파라메터 상태확인 2와 비교삿\nParameter containing:\ntensor([[-0.0234,  0.0279,  0.0242,  ...,  0.0091, -0.0063, -0.0133],\n        [ 0.0087,  0.0007, -0.0099,  ...,  0.0183, -0.0007,  0.0295]],\n       device='cuda:0', requires_grad=True)\n숫자들이 바뀐걸 확인 \\(\\to\\) 뭔가 다른 계산결과를 준다는 의미겠지? \\(\\to\\) 진짜 그런지 보자..\n- 인공지능을 이용한 예측 3\n\n확률계산하기(인공지능(**데이터콜렉터([토크나이저(\"This movie was a huge disappointment.\")]).to(\"cuda:0\")))\n\ntensor([[0.9885, 0.0115]], device='cuda:0', grad_fn=&lt;DivBackward0&gt;)\n\n\n\n확률계산하기(인공지능(**데이터콜렉터([토크나이저(\"This was a masterpiece.\")]).to(\"cuda:0\")))\n\ntensor([[0.0219, 0.9781]], device='cuda:0', grad_fn=&lt;DivBackward0&gt;)\n\n\n- 다시 트레이너를 이용하여 train_data, test_data 의 prediction 값을 구해보자.\n\n트레이너.predict(전처리된데이터['train'])\n\n\n\n\nPredictionOutput(predictions=array([[ 1.5927906 , -1.3617778 ],\n       [ 2.36449   , -2.1244614 ],\n       [ 2.1150742 , -1.9663404 ],\n       ...,\n       [-2.690494  ,  2.2624812 ],\n       [ 0.32332823, -0.05149931],\n       [-1.96404   ,  1.7741865 ]], dtype=float32), label_ids=array([0, 0, 0, ..., 1, 1, 1]), metrics={'test_loss': 0.1358911097049713, 'test_model_preparation_time': 0.0008, 'test_accuracy': 0.95244, 'test_runtime': 89.3077, 'test_samples_per_second': 279.931, 'test_steps_per_second': 17.501})\n\n\n\n트레이너.predict(전처리된데이터['test'])\n\nPredictionOutput(predictions=array([[ 2.5778208 , -2.250055  ],\n       [ 1.396875  , -1.2021751 ],\n       [ 2.1249173 , -1.9882215 ],\n       ...,\n       [-0.3729212 ,  0.34651938],\n       [ 0.19383232,  0.01419903],\n       [-1.2160491 ,  1.007748  ]], dtype=float32), label_ids=array([0, 0, 0, ..., 1, 1, 1]), metrics={'test_loss': 0.19937647879123688, 'test_model_preparation_time': 0.0008, 'test_accuracy': 0.923, 'test_runtime': 90.2744, 'test_samples_per_second': 276.933, 'test_steps_per_second': 17.314})\n\n\n- 우리가 가져야할 생각: 신기하다 X // 노가다 많이 했구나.. O"
  },
  {
    "objectID": "posts/02wk-1.html#a.-step1-데이터",
    "href": "posts/02wk-1.html#a.-step1-데이터",
    "title": "02wk-1: IMDB 영화평 감성분석",
    "section": "A. Step1 – 데이터",
    "text": "A. Step1 – 데이터\n- 데이터불러오기\n\nfrom datasets import load_dataset\nimdb = load_dataset(\"imdb\")\n\n/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n\n/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n\n\n뭐하는거?\n\ntokenizer(\"Transformers are really useful for natural language processing.\") # 텍스트 -&gt; 숫자들 \n\n{'input_ids': [101, 19081, 2024, 2428, 6179, 2005, 3019, 2653, 6364, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n- preprocess_function 를 선언\n\ndef preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True)\n\n뭐하는 함수?\n\ntokenizer(\"Transformers are really useful for natural language processing.\",truncation=True) # 텍스트 -&gt; 숫자들 \n\n{'input_ids': [101, 19081, 2024, 2428, 6179, 2005, 3019, 2653, 6364, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\nexamples = imdb['train'][1]\n#tokenizer(examples[\"text\"], truncation=True)\npreprocess_function(examples)\n\n{'input_ids': [101, 1000, 1045, 2572, 8025, 1024, 3756, 1000, 2003, 1037, 15544, 19307, 1998, 3653, 6528, 20771, 19986, 8632, 1012, 2009, 2987, 1005, 1056, 3043, 2054, 2028, 1005, 1055, 2576, 5328, 2024, 2138, 2023, 2143, 2064, 6684, 2022, 2579, 5667, 2006, 2151, 2504, 1012, 2004, 2005, 1996, 4366, 2008, 19124, 3287, 16371, 25469, 2003, 2019, 6882, 13316, 1011, 2459, 1010, 2008, 3475, 1005, 1056, 2995, 1012, 1045, 1005, 2310, 2464, 1054, 1011, 6758, 3152, 2007, 3287, 16371, 25469, 1012, 4379, 1010, 2027, 2069, 3749, 2070, 25085, 5328, 1010, 2021, 2073, 2024, 1996, 1054, 1011, 6758, 3152, 2007, 21226, 24728, 22144, 2015, 1998, 20916, 4691, 6845, 2401, 1029, 7880, 1010, 2138, 2027, 2123, 1005, 1056, 4839, 1012, 1996, 2168, 3632, 2005, 2216, 10231, 7685, 5830, 3065, 1024, 8040, 7317, 5063, 2015, 11820, 1999, 1996, 9478, 2021, 2025, 1037, 17962, 21239, 1999, 4356, 1012, 1998, 2216, 3653, 6528, 20771, 10271, 5691, 2066, 1996, 2829, 16291, 1010, 1999, 2029, 2057, 1005, 2128, 5845, 2000, 1996, 2609, 1997, 6320, 25624, 1005, 1055, 17061, 3779, 1010, 2021, 2025, 1037, 7637, 1997, 5061, 5710, 2006, 9318, 7367, 5737, 19393, 1012, 2077, 6933, 1006, 2030, 20242, 1007, 1000, 3313, 1011, 3115, 1000, 1999, 5609, 1997, 16371, 25469, 1010, 1996, 10597, 27885, 5809, 2063, 2323, 2202, 2046, 4070, 2028, 14477, 6767, 8524, 6321, 5793, 28141, 4489, 2090, 2273, 1998, 2308, 1024, 2045, 2024, 2053, 8991, 18400, 2015, 2006, 4653, 2043, 19910, 3544, 15287, 1010, 1998, 1996, 2168, 3685, 2022, 2056, 2005, 1037, 2158, 1012, 1999, 2755, 1010, 2017, 3227, 2180, 1005, 1056, 2156, 2931, 8991, 18400, 2015, 1999, 2019, 2137, 2143, 1999, 2505, 2460, 1997, 22555, 2030, 13216, 14253, 2050, 1012, 2023, 6884, 3313, 1011, 3115, 2003, 2625, 1037, 3313, 3115, 2084, 2019, 4914, 2135, 2139, 24128, 3754, 2000, 2272, 2000, 3408, 20547, 2007, 1996, 19008, 1997, 2308, 1005, 1055, 4230, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\nexamples = imdb['train'][:2]\n#tokenizer(examples[\"text\"], truncation=True)\npreprocess_function(examples)\n\n{'input_ids': [[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102], [101, 1000, 1045, 2572, 8025, 1024, 3756, 1000, 2003, 1037, 15544, 19307, 1998, 3653, 6528, 20771, 19986, 8632, 1012, 2009, 2987, 1005, 1056, 3043, 2054, 2028, 1005, 1055, 2576, 5328, 2024, 2138, 2023, 2143, 2064, 6684, 2022, 2579, 5667, 2006, 2151, 2504, 1012, 2004, 2005, 1996, 4366, 2008, 19124, 3287, 16371, 25469, 2003, 2019, 6882, 13316, 1011, 2459, 1010, 2008, 3475, 1005, 1056, 2995, 1012, 1045, 1005, 2310, 2464, 1054, 1011, 6758, 3152, 2007, 3287, 16371, 25469, 1012, 4379, 1010, 2027, 2069, 3749, 2070, 25085, 5328, 1010, 2021, 2073, 2024, 1996, 1054, 1011, 6758, 3152, 2007, 21226, 24728, 22144, 2015, 1998, 20916, 4691, 6845, 2401, 1029, 7880, 1010, 2138, 2027, 2123, 1005, 1056, 4839, 1012, 1996, 2168, 3632, 2005, 2216, 10231, 7685, 5830, 3065, 1024, 8040, 7317, 5063, 2015, 11820, 1999, 1996, 9478, 2021, 2025, 1037, 17962, 21239, 1999, 4356, 1012, 1998, 2216, 3653, 6528, 20771, 10271, 5691, 2066, 1996, 2829, 16291, 1010, 1999, 2029, 2057, 1005, 2128, 5845, 2000, 1996, 2609, 1997, 6320, 25624, 1005, 1055, 17061, 3779, 1010, 2021, 2025, 1037, 7637, 1997, 5061, 5710, 2006, 9318, 7367, 5737, 19393, 1012, 2077, 6933, 1006, 2030, 20242, 1007, 1000, 3313, 1011, 3115, 1000, 1999, 5609, 1997, 16371, 25469, 1010, 1996, 10597, 27885, 5809, 2063, 2323, 2202, 2046, 4070, 2028, 14477, 6767, 8524, 6321, 5793, 28141, 4489, 2090, 2273, 1998, 2308, 1024, 2045, 2024, 2053, 8991, 18400, 2015, 2006, 4653, 2043, 19910, 3544, 15287, 1010, 1998, 1996, 2168, 3685, 2022, 2056, 2005, 1037, 2158, 1012, 1999, 2755, 1010, 2017, 3227, 2180, 1005, 1056, 2156, 2931, 8991, 18400, 2015, 1999, 2019, 2137, 2143, 1999, 2505, 2460, 1997, 22555, 2030, 13216, 14253, 2050, 1012, 2023, 6884, 3313, 1011, 3115, 2003, 2625, 1037, 3313, 3115, 2084, 2019, 4914, 2135, 2139, 24128, 3754, 2000, 2272, 2000, 3408, 20547, 2007, 1996, 19008, 1997, 2308, 1005, 1055, 4230, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n\n\n- map 메소드 사용\n\ntokenized_imdb = imdb.map(preprocess_function, batched=True)\n\n\n#imdb['train'][0], tokenized_imdb['train'][0]\n\n\nfor i in range(5):\n    print(preprocess_function(imdb['train'][i]))\n\n{'input_ids': [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n{'input_ids': [101, 1000, 1045, 2572, 8025, 1024, 3756, 1000, 2003, 1037, 15544, 19307, 1998, 3653, 6528, 20771, 19986, 8632, 1012, 2009, 2987, 1005, 1056, 3043, 2054, 2028, 1005, 1055, 2576, 5328, 2024, 2138, 2023, 2143, 2064, 6684, 2022, 2579, 5667, 2006, 2151, 2504, 1012, 2004, 2005, 1996, 4366, 2008, 19124, 3287, 16371, 25469, 2003, 2019, 6882, 13316, 1011, 2459, 1010, 2008, 3475, 1005, 1056, 2995, 1012, 1045, 1005, 2310, 2464, 1054, 1011, 6758, 3152, 2007, 3287, 16371, 25469, 1012, 4379, 1010, 2027, 2069, 3749, 2070, 25085, 5328, 1010, 2021, 2073, 2024, 1996, 1054, 1011, 6758, 3152, 2007, 21226, 24728, 22144, 2015, 1998, 20916, 4691, 6845, 2401, 1029, 7880, 1010, 2138, 2027, 2123, 1005, 1056, 4839, 1012, 1996, 2168, 3632, 2005, 2216, 10231, 7685, 5830, 3065, 1024, 8040, 7317, 5063, 2015, 11820, 1999, 1996, 9478, 2021, 2025, 1037, 17962, 21239, 1999, 4356, 1012, 1998, 2216, 3653, 6528, 20771, 10271, 5691, 2066, 1996, 2829, 16291, 1010, 1999, 2029, 2057, 1005, 2128, 5845, 2000, 1996, 2609, 1997, 6320, 25624, 1005, 1055, 17061, 3779, 1010, 2021, 2025, 1037, 7637, 1997, 5061, 5710, 2006, 9318, 7367, 5737, 19393, 1012, 2077, 6933, 1006, 2030, 20242, 1007, 1000, 3313, 1011, 3115, 1000, 1999, 5609, 1997, 16371, 25469, 1010, 1996, 10597, 27885, 5809, 2063, 2323, 2202, 2046, 4070, 2028, 14477, 6767, 8524, 6321, 5793, 28141, 4489, 2090, 2273, 1998, 2308, 1024, 2045, 2024, 2053, 8991, 18400, 2015, 2006, 4653, 2043, 19910, 3544, 15287, 1010, 1998, 1996, 2168, 3685, 2022, 2056, 2005, 1037, 2158, 1012, 1999, 2755, 1010, 2017, 3227, 2180, 1005, 1056, 2156, 2931, 8991, 18400, 2015, 1999, 2019, 2137, 2143, 1999, 2505, 2460, 1997, 22555, 2030, 13216, 14253, 2050, 1012, 2023, 6884, 3313, 1011, 3115, 2003, 2625, 1037, 3313, 3115, 2084, 2019, 4914, 2135, 2139, 24128, 3754, 2000, 2272, 2000, 3408, 20547, 2007, 1996, 19008, 1997, 2308, 1005, 1055, 4230, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n{'input_ids': [101, 2065, 2069, 2000, 4468, 2437, 2023, 2828, 1997, 2143, 1999, 1996, 2925, 1012, 2023, 2143, 2003, 5875, 2004, 2019, 7551, 2021, 4136, 2053, 2522, 11461, 2466, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2514, 6819, 5339, 8918, 2005, 3564, 27046, 2009, 2138, 2009, 12817, 2006, 2061, 2116, 2590, 3314, 2021, 2009, 2515, 2061, 2302, 2151, 5860, 11795, 3085, 15793, 1012, 1996, 13972, 3310, 2185, 2007, 2053, 2047, 15251, 1006, 4983, 2028, 3310, 2039, 2007, 2028, 2096, 2028, 1005, 1055, 2568, 17677, 2015, 1010, 2004, 2009, 2097, 26597, 2079, 2076, 2023, 23100, 2143, 1007, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2488, 5247, 2028, 1005, 1055, 2051, 4582, 2041, 1037, 3332, 2012, 1037, 3392, 3652, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n{'input_ids': [101, 2023, 2143, 2001, 2763, 4427, 2011, 2643, 4232, 1005, 1055, 16137, 10841, 4115, 1010, 10768, 25300, 2078, 1998, 1045, 9075, 2017, 2000, 2156, 2008, 2143, 2612, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2143, 2038, 2048, 2844, 3787, 1998, 2216, 2024, 1010, 1006, 1015, 1007, 1996, 12689, 3772, 1006, 1016, 1007, 1996, 8052, 1010, 6151, 6810, 2099, 7178, 2135, 2204, 1010, 6302, 1012, 4237, 2013, 2008, 1010, 2054, 9326, 2033, 2087, 2003, 1996, 10866, 5460, 1997, 9033, 21202, 7971, 1012, 14229, 6396, 2386, 2038, 2000, 2022, 2087, 15703, 3883, 1999, 1996, 2088, 1012, 2016, 4490, 2061, 5236, 1998, 2007, 2035, 1996, 16371, 25469, 1999, 2023, 2143, 1010, 1012, 1012, 1012, 2009, 1005, 1055, 14477, 4779, 26884, 1012, 13599, 2000, 2643, 4232, 1005, 1055, 2143, 1010, 7789, 3012, 2038, 2042, 2999, 2007, 28072, 1012, 2302, 2183, 2205, 2521, 2006, 2023, 3395, 1010, 1045, 2052, 2360, 2008, 4076, 2013, 1996, 4489, 1999, 15084, 2090, 1996, 2413, 1998, 1996, 4467, 2554, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1037, 3185, 1997, 2049, 2051, 1010, 1998, 2173, 1012, 1016, 1013, 2184, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n{'input_ids': [101, 2821, 1010, 2567, 1012, 1012, 1012, 2044, 4994, 2055, 2023, 9951, 2143, 2005, 8529, 13876, 12129, 2086, 2035, 1045, 2064, 2228, 1997, 2003, 2008, 2214, 14911, 3389, 2299, 1012, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1000, 2003, 2008, 2035, 2045, 2003, 1029, 1029, 1000, 1012, 1012, 1012, 1045, 2001, 2074, 2019, 2220, 9458, 2043, 2023, 20482, 3869, 2718, 1996, 1057, 1012, 1055, 1012, 1045, 2001, 2205, 2402, 2000, 2131, 1999, 1996, 4258, 1006, 2348, 1045, 2106, 6133, 2000, 13583, 2046, 1000, 9119, 8912, 1000, 1007, 1012, 2059, 1037, 11326, 2012, 1037, 2334, 2143, 2688, 10272, 17799, 1011, 2633, 1045, 2071, 2156, 2023, 2143, 1010, 3272, 2085, 1045, 2001, 2004, 2214, 2004, 2026, 3008, 2020, 2043, 2027, 8040, 7317, 13699, 5669, 2000, 2156, 2009, 999, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2069, 3114, 2023, 2143, 2001, 2025, 10033, 2000, 1996, 10812, 13457, 1997, 2051, 2001, 2138, 1997, 1996, 27885, 11020, 20693, 2553, 13977, 2011, 2049, 1057, 1012, 1055, 1012, 2713, 1012, 8817, 1997, 2111, 19311, 2098, 2000, 2023, 27136, 2121, 1010, 3241, 2027, 2020, 2183, 2000, 2156, 1037, 3348, 2143, 1012, 1012, 1012, 2612, 1010, 2027, 2288, 7167, 1997, 2485, 22264, 1997, 1043, 11802, 2135, 1010, 16360, 23004, 25430, 18352, 1010, 2006, 1011, 2395, 7636, 1999, 20857, 6023, 25943, 1010, 2004, 5498, 2063, 2576, 3653, 29048, 1012, 1012, 1012, 1998, 7408, 3468, 2040, 1011, 14977, 23599, 3348, 5019, 2007, 7842, 22772, 1010, 5122, 5889, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 3451, 12696, 1010, 4151, 24665, 12502, 1010, 3181, 20785, 1012, 1012, 3649, 2023, 2518, 2001, 1010, 14021, 5596, 2009, 1010, 6402, 2009, 1010, 2059, 4933, 1996, 11289, 1999, 1037, 2599, 3482, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 7069, 9765, 27065, 2229, 2145, 26988, 2000, 2424, 3643, 1999, 2049, 11771, 18404, 6208, 2576, 11867, 7974, 8613, 1012, 1012, 2021, 2065, 2009, 4694, 1005, 1056, 2005, 1996, 15657, 9446, 1010, 2009, 2052, 2031, 2042, 6439, 1010, 2059, 6404, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2612, 1010, 1996, 1000, 1045, 2572, 8744, 1010, 8744, 1000, 1054, 10536, 16921, 7583, 2516, 2001, 5567, 10866, 2135, 2005, 2086, 2004, 1037, 14841, 26065, 3508, 2005, 22555, 2080, 3152, 1006, 1045, 2572, 8025, 1010, 20920, 1011, 2005, 5637, 3152, 1010, 1045, 2572, 8025, 1010, 2304, 1011, 2005, 1038, 2721, 2595, 24759, 28100, 3370, 3152, 1010, 4385, 1012, 1012, 1007, 1998, 2296, 2702, 2086, 2030, 2061, 1996, 2518, 9466, 2013, 1996, 2757, 1010, 2000, 2022, 7021, 2011, 1037, 2047, 4245, 1997, 26476, 2015, 2040, 2215, 2000, 2156, 2008, 1000, 20355, 3348, 2143, 1000, 2008, 1000, 4329, 3550, 1996, 2143, 3068, 1000, 1012, 1012, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 6300, 9953, 1010, 4468, 2066, 1996, 11629, 1012, 1012, 2030, 2065, 2017, 2442, 2156, 2009, 1011, 9278, 1996, 2678, 1998, 3435, 2830, 2000, 1996, 1000, 6530, 1000, 3033, 1010, 2074, 2000, 2131, 2009, 2058, 2007, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\nprint(tokenized_imdb['train'][0]['input_ids'])\nprint(tokenized_imdb['train'][1]['input_ids'])\nprint(tokenized_imdb['train'][2]['input_ids'])\nprint(tokenized_imdb['train'][3]['input_ids'])\nprint(tokenized_imdb['train'][4]['input_ids'])\n\n[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102]\n[101, 1000, 1045, 2572, 8025, 1024, 3756, 1000, 2003, 1037, 15544, 19307, 1998, 3653, 6528, 20771, 19986, 8632, 1012, 2009, 2987, 1005, 1056, 3043, 2054, 2028, 1005, 1055, 2576, 5328, 2024, 2138, 2023, 2143, 2064, 6684, 2022, 2579, 5667, 2006, 2151, 2504, 1012, 2004, 2005, 1996, 4366, 2008, 19124, 3287, 16371, 25469, 2003, 2019, 6882, 13316, 1011, 2459, 1010, 2008, 3475, 1005, 1056, 2995, 1012, 1045, 1005, 2310, 2464, 1054, 1011, 6758, 3152, 2007, 3287, 16371, 25469, 1012, 4379, 1010, 2027, 2069, 3749, 2070, 25085, 5328, 1010, 2021, 2073, 2024, 1996, 1054, 1011, 6758, 3152, 2007, 21226, 24728, 22144, 2015, 1998, 20916, 4691, 6845, 2401, 1029, 7880, 1010, 2138, 2027, 2123, 1005, 1056, 4839, 1012, 1996, 2168, 3632, 2005, 2216, 10231, 7685, 5830, 3065, 1024, 8040, 7317, 5063, 2015, 11820, 1999, 1996, 9478, 2021, 2025, 1037, 17962, 21239, 1999, 4356, 1012, 1998, 2216, 3653, 6528, 20771, 10271, 5691, 2066, 1996, 2829, 16291, 1010, 1999, 2029, 2057, 1005, 2128, 5845, 2000, 1996, 2609, 1997, 6320, 25624, 1005, 1055, 17061, 3779, 1010, 2021, 2025, 1037, 7637, 1997, 5061, 5710, 2006, 9318, 7367, 5737, 19393, 1012, 2077, 6933, 1006, 2030, 20242, 1007, 1000, 3313, 1011, 3115, 1000, 1999, 5609, 1997, 16371, 25469, 1010, 1996, 10597, 27885, 5809, 2063, 2323, 2202, 2046, 4070, 2028, 14477, 6767, 8524, 6321, 5793, 28141, 4489, 2090, 2273, 1998, 2308, 1024, 2045, 2024, 2053, 8991, 18400, 2015, 2006, 4653, 2043, 19910, 3544, 15287, 1010, 1998, 1996, 2168, 3685, 2022, 2056, 2005, 1037, 2158, 1012, 1999, 2755, 1010, 2017, 3227, 2180, 1005, 1056, 2156, 2931, 8991, 18400, 2015, 1999, 2019, 2137, 2143, 1999, 2505, 2460, 1997, 22555, 2030, 13216, 14253, 2050, 1012, 2023, 6884, 3313, 1011, 3115, 2003, 2625, 1037, 3313, 3115, 2084, 2019, 4914, 2135, 2139, 24128, 3754, 2000, 2272, 2000, 3408, 20547, 2007, 1996, 19008, 1997, 2308, 1005, 1055, 4230, 1012, 102]\n[101, 2065, 2069, 2000, 4468, 2437, 2023, 2828, 1997, 2143, 1999, 1996, 2925, 1012, 2023, 2143, 2003, 5875, 2004, 2019, 7551, 2021, 4136, 2053, 2522, 11461, 2466, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2514, 6819, 5339, 8918, 2005, 3564, 27046, 2009, 2138, 2009, 12817, 2006, 2061, 2116, 2590, 3314, 2021, 2009, 2515, 2061, 2302, 2151, 5860, 11795, 3085, 15793, 1012, 1996, 13972, 3310, 2185, 2007, 2053, 2047, 15251, 1006, 4983, 2028, 3310, 2039, 2007, 2028, 2096, 2028, 1005, 1055, 2568, 17677, 2015, 1010, 2004, 2009, 2097, 26597, 2079, 2076, 2023, 23100, 2143, 1007, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2488, 5247, 2028, 1005, 1055, 2051, 4582, 2041, 1037, 3332, 2012, 1037, 3392, 3652, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 102]\n[101, 2023, 2143, 2001, 2763, 4427, 2011, 2643, 4232, 1005, 1055, 16137, 10841, 4115, 1010, 10768, 25300, 2078, 1998, 1045, 9075, 2017, 2000, 2156, 2008, 2143, 2612, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2143, 2038, 2048, 2844, 3787, 1998, 2216, 2024, 1010, 1006, 1015, 1007, 1996, 12689, 3772, 1006, 1016, 1007, 1996, 8052, 1010, 6151, 6810, 2099, 7178, 2135, 2204, 1010, 6302, 1012, 4237, 2013, 2008, 1010, 2054, 9326, 2033, 2087, 2003, 1996, 10866, 5460, 1997, 9033, 21202, 7971, 1012, 14229, 6396, 2386, 2038, 2000, 2022, 2087, 15703, 3883, 1999, 1996, 2088, 1012, 2016, 4490, 2061, 5236, 1998, 2007, 2035, 1996, 16371, 25469, 1999, 2023, 2143, 1010, 1012, 1012, 1012, 2009, 1005, 1055, 14477, 4779, 26884, 1012, 13599, 2000, 2643, 4232, 1005, 1055, 2143, 1010, 7789, 3012, 2038, 2042, 2999, 2007, 28072, 1012, 2302, 2183, 2205, 2521, 2006, 2023, 3395, 1010, 1045, 2052, 2360, 2008, 4076, 2013, 1996, 4489, 1999, 15084, 2090, 1996, 2413, 1998, 1996, 4467, 2554, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1037, 3185, 1997, 2049, 2051, 1010, 1998, 2173, 1012, 1016, 1013, 2184, 1012, 102]\n[101, 2821, 1010, 2567, 1012, 1012, 1012, 2044, 4994, 2055, 2023, 9951, 2143, 2005, 8529, 13876, 12129, 2086, 2035, 1045, 2064, 2228, 1997, 2003, 2008, 2214, 14911, 3389, 2299, 1012, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1000, 2003, 2008, 2035, 2045, 2003, 1029, 1029, 1000, 1012, 1012, 1012, 1045, 2001, 2074, 2019, 2220, 9458, 2043, 2023, 20482, 3869, 2718, 1996, 1057, 1012, 1055, 1012, 1045, 2001, 2205, 2402, 2000, 2131, 1999, 1996, 4258, 1006, 2348, 1045, 2106, 6133, 2000, 13583, 2046, 1000, 9119, 8912, 1000, 1007, 1012, 2059, 1037, 11326, 2012, 1037, 2334, 2143, 2688, 10272, 17799, 1011, 2633, 1045, 2071, 2156, 2023, 2143, 1010, 3272, 2085, 1045, 2001, 2004, 2214, 2004, 2026, 3008, 2020, 2043, 2027, 8040, 7317, 13699, 5669, 2000, 2156, 2009, 999, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2069, 3114, 2023, 2143, 2001, 2025, 10033, 2000, 1996, 10812, 13457, 1997, 2051, 2001, 2138, 1997, 1996, 27885, 11020, 20693, 2553, 13977, 2011, 2049, 1057, 1012, 1055, 1012, 2713, 1012, 8817, 1997, 2111, 19311, 2098, 2000, 2023, 27136, 2121, 1010, 3241, 2027, 2020, 2183, 2000, 2156, 1037, 3348, 2143, 1012, 1012, 1012, 2612, 1010, 2027, 2288, 7167, 1997, 2485, 22264, 1997, 1043, 11802, 2135, 1010, 16360, 23004, 25430, 18352, 1010, 2006, 1011, 2395, 7636, 1999, 20857, 6023, 25943, 1010, 2004, 5498, 2063, 2576, 3653, 29048, 1012, 1012, 1012, 1998, 7408, 3468, 2040, 1011, 14977, 23599, 3348, 5019, 2007, 7842, 22772, 1010, 5122, 5889, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 3451, 12696, 1010, 4151, 24665, 12502, 1010, 3181, 20785, 1012, 1012, 3649, 2023, 2518, 2001, 1010, 14021, 5596, 2009, 1010, 6402, 2009, 1010, 2059, 4933, 1996, 11289, 1999, 1037, 2599, 3482, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 7069, 9765, 27065, 2229, 2145, 26988, 2000, 2424, 3643, 1999, 2049, 11771, 18404, 6208, 2576, 11867, 7974, 8613, 1012, 1012, 2021, 2065, 2009, 4694, 1005, 1056, 2005, 1996, 15657, 9446, 1010, 2009, 2052, 2031, 2042, 6439, 1010, 2059, 6404, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2612, 1010, 1996, 1000, 1045, 2572, 8744, 1010, 8744, 1000, 1054, 10536, 16921, 7583, 2516, 2001, 5567, 10866, 2135, 2005, 2086, 2004, 1037, 14841, 26065, 3508, 2005, 22555, 2080, 3152, 1006, 1045, 2572, 8025, 1010, 20920, 1011, 2005, 5637, 3152, 1010, 1045, 2572, 8025, 1010, 2304, 1011, 2005, 1038, 2721, 2595, 24759, 28100, 3370, 3152, 1010, 4385, 1012, 1012, 1007, 1998, 2296, 2702, 2086, 2030, 2061, 1996, 2518, 9466, 2013, 1996, 2757, 1010, 2000, 2022, 7021, 2011, 1037, 2047, 4245, 1997, 26476, 2015, 2040, 2215, 2000, 2156, 2008, 1000, 20355, 3348, 2143, 1000, 2008, 1000, 4329, 3550, 1996, 2143, 3068, 1000, 1012, 1012, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 6300, 9953, 1010, 4468, 2066, 1996, 11629, 1012, 1012, 2030, 2065, 2017, 2442, 2156, 2009, 1011, 9278, 1996, 2678, 1998, 3435, 2830, 2000, 1996, 1000, 6530, 1000, 3033, 1010, 2074, 2000, 2131, 2009, 2058, 2007, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 102]\n\n\n- 결론: imdb 에는 자료가 “텍스트” 형태로 저장되어있고, tokenized_imdb 는 자료가 “텍스트 \\(\\cup\\) 숫자” 형태로 저장되어 있음. 즉 tokenized_imdb 는 원래데이터 (raw data) 와 전처리된 데이터 (preprocessed data) 가 같이 있음."
  },
  {
    "objectID": "posts/02wk-1.html#b.-step2-인공지능-생성",
    "href": "posts/02wk-1.html#b.-step2-인공지능-생성",
    "title": "02wk-1: IMDB 영화평 감성분석",
    "section": "B. Step2 – 인공지능 생성",
    "text": "B. Step2 – 인공지능 생성\n- 인공지능을 생성하는 코드\n\nfrom transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert/distilbert-base-uncased\", num_labels=2\n)\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
  },
  {
    "objectID": "posts/02wk-1.html#c.-step3-인공지능-학습",
    "href": "posts/02wk-1.html#c.-step3-인공지능-학습",
    "title": "02wk-1: IMDB 영화평 감성분석",
    "section": "C. Step3 – 인공지능 학습",
    "text": "C. Step3 – 인공지능 학습\n- 트레이너를 만들때 필요한 재료 data_collator 생성\n\nfrom transformers import DataCollatorWithPadding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n- 트레이너를 만들때 필요한 재료 compute_metrics 생성\n\nimport evaluate\nimport numpy as np\naccuracy = evaluate.load(\"accuracy\")\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy.compute(predictions=predictions, references=labels)\n\n- 트레이너를 만들때 필요한 재료 training_args(훈련지침?) 생성\n\nfrom transformers import TrainingArguments, Trainer\ntraining_args = TrainingArguments(\n    output_dir=\"my_awesome_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2, # 전체문제세트를 2번 공부하라..\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n)\n\n- 트레이너 생성\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_imdb[\"train\"],\n    eval_dataset=tokenized_imdb[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n- 트레이너를 이용하여 인공지능을 학습시키는 코드\n\ntrainer.train()\n\n\n    \n      \n      \n      [3126/3126 12:33, Epoch 2/2]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n0.222000\n0.202918\n0.921160\n\n\n2\n0.147300\n0.226498\n0.932000\n\n\n\n\n\n\nTrainOutput(global_step=3126, training_loss=0.20518110291132619, metrics={'train_runtime': 753.5002, 'train_samples_per_second': 66.357, 'train_steps_per_second': 4.149, 'total_flos': 6556904415524352.0, 'train_loss': 0.20518110291132619, 'epoch': 2.0})"
  },
  {
    "objectID": "posts/02wk-1.html#d.-step4-예측",
    "href": "posts/02wk-1.html#d.-step4-예측",
    "title": "02wk-1: IMDB 영화평 감성분석",
    "section": "D. Step4 – 예측",
    "text": "D. Step4 – 예측\n\ntext0 = \"The movie was a major disappointment. The plot was weak, and the pacing felt disjointed. The characters lacked depth, making it hard to connect with them. Predictable twists and a cliché resolution left no sense of excitement. Visually, it was unimpressive, and the soundtrack didn’t fit the scenes, pulling me out of the experience. Overall, it failed to deliver anything memorable.\"\ntext1 = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\"\n\n\nfrom transformers import pipeline\nclassifier1 = pipeline(\"sentiment-analysis\", model=\"my_awesome_model/checkpoint-1563\")\nprint(classifier1(text0))\nprint(classifier1(text1))\n\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n\n\n[{'label': 'LABEL_0', 'score': 0.9910238981246948}]\n[{'label': 'LABEL_1', 'score': 0.993097722530365}]\n\n\n\nclassifier2 = pipeline(\"sentiment-analysis\", model=\"my_awesome_model/checkpoint-3126\")\nprint(classifier2(text0))\nprint(classifier2(text1))\n\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n\n\n[{'label': 'LABEL_0', 'score': 0.9966901540756226}]\n[{'label': 'LABEL_1', 'score': 0.9965715408325195}]"
  },
  {
    "objectID": "posts/05wk-1.html#a.-데이터불러오기",
    "href": "posts/05wk-1.html#a.-데이터불러오기",
    "title": "05wk-1: Food-101 이미지자료의 분류",
    "section": "A. 데이터불러오기",
    "text": "A. 데이터불러오기\n- 원래는 자료가 많음\n\nfood_full = datasets.load_dataset(\"food101\")\nfood_full\n# 자료가 약 10만개, 자료형은 DatasetDict 임 \n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 75750\n    })\n    validation: Dataset({\n        features: ['image', 'label'],\n        num_rows: 25250\n    })\n})\n\n\n- train에서 5000장만 가져옴\n\nfood5000 = datasets.load_dataset(\"food101\", split=\"train[:5000]\")\nfood5000\n# 자료는 5000개, 자료형은 Dataset \n\nDataset({\n    features: ['image', 'label'],\n    num_rows: 5000\n})\n\n\n- food5000에서 8:2로 데이터를 분리\n\nfood = food5000.train_test_split(test_size=0.2)\nfood\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 4000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 1000\n    })\n})"
  },
  {
    "objectID": "posts/05wk-1.html#b.-데이터-살펴보기",
    "href": "posts/05wk-1.html#b.-데이터-살펴보기",
    "title": "05wk-1: Food-101 이미지자료의 분류",
    "section": "B. 데이터 살펴보기",
    "text": "B. 데이터 살펴보기\n- 이미지를 보는 방법\n\nimg = food['train'][0]['image']\nimg\n\n\n\n\n\n\n\n\n\ntype(img) # img의 자료형\n\nPIL.JpegImagePlugin.JpegImageFile\n\n\n- 이미지에 해당하는 라벨을 같이 확인하는 방법\n\n# food['train'][0]['image'] --- # 0번이미지\nfood['train'][0]['label'] # 0번이미지에 해당하는 라벨\n\n20\n\n\n20이 의미하는바가 무엇이지?\n\nlabels = food['train'].features['label'].names # 라벨들의 정보들\nlabels[:5]\n\n['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare']\n\n\n\nlabels[20] # 20이라는 숫자가 의미하는 음식이름은 'chicken_wings' 임\n\n'chicken_wings'\n\n\n정리하면\n\nimg_num = 11\nprint(labels[food['train'][img_num]['label']])\nfood['train'][img_num]['image']\n\nprime_rib\n\n\n\n\n\n\n\n\n\n- 이미지를 확인하면서 느낀점: 크기가 다름 \\(\\to\\) 각 이미지의 크기를 조사하고 싶다면?\n\n# 방법1 \nfood['train'][3]['image'].__str__().split(' ')[-3].split(\"=\")[-1].split(\"x\")\nsizes = [l['image'].__str__().split(' ')[-3].split(\"=\")[-1].split(\"x\") for l in food['train']]\nsizes[:4]\n\n[['512', '512'], ['512', '384'], ['512', '512'], ['512', '512']]\n\n\n\n# 방법2\nsizes = [l['image'].size for l in food['train']]\nsizes[:4]\n\n[(512, 512), (512, 384), (512, 512), (512, 512)]\n\n\n\n이미지의 크기가 서로 다르네? (텍스트의 길이가 서로 다르듯이?)\n인공지능입장에서는 이렇게 이미지의 크기가 통일되어 있지 않다면 싫어하겠는걸?\n이미지를 resize 하여 크기를 통일시켜주는 코드가 어딘가 반드시 필요하겠군.."
  },
  {
    "objectID": "posts/05wk-1.html#c.-torchvision.transforms",
    "href": "posts/05wk-1.html#c.-torchvision.transforms",
    "title": "05wk-1: Food-101 이미지자료의 분류",
    "section": "C. torchvision.transforms",
    "text": "C. torchvision.transforms\n- 이미지자료 하나 받아두기\n\nimg = datasets.load_dataset(\"food101\",split=\"train[:1]\")[0]['image']\nimg\n\n\n\n\n\n\n\n\n\n# torchvision.transforms.RandomResizedCrop\n# 예시1 – 사이즈를 224,112로 조정\n\n자르고크기조정하기 = torchvision.transforms.RandomResizedCrop((224,112))\n\n\n자르고크기조정하기(img)\n\n\n\n\n\n\n\n\n#\n# 예시2 – 사이즈를 224,224로 조정\n\n# 방법1\n자르고크기조정하기 = torchvision.transforms.RandomResizedCrop((224,224))\n자르고크기조정하기(img)\n\n\n\n\n\n\n\n\n\n# 방법2\n자르고크기조정하기 = torchvision.transforms.RandomResizedCrop(224)\n자르고크기조정하기(img)\n\n\n\n\n\n\n\n\n#\n\n\n# torchvision.transforms.ToTensor()\n# 예시1\n\n텐서화하기 = torchvision.transforms.ToTensor()\n\n\n텐서화하기(img)\n\ntensor([[[0.1216, 0.1137, 0.1098,  ..., 0.0039, 0.0039, 0.0000],\n         [0.1255, 0.1216, 0.1176,  ..., 0.0039, 0.0039, 0.0000],\n         [0.1294, 0.1255, 0.1255,  ..., 0.0039, 0.0000, 0.0000],\n         ...,\n         [0.2588, 0.2745, 0.2863,  ..., 0.3765, 0.3882, 0.3922],\n         [0.2353, 0.2471, 0.2667,  ..., 0.3373, 0.3373, 0.3373],\n         [0.2235, 0.2275, 0.2471,  ..., 0.3333, 0.3176, 0.3059]],\n\n        [[0.1373, 0.1294, 0.1255,  ..., 0.1020, 0.1020, 0.0980],\n         [0.1412, 0.1373, 0.1333,  ..., 0.1020, 0.1020, 0.0980],\n         [0.1451, 0.1412, 0.1412,  ..., 0.1020, 0.0980, 0.0980],\n         ...,\n         [0.2471, 0.2627, 0.2745,  ..., 0.3647, 0.3765, 0.3882],\n         [0.2235, 0.2353, 0.2549,  ..., 0.3255, 0.3333, 0.3333],\n         [0.2118, 0.2157, 0.2353,  ..., 0.3216, 0.3137, 0.3020]],\n\n        [[0.1412, 0.1333, 0.1294,  ..., 0.0902, 0.0902, 0.0863],\n         [0.1451, 0.1412, 0.1451,  ..., 0.0902, 0.0902, 0.0863],\n         [0.1490, 0.1451, 0.1529,  ..., 0.0902, 0.0863, 0.0863],\n         ...,\n         [0.1725, 0.1882, 0.2000,  ..., 0.2431, 0.2549, 0.2667],\n         [0.1490, 0.1608, 0.1804,  ..., 0.2039, 0.2118, 0.2118],\n         [0.1373, 0.1412, 0.1608,  ..., 0.2000, 0.1922, 0.1804]]])\n\n\n\n텐서화하기(img).shape\n\ntorch.Size([3, 512, 384])\n\n\n#\n# 예시2 – 자르고크기조정하기 와 텐서화하기를 동시에 사용하는 경우\n\n텐서화하기(자르고크기조정하기(img)).shape\n\ntorch.Size([3, 224, 224])\n\n\n\n자르고크기조정하기(텐서화하기(img)).shape\n\ntorch.Size([3, 224, 224])\n\n\n#\n\n\n# torchvision.transforms.Normalize\n# 예시1\n\n표준화하기 = torchvision.transforms.Normalize(mean=[10,20,30],std=[0.5,1.0,1.5])\n\n\n표준화하기는 각 채널별로, mean을 뺸 뒤 std를 나눈 계산값을 리턴한다.\n\n\n표준화하기(텐서화하기(img)) # 숫자들이 계산됨\n\ntensor([[[-19.7569, -19.7725, -19.7804,  ..., -19.9922, -19.9922, -20.0000],\n         [-19.7490, -19.7569, -19.7647,  ..., -19.9922, -19.9922, -20.0000],\n         [-19.7412, -19.7490, -19.7490,  ..., -19.9922, -20.0000, -20.0000],\n         ...,\n         [-19.4824, -19.4510, -19.4275,  ..., -19.2471, -19.2235, -19.2157],\n         [-19.5294, -19.5059, -19.4667,  ..., -19.3255, -19.3255, -19.3255],\n         [-19.5529, -19.5451, -19.5059,  ..., -19.3333, -19.3647, -19.3882]],\n\n        [[-19.8627, -19.8706, -19.8745,  ..., -19.8980, -19.8980, -19.9020],\n         [-19.8588, -19.8627, -19.8667,  ..., -19.8980, -19.8980, -19.9020],\n         [-19.8549, -19.8588, -19.8588,  ..., -19.8980, -19.9020, -19.9020],\n         ...,\n         [-19.7529, -19.7373, -19.7255,  ..., -19.6353, -19.6235, -19.6118],\n         [-19.7765, -19.7647, -19.7451,  ..., -19.6745, -19.6667, -19.6667],\n         [-19.7882, -19.7843, -19.7647,  ..., -19.6784, -19.6863, -19.6980]],\n\n        [[-19.9059, -19.9111, -19.9137,  ..., -19.9399, -19.9399, -19.9425],\n         [-19.9033, -19.9059, -19.9033,  ..., -19.9399, -19.9399, -19.9425],\n         [-19.9007, -19.9033, -19.8980,  ..., -19.9399, -19.9425, -19.9425],\n         ...,\n         [-19.8850, -19.8745, -19.8667,  ..., -19.8379, -19.8301, -19.8222],\n         [-19.9007, -19.8928, -19.8797,  ..., -19.8641, -19.8588, -19.8588],\n         [-19.9085, -19.9059, -19.8928,  ..., -19.8667, -19.8719, -19.8797]]])\n\n\n숫자들이 어떻게 계산되었는가?\n\nprint(\"첫번째 채널(R)\")\n(텐서화하기(img)[0] - 10)/0.5, 표준화하기(텐서화하기(img))[0]\n\n첫번째 채널(R)\n\n\n(tensor([[-19.7569, -19.7725, -19.7804,  ..., -19.9922, -19.9922, -20.0000],\n         [-19.7490, -19.7569, -19.7647,  ..., -19.9922, -19.9922, -20.0000],\n         [-19.7412, -19.7490, -19.7490,  ..., -19.9922, -20.0000, -20.0000],\n         ...,\n         [-19.4824, -19.4510, -19.4275,  ..., -19.2471, -19.2235, -19.2157],\n         [-19.5294, -19.5059, -19.4667,  ..., -19.3255, -19.3255, -19.3255],\n         [-19.5529, -19.5451, -19.5059,  ..., -19.3333, -19.3647, -19.3882]]),\n tensor([[-19.7569, -19.7725, -19.7804,  ..., -19.9922, -19.9922, -20.0000],\n         [-19.7490, -19.7569, -19.7647,  ..., -19.9922, -19.9922, -20.0000],\n         [-19.7412, -19.7490, -19.7490,  ..., -19.9922, -20.0000, -20.0000],\n         ...,\n         [-19.4824, -19.4510, -19.4275,  ..., -19.2471, -19.2235, -19.2157],\n         [-19.5294, -19.5059, -19.4667,  ..., -19.3255, -19.3255, -19.3255],\n         [-19.5529, -19.5451, -19.5059,  ..., -19.3333, -19.3647, -19.3882]]))\n\n\n\nprint(\"두번째 채널(G)\")\n(텐서화하기(img)[1] - 20)/1.0, 표준화하기(텐서화하기(img))[1]\n\n두번째 채널(G)\n\n\n(tensor([[-19.8627, -19.8706, -19.8745,  ..., -19.8980, -19.8980, -19.9020],\n         [-19.8588, -19.8627, -19.8667,  ..., -19.8980, -19.8980, -19.9020],\n         [-19.8549, -19.8588, -19.8588,  ..., -19.8980, -19.9020, -19.9020],\n         ...,\n         [-19.7529, -19.7373, -19.7255,  ..., -19.6353, -19.6235, -19.6118],\n         [-19.7765, -19.7647, -19.7451,  ..., -19.6745, -19.6667, -19.6667],\n         [-19.7882, -19.7843, -19.7647,  ..., -19.6784, -19.6863, -19.6980]]),\n tensor([[-19.8627, -19.8706, -19.8745,  ..., -19.8980, -19.8980, -19.9020],\n         [-19.8588, -19.8627, -19.8667,  ..., -19.8980, -19.8980, -19.9020],\n         [-19.8549, -19.8588, -19.8588,  ..., -19.8980, -19.9020, -19.9020],\n         ...,\n         [-19.7529, -19.7373, -19.7255,  ..., -19.6353, -19.6235, -19.6118],\n         [-19.7765, -19.7647, -19.7451,  ..., -19.6745, -19.6667, -19.6667],\n         [-19.7882, -19.7843, -19.7647,  ..., -19.6784, -19.6863, -19.6980]]))\n\n\n\nprint(\"세번째 채널(B)\")\n(텐서화하기(img)[2] - 30)/1.5, 표준화하기(텐서화하기(img))[2]\n\n세번째 채널(B)\n\n\n(tensor([[-19.9059, -19.9111, -19.9137,  ..., -19.9399, -19.9399, -19.9425],\n         [-19.9033, -19.9059, -19.9033,  ..., -19.9399, -19.9399, -19.9425],\n         [-19.9007, -19.9033, -19.8980,  ..., -19.9399, -19.9425, -19.9425],\n         ...,\n         [-19.8850, -19.8745, -19.8667,  ..., -19.8379, -19.8301, -19.8222],\n         [-19.9007, -19.8928, -19.8797,  ..., -19.8641, -19.8588, -19.8588],\n         [-19.9085, -19.9059, -19.8928,  ..., -19.8667, -19.8719, -19.8797]]),\n tensor([[-19.9059, -19.9111, -19.9137,  ..., -19.9399, -19.9399, -19.9425],\n         [-19.9033, -19.9059, -19.9033,  ..., -19.9399, -19.9399, -19.9425],\n         [-19.9007, -19.9033, -19.8980,  ..., -19.9399, -19.9425, -19.9425],\n         ...,\n         [-19.8850, -19.8745, -19.8667,  ..., -19.8379, -19.8301, -19.8222],\n         [-19.9007, -19.8928, -19.8797,  ..., -19.8641, -19.8588, -19.8588],\n         [-19.9085, -19.9059, -19.8928,  ..., -19.8667, -19.8719, -19.8797]]))\n\n\n#\n\n\n# torchvision.transforms.Compose\n# 예시1 – 여러함수를 묶어 하나의 함수를 만드는 방법\n\n이미지처리하기 = torchvision.transforms.Compose([자르고크기조정하기, 텐서화하기, 표준화하기])\n\n\n이미지처리하기(img)\n\ntensor([[[-19.3882, -19.4039, -19.4275,  ..., -19.4431, -19.4667, -19.4510],\n         [-19.4275, -19.5059, -19.4902,  ..., -19.4431, -19.4588, -19.4667],\n         [-19.2941, -19.4431, -19.3647,  ..., -19.4510, -19.4667, -19.4588],\n         ...,\n         [-18.9333, -18.9882, -18.9490,  ..., -19.2078, -19.1529, -19.1294],\n         [-19.0196, -18.9961, -18.9725,  ..., -19.2392, -19.2314, -19.2627],\n         [-19.0039, -18.9961, -18.9882,  ..., -19.2078, -19.2235, -19.3098]],\n\n        [[-19.6118, -19.6196, -19.6392,  ..., -19.7843, -19.7961, -19.7882],\n         [-19.6353, -19.6784, -19.6745,  ..., -19.7804, -19.7882, -19.7922],\n         [-19.5647, -19.6471, -19.6118,  ..., -19.7843, -19.7922, -19.7882],\n         ...,\n         [-19.4980, -19.5255, -19.5098,  ..., -19.6627, -19.6353, -19.6235],\n         [-19.5373, -19.5255, -19.5176,  ..., -19.6784, -19.6745, -19.6902],\n         [-19.5294, -19.5255, -19.5216,  ..., -19.6627, -19.6706, -19.7137]],\n\n        [[-19.6863, -19.6941, -19.7072,  ..., -19.8954, -19.9033, -19.9007],\n         [-19.7072, -19.7359, -19.7386,  ..., -19.9033, -19.9085, -19.9111],\n         [-19.6575, -19.7124, -19.6941,  ..., -19.9059, -19.9111, -19.9085],\n         ...,\n         [-19.7124, -19.7281, -19.7176,  ..., -19.8327, -19.8118, -19.7987],\n         [-19.7386, -19.7307, -19.7229,  ..., -19.8431, -19.8379, -19.8431],\n         [-19.7333, -19.7307, -19.7255,  ..., -19.8327, -19.8353, -19.8614]]])\n\n\n#"
  },
  {
    "objectID": "posts/05wk-1.html#d.-이미지전처리",
    "href": "posts/05wk-1.html#d.-이미지전처리",
    "title": "05wk-1: Food-101 이미지자료의 분류",
    "section": "D. 이미지전처리",
    "text": "D. 이미지전처리\n- 아래의 샘플코드를 살펴보자.\nimage_processor = transformers.AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\nnormalize = torchvision.transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\nsize = (\n    image_processor.size[\"shortest_edge\"]\n    if \"shortest_edge\" in image_processor.size\n    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n)\n_transforms = torchvision.transforms.Compose([\n    torchvision.transforms.RandomResizedCrop(size), \n    torchvision.transforms.ToTensor(), \n    normalize\n])\ndef transforms(examples):\n    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n    del examples[\"image\"]\n    return examples\nfood = food.with_transform(transforms)\n# 예비학습 1\n\na = +2\n(\n    (a,\"음수\") if a&lt;0 else (a,\"양수\")\n)\n\n(2, '양수')\n\n\n\ndct = {'shortest_edge':16, 'height': 224, 'width': 224}\ndct = {'height': 224, 'width': 224}\n(\n    dct['shortest_edge'] \n    if 'shortest_edge' in dct \n    else (dct['height'],dct['width'])\n)\n\n(224, 224)\n\n\n#\n# 예비학습 2\n\nimg.convert(\"L\")\n\n\n\n\n\n\n\n\n#\n- image_processor 살펴보기\n\nimage_processor = transformers.AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\nimage_processor\n\nFast image processor class &lt;class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'&gt; is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n\n\nViTImageProcessor {\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"image_processor_type\": \"ViTImageProcessor\",\n  \"image_std\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"resample\": 2,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"height\": 224,\n    \"width\": 224\n  }\n}\n\n\n\nimage_processor.image_mean\n\n[0.5, 0.5, 0.5]\n\n\n\nimage_processor.image_std\n\n[0.5, 0.5, 0.5]\n\n\n\nimage_processor.size['height'],image_processor.size['width']\n\n(224, 224)\n\n\n- 코드는 아래와 같이 단순화하여 이해할 수 있음.\n\n_transforms = torchvision.transforms.Compose([\n    torchvision.transforms.RandomResizedCrop((224,224)), \n    torchvision.transforms.ToTensor(), \n    torchvision.transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n])\ndef transforms(examples):\n    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n    del examples[\"image\"]\n    return examples\nfood_transformed = food.with_transform(transforms)\n\n- 주의점: food.with_transform(transforms)는 지연처리되는 코드임\nfood_transformed['train'][0:5] 이것이 나오는 원리?\n\nexamples = food['train'][0:5]\ntransforms(examples)\n\n{'label': [20, 6, 6, 81, 79],\n 'pixel_values': [tensor([[[-0.4196, -0.4588, -0.4667,  ...,  0.9843,  0.9922,  0.9765],\n           [-0.4353, -0.4745, -0.4588,  ...,  0.9922,  0.9922,  1.0000],\n           [-0.4275, -0.4745, -0.4275,  ...,  0.9765,  0.9922,  1.0000],\n           ...,\n           [-0.3961, -0.4510, -0.5216,  ..., -0.6471, -0.7098, -0.7333],\n           [-0.3725, -0.4275, -0.4980,  ..., -0.6863, -0.7412, -0.7647],\n           [-0.3569, -0.4039, -0.4902,  ..., -0.7333, -0.7647, -0.7804]],\n  \n          [[-0.4510, -0.4980, -0.5059,  ...,  0.9922,  1.0000,  0.9922],\n           [-0.4667, -0.5137, -0.4980,  ...,  0.9843,  0.9922,  1.0000],\n           [-0.4588, -0.5137, -0.4667,  ...,  0.9765,  0.9922,  0.9922],\n           ...,\n           [-0.5216, -0.5686, -0.6314,  ..., -0.8588, -0.8588, -0.8588],\n           [-0.5216, -0.5529, -0.6157,  ..., -0.8431, -0.8510, -0.8588],\n           [-0.5294, -0.5608, -0.6157,  ..., -0.8510, -0.8588, -0.8510]],\n  \n          [[-0.6078, -0.6471, -0.6549,  ...,  0.9608,  0.9922,  1.0000],\n           [-0.6235, -0.6706, -0.6549,  ...,  0.9451,  0.9686,  0.9922],\n           [-0.6314, -0.6863, -0.6235,  ...,  0.9137,  0.9608,  0.9765],\n           ...,\n           [-0.7882, -0.8039, -0.8275,  ..., -0.9922, -0.9922, -0.9922],\n           [-0.7804, -0.7804, -0.8039,  ..., -0.9686, -0.9843, -0.9843],\n           [-0.7882, -0.7882, -0.8196,  ..., -0.9765, -0.9843, -0.9843]]]),\n  tensor([[[-0.5059, -0.6078, -0.7412,  ...,  0.3569,  0.3647,  0.3255],\n           [-0.4118, -0.4118, -0.4039,  ...,  0.3255,  0.3098,  0.2235],\n           [-0.3882, -0.3882, -0.3647,  ...,  0.3412,  0.2471,  0.1137],\n           ...,\n           [-0.8824, -0.8745, -0.8745,  ..., -0.0353, -0.0353, -0.0353],\n           [-0.8745, -0.8745, -0.8667,  ..., -0.0588, -0.0510, -0.0431],\n           [-0.8902, -0.8745, -0.8588,  ..., -0.0745, -0.0745, -0.0588]],\n  \n          [[-0.4353, -0.5451, -0.6784,  ...,  0.4353,  0.4431,  0.4118],\n           [-0.2941, -0.2941, -0.3098,  ...,  0.4039,  0.3961,  0.3098],\n           [-0.2471, -0.2471, -0.2392,  ...,  0.4039,  0.3098,  0.1843],\n           ...,\n           [-0.8039, -0.7961, -0.7961,  ..., -0.0980, -0.0902, -0.0902],\n           [-0.7882, -0.7961, -0.7882,  ..., -0.1294, -0.1137, -0.1059],\n           [-0.8039, -0.7961, -0.7804,  ..., -0.1451, -0.1373, -0.1216]],\n  \n          [[-0.3176, -0.4431, -0.5843,  ...,  0.4510,  0.4588,  0.4275],\n           [-0.0980, -0.1216, -0.1451,  ...,  0.4196,  0.4118,  0.3255],\n           [ 0.0118,  0.0039,  0.0118,  ...,  0.4353,  0.3333,  0.2000],\n           ...,\n           [-0.7255, -0.7176, -0.7255,  ..., -0.1294, -0.1216, -0.1137],\n           [-0.7020, -0.7176, -0.7098,  ..., -0.1451, -0.1294, -0.1216],\n           [-0.7098, -0.7020, -0.6941,  ..., -0.1608, -0.1529, -0.1373]]]),\n  tensor([[[-0.6784, -0.6706, -0.6549,  ..., -0.5451, -0.5373, -0.5294],\n           [-0.6784, -0.6784, -0.6627,  ..., -0.5451, -0.5373, -0.5294],\n           [-0.6863, -0.6941, -0.4824,  ..., -0.5451, -0.5451, -0.5451],\n           ...,\n           [-0.4118, -0.3020, -0.2863,  ..., -0.5137, -0.4980, -0.5059],\n           [-0.7569, -0.5608, -0.3333,  ..., -0.5216, -0.5059, -0.5059],\n           [-0.8902, -0.8275, -0.6157,  ..., -0.5765, -0.5608, -0.5529]],\n  \n          [[-0.8275, -0.8275, -0.8039,  ..., -0.8039, -0.7961, -0.7961],\n           [-0.8196, -0.8196, -0.8039,  ..., -0.8118, -0.8039, -0.7961],\n           [-0.8196, -0.8118, -0.6000,  ..., -0.8118, -0.8275, -0.8196],\n           ...,\n           [-0.4353, -0.3176, -0.2941,  ..., -0.7569, -0.7490, -0.7569],\n           [-0.7569, -0.5765, -0.3725,  ..., -0.7725, -0.7569, -0.7647],\n           [-0.8667, -0.8431, -0.6706,  ..., -0.8353, -0.8118, -0.8118]],\n  \n          [[-0.8824, -0.8902, -0.8824,  ..., -0.9059, -0.8980, -0.8980],\n           [-0.8824, -0.8824, -0.8824,  ..., -0.9216, -0.9137, -0.9059],\n           [-0.8824, -0.8824, -0.6863,  ..., -0.9137, -0.9216, -0.9216],\n           ...,\n           [-0.5529, -0.4667, -0.4745,  ..., -0.8353, -0.8353, -0.8431],\n           [-0.8510, -0.6863, -0.5059,  ..., -0.8431, -0.8431, -0.8510],\n           [-0.9451, -0.9373, -0.7725,  ..., -0.9059, -0.8980, -0.8980]]]),\n  tensor([[[-0.6235, -0.6235, -0.5765,  ...,  0.4824,  0.4902,  0.4902],\n           [-0.5843, -0.5843, -0.5529,  ..., -0.3725, -0.3255, -0.2706],\n           [-0.5843, -0.5686, -0.5451,  ..., -0.5843, -0.5529, -0.4980],\n           ...,\n           [-0.8275, -0.8510, -0.8510,  ..., -0.8824, -0.8588, -0.8667],\n           [-0.8275, -0.8588, -0.8667,  ..., -0.8824, -0.8667, -0.8510],\n           [-0.8118, -0.8431, -0.8588,  ..., -0.9059, -0.8902, -0.8667]],\n  \n          [[-0.4745, -0.4980, -0.5059,  ...,  0.2941,  0.2941,  0.3020],\n           [-0.5373, -0.5529, -0.5451,  ..., -0.4353, -0.4196, -0.3882],\n           [-0.5686, -0.5686, -0.5608,  ..., -0.6078, -0.6157, -0.5922],\n           ...,\n           [-0.8353, -0.8510, -0.8510,  ..., -0.9059, -0.8667, -0.8824],\n           [-0.8510, -0.8745, -0.8745,  ..., -0.8980, -0.8745, -0.8588],\n           [-0.8510, -0.8588, -0.8667,  ..., -0.9137, -0.8980, -0.8745]],\n  \n          [[-0.4902, -0.4510, -0.3725,  ...,  0.1765,  0.1922,  0.2157],\n           [-0.4902, -0.4667, -0.4196,  ..., -0.5137, -0.4824, -0.4431],\n           [-0.4902, -0.4745, -0.4353,  ..., -0.5922, -0.5922, -0.5608],\n           ...,\n           [-0.8275, -0.8353, -0.8196,  ..., -0.8510, -0.8196, -0.8353],\n           [-0.8353, -0.8510, -0.8275,  ..., -0.8431, -0.8118, -0.8039],\n           [-0.8353, -0.8196, -0.8118,  ..., -0.8588, -0.8353, -0.8118]]]),\n  tensor([[[-0.1843, -0.1529, -0.1216,  ..., -0.0745, -0.0745,  0.0353],\n           [-0.2549, -0.2078, -0.1529,  ..., -0.0588, -0.1059, -0.0588],\n           [-0.2471, -0.2157, -0.1843,  ..., -0.1373, -0.1608, -0.1529],\n           ...,\n           [ 0.2000,  0.1373,  0.0510,  ..., -0.4275, -0.4667, -0.5137],\n           [ 0.2000,  0.1216,  0.0745,  ..., -0.3804, -0.4275, -0.4980],\n           [ 0.1922,  0.1294,  0.1137,  ..., -0.3176, -0.3490, -0.4118]],\n  \n          [[-0.7020, -0.6706, -0.6392,  ..., -0.4745, -0.4667, -0.3490],\n           [-0.7725, -0.7255, -0.6706,  ..., -0.4667, -0.5059, -0.4667],\n           [-0.7569, -0.7333, -0.7020,  ..., -0.5529, -0.5765, -0.5843],\n           ...,\n           [-0.4510, -0.5137, -0.6000,  ..., -0.4510, -0.4824, -0.5294],\n           [-0.4667, -0.5451, -0.5922,  ..., -0.3961, -0.4353, -0.5059],\n           [-0.4745, -0.5373, -0.5529,  ..., -0.3333, -0.3569, -0.4118]],\n  \n          [[-0.8745, -0.8510, -0.8275,  ..., -0.6471, -0.6784, -0.6078],\n           [-0.9294, -0.8902, -0.8431,  ..., -0.6078, -0.6706, -0.6627],\n           [-0.9137, -0.8824, -0.8667,  ..., -0.6627, -0.7020, -0.7176],\n           ...,\n           [-0.6157, -0.6784, -0.7647,  ..., -0.6471, -0.6784, -0.7255],\n           [-0.6235, -0.7020, -0.7490,  ..., -0.5843, -0.6314, -0.6863],\n           [-0.6314, -0.6941, -0.7098,  ..., -0.5137, -0.5451, -0.6000]]])]}"
  },
  {
    "objectID": "posts/05wk-1.html#e.-모델생성",
    "href": "posts/05wk-1.html#e.-모델생성",
    "title": "05wk-1: Food-101 이미지자료의 분류",
    "section": "E. 모델생성",
    "text": "E. 모델생성\n- 아래의 코드를 관찰하자.\n\nlabels = food[\"train\"].features[\"label\"].names\nlabel2id, id2label = dict(), dict()\nfor i, label in enumerate(labels):\n    label2id[label] = str(i)\n    id2label[str(i)] = label\n\n- 아래와같이 해도 별로 상관없음\n\nlabels = food[\"train\"].features[\"label\"].names\nlabel2id, id2label = dict(), dict()\nfor i, label in enumerate(labels):\n    label2id[label] = i\n    id2label[i] = label\n\n\nmodel = transformers.AutoModelForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224-in21k\",\n    num_labels=len(labels),\n    id2label=id2label,\n    label2id=label2id,\n)\n\nSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n- model을 이용하여 숫자계산하기\n\nmodel(이미지처리하기(img).reshape(1,3,224,224))\n\nImageClassifierOutput(loss=None, logits=tensor([[ 0.0150,  0.0218,  0.0188,  0.1206, -0.0085,  0.0643, -0.1483,  0.0163,\n          0.0363,  0.0275,  0.0277, -0.0682,  0.0460,  0.0335, -0.0465, -0.0452,\n         -0.0227,  0.0245,  0.0860, -0.0384, -0.1852,  0.0741, -0.0597, -0.1803,\n          0.0413, -0.0195, -0.1533, -0.0174,  0.1193, -0.1062,  0.0721, -0.0009,\n          0.0555, -0.0548,  0.0182, -0.1338,  0.1468, -0.1734, -0.0658, -0.0362,\n          0.0313,  0.0357, -0.0278,  0.2275, -0.2479, -0.0601,  0.1431,  0.0543,\n         -0.0421,  0.1420,  0.0549, -0.1055,  0.0004, -0.2870, -0.0185, -0.1590,\n          0.0572, -0.1353,  0.0354, -0.0219, -0.1516,  0.0969, -0.0475, -0.1148,\n         -0.0229,  0.0242, -0.1129,  0.0818, -0.1748,  0.0409,  0.0777,  0.1075,\n         -0.0815, -0.0970,  0.0013,  0.0481, -0.0862,  0.0860,  0.1823, -0.0444,\n          0.1407, -0.0637,  0.0325,  0.1610, -0.0647,  0.3267, -0.1497,  0.0136,\n          0.1734,  0.0924, -0.0197,  0.0821, -0.1205, -0.0800, -0.2173,  0.0818,\n          0.0278,  0.1050,  0.1194,  0.0780,  0.0056]],\n       grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n\nmodel(표준화하기(자르고크기조정하기(텐서화하기(img))).reshape(1,3,224,224))\n\nImageClassifierOutput(loss=None, logits=tensor([[ 0.0141,  0.0215,  0.0185,  0.1205, -0.0082,  0.0649, -0.1479,  0.0158,\n          0.0357,  0.0280,  0.0283, -0.0683,  0.0469,  0.0340, -0.0460, -0.0457,\n         -0.0223,  0.0247,  0.0853, -0.0393, -0.1841,  0.0734, -0.0598, -0.1791,\n          0.0415, -0.0185, -0.1534, -0.0172,  0.1190, -0.1056,  0.0723, -0.0006,\n          0.0556, -0.0545,  0.0179, -0.1338,  0.1474, -0.1742, -0.0656, -0.0361,\n          0.0309,  0.0358, -0.0287,  0.2275, -0.2478, -0.0603,  0.1433,  0.0545,\n         -0.0422,  0.1422,  0.0544, -0.1059, -0.0004, -0.2870, -0.0188, -0.1595,\n          0.0575, -0.1350,  0.0361, -0.0218, -0.1517,  0.0972, -0.0482, -0.1155,\n         -0.0231,  0.0236, -0.1137,  0.0818, -0.1747,  0.0409,  0.0769,  0.1072,\n         -0.0808, -0.0964,  0.0006,  0.0487, -0.0866,  0.0859,  0.1831, -0.0451,\n          0.1412, -0.0634,  0.0333,  0.1617, -0.0648,  0.3276, -0.1505,  0.0140,\n          0.1732,  0.0923, -0.0194,  0.0820, -0.1203, -0.0808, -0.2169,  0.0811,\n          0.0283,  0.1054,  0.1201,  0.0787,  0.0057]],\n       grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)"
  },
  {
    "objectID": "posts/05wk-1.html#f.-데이터콜렉터",
    "href": "posts/05wk-1.html#f.-데이터콜렉터",
    "title": "05wk-1: Food-101 이미지자료의 분류",
    "section": "F. 데이터콜렉터",
    "text": "F. 데이터콜렉터\n- 데이터 콜렉터 생성\n\ndata_collator = transformers.DefaultDataCollator()\n\n- 데이터콜렉터에게 이번에 크게 기대하는건 없음.. 그냥 미니배치 정도만 묶어주면 됨\n\n# food_transformed['train'][:2]['pixel_values']\n# 이 자료는 [(3,224,224)-tensor, (3,224,224)-tensor] 와 같은 구조임\n# 그런데 이 자료를 그대로 model에 넣는다면 돌아가지 않음. \n# 우리는 이 자료를 (2,3,224,224)-tensor 로 바꾸어 모델에 입력해주어야함\n\n\nimport torch\nmodel(torch.stack(food_transformed['train'][:2]['pixel_values'],axis=0))\n\n\ntorch.stack(food_transformed['train'][:2]['pixel_values'],axis=0) # 이코드 어렵죠\n\ntensor([[[[-0.5608, -0.5843, -0.6000,  ..., -0.8196, -0.8196, -0.8196],\n          [-0.5765, -0.5922, -0.6078,  ..., -0.8196, -0.8196, -0.8118],\n          [-0.5922, -0.6078, -0.6235,  ..., -0.8275, -0.8275, -0.8196],\n          ...,\n          [-0.4118, -0.4745, -0.5294,  ...,  0.0431,  0.0431,  0.0588],\n          [-0.4039, -0.4667, -0.5216,  ...,  0.0353,  0.0196,  0.0275],\n          [-0.4039, -0.4667, -0.5137,  ...,  0.0353,  0.0118,  0.0039]],\n\n         [[-0.8275, -0.8510, -0.8667,  ..., -0.8353, -0.8353, -0.8353],\n          [-0.8118, -0.8275, -0.8353,  ..., -0.8431, -0.8431, -0.8431],\n          [-0.8118, -0.8196, -0.8275,  ..., -0.8588, -0.8588, -0.8510],\n          ...,\n          [-0.6392, -0.6706, -0.7098,  ..., -0.4275, -0.4431, -0.4431],\n          [-0.6392, -0.6706, -0.7020,  ..., -0.4196, -0.4510, -0.4667],\n          [-0.6392, -0.6706, -0.7020,  ..., -0.4118, -0.4588, -0.4745]],\n\n         [[-0.9216, -0.9451, -0.9608,  ..., -0.9373, -0.9451, -0.9529],\n          [-0.9529, -0.9608, -0.9686,  ..., -0.9451, -0.9451, -0.9451],\n          [-0.9765, -0.9765, -0.9843,  ..., -0.9529, -0.9529, -0.9451],\n          ...,\n          [-0.8588, -0.8667, -0.8980,  ..., -0.9216, -0.9608, -0.9843],\n          [-0.8588, -0.8667, -0.8902,  ..., -0.8980, -0.9529, -0.9765],\n          [-0.8588, -0.8667, -0.8824,  ..., -0.8824, -0.9373, -0.9686]]],\n\n\n        [[[-0.8039, -0.8588, -0.9216,  ...,  0.0353,  0.0588,  0.0196],\n          [-0.5843, -0.6784, -0.7725,  ...,  0.0510,  0.0510,  0.0039],\n          [-0.5137, -0.5373, -0.5686,  ...,  0.0667,  0.0588,  0.0353],\n          ...,\n          [ 0.1765,  0.1843,  0.2314,  ..., -0.3098, -0.2941, -0.2784],\n          [ 0.1608,  0.1608,  0.2235,  ..., -0.3255, -0.3020, -0.2706],\n          [ 0.1059,  0.0902,  0.1922,  ..., -0.3333, -0.3020, -0.2706]],\n\n         [[-0.8588, -0.9059, -0.9608,  ...,  0.0902,  0.1059,  0.0667],\n          [-0.6784, -0.7569, -0.8431,  ...,  0.0980,  0.0980,  0.0431],\n          [-0.6235, -0.6392, -0.6706,  ...,  0.1059,  0.0902,  0.0667],\n          ...,\n          [ 0.2000,  0.2078,  0.2549,  ..., -0.4353, -0.4275, -0.4118],\n          [ 0.1843,  0.1843,  0.2471,  ..., -0.4510, -0.4353, -0.4039],\n          [ 0.1294,  0.1137,  0.2157,  ..., -0.4588, -0.4353, -0.4039]],\n\n         [[-0.8745, -0.9294, -0.9765,  ...,  0.2078,  0.2157,  0.1686],\n          [-0.6863, -0.7647, -0.8510,  ...,  0.2000,  0.1922,  0.1373],\n          [-0.6235, -0.6392, -0.6706,  ...,  0.2000,  0.1765,  0.1529],\n          ...,\n          [ 0.2549,  0.2784,  0.3255,  ..., -0.5294, -0.4980, -0.4667],\n          [ 0.2392,  0.2549,  0.3098,  ..., -0.5451, -0.5059, -0.4588],\n          [ 0.1765,  0.1843,  0.2863,  ..., -0.5529, -0.5059, -0.4588]]]])\n\n\n\ndata_collator([food['train'][0],food['train'][1]])['pixel_values'].shape # 이렇게 씀\n\ntorch.Size([2, 3, 224, 224])"
  },
  {
    "objectID": "posts/05wk-1.html#g.-추론",
    "href": "posts/05wk-1.html#g.-추론",
    "title": "05wk-1: Food-101 이미지자료의 분류",
    "section": "G. 추론",
    "text": "G. 추론\n- 코드정리1에 의하여 이미 학습되어있는 Trainer\n\n# Step4 \nclassifier = transformers.pipeline(\"image-classification\", model=\"my_awesome_food_model/checkpoint-186\")\n\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n\n\n- image url 에서 PIL 오브젝트 만들기: GPT 답변\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n# 이미지 URL\nurl = \"https://example.com/image.jpg\"\n\n# URL에서 이미지 불러오기\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content))\n\n# 이미지 확인\nimage.show()\n\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n# 이미지 URL\nurl = \"https://i.namu.wiki/i/eYVvNJnEv7iWMzCOChZ0BeQm1HVG2sqOWiEABGD3Q5yUilrg8rtVrTc0MupDB9DtGBzioQ5JLfPSPR_AuajlNWsOJsm6gp-EnrcKzcHjHGvzoldry6NiVpZHrNTF2WM9bY4Y546kCtsE1n1yTv4TUg.webp\"\n\n# URL에서 이미지 불러오기\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content))\nimage\n\n\n\n\n\n\n\n\n\nclassifier(image)\n\n[{'label': 'ramen', 'score': 0.9660526514053345},\n {'label': 'hamburger', 'score': 0.648557722568512},\n {'label': 'beignets', 'score': 0.6234114170074463},\n {'label': 'prime_rib', 'score': 0.5963582992553711},\n {'label': 'bruschetta', 'score': 0.5791028141975403}]"
  },
  {
    "objectID": "quiz/Quiz-2.html",
    "href": "quiz/Quiz-2.html",
    "title": "Quiz-2 (2024.09.24) // 범위: 02wk-1 까지",
    "section": "",
    "text": "항목\n허용 여부\n비고\n\n\n\n\n강의노트 참고\n허용\n수업 중 제공된 강의노트나 본인이 정리한 자료를 참고 가능\n\n\n구글 검색\n허용\n인터넷을 통한 자료 검색 및 정보 확인 가능\n\n\n생성 모형 사용\n허용 안함\n인공지능 기반 도구(GPT 등) 사용 불가\n\n\n\n\n\n# pip install datasets evaluate accelerate\n\n\n1. emotion 자료 탐색 – 10점\n\n(1)-(2) 모두 부분점수 없음.\n\n아래는 Hugging Face의 emotion 데이터셋을 로드하는 코드이다:\n\nfrom datasets import load_dataset\nemotion = load_dataset('emotion')\n\n/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nemotion['train'][-2]는 훈련 데이터의 두 번째 마지막 항목을 출력한다. 출력된 샘플은 다음과 같다.\n\nemotion['train'][-2]\n\n{'text': 'i feel like this was such a rude comment and im glad that t',\n 'label': 3}\n\n\n출력된 샘플은 딕셔너리 형식으로, text에는 문장 “i feel like this was such a rude comment and im glad that t”이 담겨 있다. 이 문장의 감정은 label 항목에 저장되어 있으며, 값은 3으로 나타난다. label 값은 해당 텍스트가 표현하는 감정을 숫자로 표현한 것이다.\n감정 레이블은 총 6가지로 나뉘며, 각각의 감정은 다음과 같이 정의된다:\n\nemotion['train'].features['label'].names\n\n['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n\n\n\n{\n  0: 'sadness',\n  1: 'joy',\n  2: 'love',\n  3: 'anger',\n  4: 'fear',\n  5: 'surprise'\n}\n\n{0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n\n\n따라서, 문장 “i feel like this was such a rude comment and im glad that t” 의 감정은 label이 3이므로, “anger”에 해당한다.\n(1) emotion 데이터셋의 각 분할(train, validation, test)에서 감정별로 몇 개의 데이터가 있는지를 조사하라. 즉 아래의 표에서 빈칸에 해당하는 숫자를 계산할 수 있는 코드를 제시하라. – 5점\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\n0:Sadness\n1:Joy\n2:Love\n3:Anger\n4:Fear\n5:Surprise\nTotal\n\n\n\n\nTrain\n??\n??\n??\n??\n??\n??\n16000\n\n\nValidation\n??\n??\n??\n??\n??\n??\n2000\n\n\nTest\n??\n??\n??\n??\n??\n??\n2000\n\n\n\nnote: 정답예시: 아래와 같은 형식으로 출력하는 코드를 작성하면 정답으로 인정\ntrain\n{0: 4666, 1: 5362, 2: 1304, 3: 2159, 4: 1937, 5: 572}\n--\nvalidation\n{0: 550, 1: 704, 2: 178, 3: 275, 4: 212, 5: 81}\n--\ntest\n{0: 581, 1: 695, 2: 159, 3: 275, 4: 224, 5: 66}\n--\nhint: 아래중 원하는 형태를 이용하여 풀이하면 편리하다.\n\nemotion['train']['label']\nemotion['train'].to_dict()\nemotion['train'].to_pandas()\n\n(풀이)\n\n{key: {i:emotion[key]['label'].count(i) for i in set(emotion[key]['label'])} for key in emotion}\n\n{'train': {0: 4666, 1: 5362, 2: 1304, 3: 2159, 4: 1937, 5: 572},\n 'validation': {0: 550, 1: 704, 2: 178, 3: 275, 4: 212, 5: 81},\n 'test': {0: 581, 1: 695, 2: 159, 3: 275, 4: 224, 5: 66}}\n\n\n(2) emotion 데이터셋의 test셋에서 각 감정(label)별로 가장 짧은 길이를 가진 텍스트를 출력하는 코드를 작성하라. – 5점\nnote: 정답예시는 아래와 같다.\n['i feels so lame',\n 'i feel any better',\n 'i just feel tender',\n 'i feel so damn agitated',\n 'i feel alarmed',\n 'i feel all funny sometimes']\n(풀이1)\n\n_라벨0만 = [dct['text'] for dct in emotion['test'] if dct['label']==0]\nmin(map(len,_라벨0만))\n[txt for txt in _라벨0만 if len(txt)==min(map(len,_라벨0만))]\n\n['i feels so lame']\n\n\n\ndef func(_라벨0만):\n    min(map(len,_라벨0만))\n    return [txt for txt in _라벨0만 if len(txt)==min(map(len,_라벨0만))]\n\n\nlist(map(func,[[dct['text'] for dct in emotion['test'] if dct['label']==lbl] for lbl in range(6)]))\n\n[['i feels so lame'],\n ['i feel any better'],\n ['i just feel tender'],\n ['i feel so damn agitated'],\n ['i feel alarmed'],\n ['i feel all funny sometimes']]\n\n\n(풀이2)\n\ndf = emotion['test'].to_pandas()\ndf['length'] = list(map(len,df['text']))\ndf = df.sort_values('length')\ndf \n\n\n\n\n\n\n\n\ntext\nlabel\nlength\n\n\n\n\n221\ni feel alarmed\n4\n14\n\n\n1632\nid feel frantic\n4\n15\n\n\n634\ni feels so lame\n0\n15\n\n\n873\ni feel soo lonely\n0\n17\n\n\n1389\ni feel any better\n1\n17\n\n\n...\n...\n...\n...\n\n\n611\ni couldnt help feeling for him and this awful ...\n0\n284\n\n\n119\ni feel like i know who most of them are by now...\n1\n287\n\n\n966\ni have the joy of allowing kids to feel like t...\n1\n289\n\n\n475\ni feel very honored to have been shortlisted w...\n1\n294\n\n\n618\ni am feeling a little more relaxed i am certai...\n1\n296\n\n\n\n\n2000 rows × 3 columns\n\n\n\n\nfor _,subdf in df.groupby('label'):\n    display(subdf.text.iloc[0])\n\n'i feels so lame'\n\n\n'i feel any better'\n\n\n'i just feel tender'\n\n\n'i feel so damn agitated'\n\n\n'i feel alarmed'\n\n\n'i feel all funny sometimes'\n\n\n\n\n2. emotion 자료 감성분석 – 80점\n\n(1)은 부분점수 있음, (2)는 부분점수 없음.\n\n(1) 아래의 reference 를 참고하여 emotion에 대한 감성분석모델을 학습하는 코드를 작성하라. – 60점\nref:\n\nhttps://guebin.github.io/MP2024/posts/02wk-1.html\nhttps://huggingface.co/docs/transformers/tasks/sequence_classification\n\n세부지침 – 세부지침을 따르지 않을시 감점이 있음 (지침1은 30점감점 지침2는 5점감점)\n지침1. Trainer생성시 eval_dataset에는 emotion['validation']를 전처리한 데이터를 이용하라. (emotion['test'] 가 아니라)\n지침2. TrainingArguments에서 num_train_epochs은 1로 설정하라.\nhint: imdb 자료의 경우 num_labels = 2 이지만, emotion 자료의 경우 그렇지 않음을 유의하라.\n(풀이)\n\nimport datasets\nimport transformers\nimport evaluate\nimport numpy as np\n\n\n## Step1 \n데이터불러오기 = datasets.load_dataset\n데이터전처리하기1 = 토크나이저 = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") \ndef 데이터전처리하기2(examples):\n    return 데이터전처리하기1(examples[\"text\"], truncation=True)\n## Step2 \n인공지능생성하기 = transformers.AutoModelForSequenceClassification.from_pretrained\n## Step3 \n데이터콜렉터 = transformers.DataCollatorWithPadding(tokenizer=토크나이저)\ndef 평가하기(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    accuracy = evaluate.load(\"accuracy\")\n    return accuracy.compute(predictions=predictions, references=labels)\n트레이너세부지침생성기 = transformers.TrainingArguments\n트레이너생성기 = transformers.Trainer\n## Step4 \n강인공지능생성하기 = transformers.pipeline\n\n/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n\n\n\n## Step1 \n데이터 = 데이터불러오기('emotion')\n전처리된데이터 = 데이터.map(데이터전처리하기2,batched=True)\n전처리된훈련자료, 전처리된검증자료 = 전처리된데이터['train'], 전처리된데이터['validation']\n## Step2 \n인공지능 = 인공지능생성하기(\"distilbert/distilbert-base-uncased\", num_labels=6)\n## Step3 \n트레이너세부지침 = 트레이너세부지침생성기(\n    output_dir=\"my_awesome_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=1, # 전체문제세트를 2번 공부하라..\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n)\n트레이너 = 트레이너생성기(\n    model=인공지능,\n    args=트레이너세부지침,\n    train_dataset=전처리된훈련자료,\n    eval_dataset=전처리된검증자료,\n    tokenizer=토크나이저,\n    data_collator=데이터콜렉터,\n    compute_metrics=평가하기,\n)\n트레이너.train()\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n    \n      \n      \n      [1000/1000 00:26, Epoch 1/1]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n0.264700\n0.203025\n0.932500\n\n\n\n\n\n\nTrainOutput(global_step=1000, training_loss=0.5039049835205078, metrics={'train_runtime': 27.0307, 'train_samples_per_second': 591.921, 'train_steps_per_second': 36.995, 'total_flos': 194625049506048.0, 'train_loss': 0.5039049835205078, 'epoch': 1.0})\n\n\n(2) 1-(2)에서 구해진 text에 대하여 감성분석을 수행하라. – 20점\n힌트 1-(2)를 풀지못하였다면 아래의 코드를 이용하여 강제설정할 것\n['i feels so lame',\n 'i feel any better',\n 'i just feel tender',\n 'i feel so damn agitated',\n 'i feel alarmed',\n 'i feel all funny sometimes']\n\n## Step4 \n강인공지능 = 강인공지능생성하기(\"sentiment-analysis\", model=\"my_awesome_model/checkpoint-1000\")\n강인공지능([\n    'i feels so lame',\n    'i feel any better',\n    'i just feel tender',\n    'i feel so damn agitated',\n    'i feel alarmed',\n    'i feel all funny sometimes'\n])\n\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n\n\n[{'label': 'LABEL_0', 'score': 0.9876920580863953},\n {'label': 'LABEL_1', 'score': 0.9838089942932129},\n {'label': 'LABEL_2', 'score': 0.9324933886528015},\n {'label': 'LABEL_3', 'score': 0.7004759907722473},\n {'label': 'LABEL_4', 'score': 0.9285796880722046},\n {'label': 'LABEL_5', 'score': 0.8900141716003418}]\n\n\n\n\n3. O/X. – 10점\n\n모두 맞출경우만 정답으로 인정\n\n아래의 제시문을 읽고 올바르게 해석한 사람을 모두 고르라.\n\n\n\n\n\n\nNote\n\n\n\n나는 50,000개의 “텍스트-라벨” 데이터를 인공지능에게 학습시켰다. 학습이 끝난 후, 50,000개의 데이터 중 20개의 샘플을 무작위로 뽑아 테스트한 결과, 인공지능은 20개의 텍스트에 대한 라벨을 모두 정확히 맞췄다. 이 결과만으로 인공지능이 영화 리뷰에 대한 감성 분석(긍정/부정)을 성공적으로 학습했다고 결론을 내려도 될까?\n\n\n민지: 제시문에 따르면 50,000개의 훈련 데이터를 사용하여 인공지능을 학습시켰으니, train_data의 크기는 50,000이라고 볼 수 있어.\n하니: 50,000개의 데이터 중 일부를 무작위로 샘플링하여 평가하는 것은 올바른 방법이 아니야. 학습에 사용되지 않은 별도의 테스트 데이터를 사용해 성능을 평가해야 인공지능이 제대로 학습했는지 알 수 있어.\n다니엘: 하니의 말이 맞아. 50,000개의 데이터 중 20개를 샘플링한 게 아니라, 50,000개의 데이터를 모두 올바르게 맞췄다고 하더라도, 새로운 데이터에 대해 성능이 좋다고 단정할 수는 없어. 중요한 건 새로운 데이터에 대한 예측 성능이지.\n해린: 맞아, 훈련 데이터 (=학습 데이터) 를 너무 반복해서 학습하다 보면, 인공지능이 그 데이터에만 지나치게 맞춰져서 새로운 데이터를 잘 처리하지 못할 수 있어. 결국 모델이 학습 데이터에서는 좋은 성능을 내지만, 학습하지 않은 데이터나 실제 환경에서는 성능이 떨어질 위험이 생기는 거야.”\n혜인: 그렇구나! 그래서 50,000개의 데이터가 있더라도, 그 중 일부만 학습에 사용하고, 나머지는 평가용으로 따로 남겨두기도 하는 거네. 이렇게 하면 인공지능이 새로운 데이터에서도 잘 작동하는지, 성능을 확인할 수 있는 거잖아?\n\n모두 정답"
  },
  {
    "objectID": "quiz.html",
    "href": "quiz.html",
    "title": "Quiz",
    "section": "",
    "text": "공지사항\n\n퀴즈1은 점수에 포함되지 않습니다.\n퀴즈2의 2-(2)의 채점은 가산점으로 처리합니다. (따라서 퀴즈2는 80점만점)\n\n퀴즈\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 24, 2024\n\n\nQuiz-2 (2024.09.24) // 범위: 02wk-1 까지\n\n\n최규빈 \n\n\n\n\nSep 5, 2024\n\n\nQuiz-1 (2024.09.05) // 범위: 파이썬기본문법\n\n\n최규빈 \n\n\n\n\n\nNo matching items"
  }
]