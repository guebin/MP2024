[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "기계학습활용 (2024)",
    "section": "",
    "text": "질문하는 방법\n\n카카오톡: 질문하러 가기\n이메일: guebin@jbnu.ac.kr\n직접방문: 자연과학대학 본관 205호\nZoom: 카카오톡이나 이메일로 미리 시간을 정할 것\nLMS쪽지: https://ieilms.jbnu.ac.kr/\n\nQuiz: https://guebin.github.io/MP2024/quiz.html\n강의노트\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nNov 12, 2024\n\n\n10wk-1: Datasets 클래스\n\n\n최규빈 \n\n\n\n\nNov 9, 2024\n\n\n09wk-2: model의 입력파악, model의 사용연습\n\n\n최규빈 \n\n\n\n\nNov 1, 2024\n\n\n09wk-1: with의 사용\n\n\n최규빈 \n\n\n\n\nOct 31, 2024\n\n\n08wk-2: numpy와 torch의 차이\n\n\n최규빈 \n\n\n\n\nOct 30, 2024\n\n\n08wk-1: 모듈설치 및 변경\n\n\n최규빈 \n\n\n\n\nOct 17, 2024\n\n\n06wk-1, 07wk-1: UCF101 영상자료 분류\n\n\n최규빈 \n\n\n\n\nOct 4, 2024\n\n\n05wk-1: Food-101 이미지자료 분류\n\n\n최규빈 \n\n\n\n\nSep 27, 2024\n\n\n04wk-1: 감성분석 파고들기 (2)\n\n\n최규빈 \n\n\n\n\nSep 19, 2024\n\n\n03wk-1: 감성분석 파고들기 (1)\n\n\n최규빈 \n\n\n\n\nSep 12, 2024\n\n\n02wk-1: IMDB 영화평 감성분석\n\n\n최규빈 \n\n\n\n\nSep 6, 2024\n\n\n01wk-2: IMDB 자료 살펴보기, 지도학습의 개념\n\n\n최규빈 \n\n\n\n\nSep 3, 2024\n\n\n01wk-1: 강의소개\n\n\n최규빈 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "quiz/Quiz-3.html",
    "href": "quiz/Quiz-3.html",
    "title": "Quiz-3 (2024.10.08) // 범위: 04wk-1 까지",
    "section": "",
    "text": "항목\n허용 여부\n비고\n\n\n\n\n강의노트 참고\n허용\n수업 중 제공된 강의노트나 본인이 정리한 자료를 참고 가능\n\n\n구글 검색\n허용\n인터넷을 통한 자료 검색 및 정보 확인 가능\n\n\n생성 모형 사용\n허용 안함\n인공지능 기반 도구(GPT 등) 사용 불가\n\n\n\n\n\nimport datasets\nimport transformers\nimport evaluate\nimport numpy as np\nimport pandas as pd \nimport torch \nfrom sklearn.model_selection import train_test_split\n\n/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n1. MBTI 자료분석 – 30점\nKaggle 링크: MBTI 데이터셋\nKaggle에 가입한 후, archive.zip 파일을 다운로드하고 Colab에 업로드한 뒤, 아래 명령어를 실행하여 데이터를 불러오라.\n!unzip archive.zip\ndf = pd.read_csv(\"mbti_1.csv\")\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\ndf_train = df_train.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n이 데이터는 Myers-Briggs Type Indicator(MBTI) 성격 유형과 관련된 사람들의 텍스트 데이터를 포함하고 있다. 데이터는 사람들이 작성한 게시물의 내용과 MBTI 성격 유형 간의 관계를 분석할 수 있도록 구성되어 있다.\n\n총 데이터 수: 약 8600개의 행\n\n학습용 데이터: 6940개 행\n\n테스트용 데이터: 1735개 행\n\n각 행의 구성:\n\nType (성격 유형): MBTI 성격 유형 (예: INTP, ENFJ 등)\nPosts (게시물): 해당 사용자가 작성한 게시물\n\n\n(1) 주어진 MBTI 데이터셋을 활용하여 T(Thinking) 성향의 사람이 작성한 게시물인지 F(Feeling) 성향의 사람이 작성한 게시물인지 구분하는 모델을 학습하라.\n주의\n\ndf_train을 훈련자료로, df_test를 검증자료로 사용하라.\n검증자료에 대한 정확도가 80%이상일 경우만 정답으로 인정한다.\n\n(풀이)\n\n!unzip archive.zip\ndf = pd.read_csv(\"mbti_1.csv\")\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\ndf_train = df_train.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\nArchive:  archive.zip\n  inflating: mbti_1.csv              \n\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n\ndf_train['label'] = [int(\"F\" in l) for l in df_train.type]\ndf_test['label'] = df_test.type.map(lambda x: int(\"F\" in x))\n\n\ntrain = datasets.Dataset.from_pandas(df_train)\ntest = datasets.Dataset.from_pandas(df_test)\n\n\n데이터 = datasets.dataset_dict.DatasetDict({\n    'train':train,\n    'test':test\n})\n\n\n데이터\n\nDatasetDict({\n    train: Dataset({\n        features: ['type', 'posts', 'label'],\n        num_rows: 6940\n    })\n    test: Dataset({\n        features: ['type', 'posts', 'label'],\n        num_rows: 1735\n    })\n})\n\n\n\n## Step1 \n데이터전처리하기1 = 토크나이저 = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") \ndef 데이터전처리하기2(examples):\n    return 데이터전처리하기1(examples[\"posts\"], truncation=True)\n## Step2 \n인공지능생성하기 = transformers.AutoModelForSequenceClassification.from_pretrained\n## Step3 \n데이터콜렉터 = transformers.DataCollatorWithPadding(tokenizer=토크나이저)\ndef 평가하기(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    accuracy = evaluate.load(\"accuracy\")\n    return accuracy.compute(predictions=predictions, references=labels)\n트레이너세부지침생성기 = transformers.TrainingArguments\n트레이너생성기 = transformers.Trainer\n## Step4 \n강인공지능생성하기 = transformers.pipeline\n#---#\n## Step1 \n#데이터 = 데이터불러오기('imdb')\n전처리된데이터 = 데이터.map(데이터전처리하기2,batched=True)\n전처리된훈련자료, 전처리된검증자료 = 전처리된데이터['train'], 전처리된데이터['test']\n## Step2 \n인공지능 = 인공지능생성하기(\"distilbert/distilbert-base-uncased\", num_labels=2)\n## Step3 \n트레이너세부지침 = 트레이너세부지침생성기(\n    output_dir=\"my_awesome_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2, # 전체문제세트를 2번 공부하라..\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n)\n트레이너 = 트레이너생성기(\n    model=인공지능,\n    args=트레이너세부지침,\n    train_dataset=전처리된훈련자료,\n    eval_dataset=전처리된검증자료,\n    tokenizer=토크나이저,\n    data_collator=데이터콜렉터,\n    compute_metrics=평가하기,\n)\n트레이너.train()\n\nMap: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6940/6940 [00:02&lt;00:00, 2420.93 examples/s]\nMap: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1735/1735 [00:00&lt;00:00, 2450.23 examples/s]\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n    \n      \n      \n      [868/868 02:53, Epoch 2/2]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\nNo log\n0.516274\n0.752738\n\n\n2\n0.526700\n0.432911\n0.817867\n\n\n\n\n\n\nTrainOutput(global_step=868, training_loss=0.4732443638111589, metrics={'train_runtime': 173.4952, 'train_samples_per_second': 80.002, 'train_steps_per_second': 5.003, 'total_flos': 1838647493345280.0, 'train_loss': 0.4732443638111589, 'epoch': 2.0})\n\n\n(2) 학습된 모형을 사용하여 아래의 Text에 대한 분류를 수행하라.\n\n[\"I prefer making decisions based on logic and facts. Analyzing situations objectively is more important to me than considering emotions.\", \n \"When making decisions, I think it’s most important to consider how others feel. Understanding and empathizing with the situation is always my top priority. I feel happiest when relationships are harmonious, and I try to maintain an emotional balance in everything I do.\"]\n\n['I prefer making decisions based on logic and facts. Analyzing situations objectively is more important to me than considering emotions.',\n 'When making decisions, I think it’s most important to consider how others feel. Understanding and empathizing with the situation is always my top priority. I feel happiest when relationships are harmonious, and I try to maintain an emotional balance in everything I do.']\n\n\n(풀이)\n\n## Step4 \n강인공지능 = 강인공지능생성하기(\"sentiment-analysis\", model=\"my_awesome_model/checkpoint-868\")\ntexts = [\"I prefer making decisions based on logic and facts. Analyzing situations objectively is more important to me than considering emotions.\", \n \"When making decisions, I think it’s most important to consider how others feel. Understanding and empathizing with the situation is always my top priority. I feel happiest when relationships are harmonious, and I try to maintain an emotional balance in everything I do.\"]\n강인공지능(texts)\n\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n\n\n[{'label': 'LABEL_0', 'score': 0.7769721150398254},\n {'label': 'LABEL_1', 'score': 0.8229644298553467}]\n\n\n\n\n2. sms_spam 데이터분석 – 70점\n아래는 Hugging Face의 sms_spam 데이터셋을 로드하는 코드이다:\n\nsms_spam = datasets.load_dataset('sms_spam')['train'].train_test_split(test_size=0.2, seed=42)\nsms_spam\n\nDatasetDict({\n    train: Dataset({\n        features: ['sms', 'label'],\n        num_rows: 4459\n    })\n    test: Dataset({\n        features: ['sms', 'label'],\n        num_rows: 1115\n    })\n})\n\n\nsms_spam['train'][-3] 는 훈련 데이터의 마지막에서 세번째 항목을 출력한다. 출력된 샘플은 다음과 같다.\n\nsms_spam['train'][-3]\n\n{'sms': 'FREE camera phones with linerental from 4.49/month with 750 cross ntwk mins. 1/2 price txt bundle deals also avble. Call 08001950382 or call2optout/J MF\\n',\n 'label': 1}\n\n\n출력된 샘플은 딕셔너리 형식으로, sms 항목에는 “FREE camera phones with linerental from 4.49/month…”와 같은 문장이 담겨 있다. 이 문장은 스팸(Spam) 메시지로 분류되며, label 항목에 1로 저장되어 있다.\nlabel이 나타내는 분류는 다음과 같이 정의된다:\n\nsms_spam['train'].features['label'].names\n\n['ham', 'spam']\n\n\n분류 레이블은 총 2가지로 나누며, 각각의 레이블은 다음과 같이 정의된다:\n\n{0: 'ham', 1: 'spam'}\n\n{0: 'ham', 1: 'spam'}\n\n\n따라서, 문장 “FREE camera phones with linerental…”의 분류는 label이 1이므로, 스팸(Spam)에 해당한다.\n(1) 아래의 코드를 참고하여 sms_spam 자료를 전처리하라. – 10점\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") \ndef preprocess_function(examples):\n    return tokenizer(examples[\"sms\"], truncation=True)\n전처리된 결과의 샘플은 아래와 같다.\n\nsms_spam_preprocessed['train'][9]\n\n{'sms': 'Headin towards busetop\\n',\n 'label': 0,\n 'input_ids': [101, 2132, 2378, 2875, 3902, 18903, 2361, 102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n(풀이)\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") \ndef preprocess_function(examples):\n    return tokenizer(examples[\"sms\"], truncation=True)\n\n/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n\n\n\nsms_spam_preprocessed = sms_spam.map(preprocess_function,batched=True)\n\nMap: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1115/1115 [00:00&lt;00:00, 22849.15 examples/s]\n\n\n\nsms_spam_preprocessed['train'][9]\n\n{'sms': 'Headin towards busetop\\n',\n 'label': 0,\n 'input_ids': [101, 2132, 2378, 2875, 3902, 18903, 2361, 102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n(2) 적절한 모델을 설계하고 sms_spam 데이터를 분석하여 스팸 여부를 판별하는 코드를 작성하라. – 30점\n\nsms_spam['train']을 훈련자료로, sms_spam['test']를 검증자료로 사용하라.\n검증자료에 대한 정확도가 90%이상일 경우만 정답으로 인정한다.\n\n(풀이)\n\n## Step1 \n#데이터전처리하기1 = 토크나이저 = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") \n# def 데이터전처리하기2(examples):\n#     return 데이터전처리하기1(examples[\"posts\"], truncation=True)\n데이터전처리하기2 = preprocess_function\n데이터전처리하기1 = 토크나이저 = tokenizer\n## Step2 \n인공지능생성하기 = transformers.AutoModelForSequenceClassification.from_pretrained\n## Step3 \n데이터콜렉터 = transformers.DataCollatorWithPadding(tokenizer=토크나이저)\ndef 평가하기(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    accuracy = evaluate.load(\"accuracy\")\n    return accuracy.compute(predictions=predictions, references=labels)\n트레이너세부지침생성기 = transformers.TrainingArguments\n트레이너생성기 = transformers.Trainer\n## Step4 \n강인공지능생성하기 = transformers.pipeline\n#---#\n## Step1 \n데이터 = sms_spam\n#전처리된데이터 = 데이터.map(데이터전처리하기2,batched=True)\n전처리된데이터 = sms_spam_preprocessed\n전처리된훈련자료, 전처리된검증자료 = 전처리된데이터['train'], 전처리된데이터['test']\n## Step2 \n인공지능 = 인공지능생성하기(\"distilbert/distilbert-base-uncased\", num_labels=2)\n## Step3 \n트레이너세부지침 = 트레이너세부지침생성기(\n    output_dir=\"my_awesome_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2, # 전체문제세트를 2번 공부하라..\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n)\n트레이너 = 트레이너생성기(\n    model=인공지능,\n    args=트레이너세부지침,\n    train_dataset=전처리된훈련자료,\n    eval_dataset=전처리된검증자료,\n    tokenizer=토크나이저,\n    data_collator=데이터콜렉터,\n    compute_metrics=평가하기,\n)\n트레이너.train()\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n    \n      \n      \n      [558/558 00:25, Epoch 2/2]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\nNo log\n0.044485\n0.990135\n\n\n2\n0.058800\n0.050162\n0.989238\n\n\n\n\n\n\nTrainOutput(global_step=558, training_loss=0.05353698769991543, metrics={'train_runtime': 25.5284, 'train_samples_per_second': 349.336, 'train_steps_per_second': 21.858, 'total_flos': 149317665725664.0, 'train_loss': 0.05353698769991543, 'epoch': 2.0})\n\n\n\n트레이너.predict(전처리된훈련자료)\n\n\n\n\nPredictionOutput(predictions=array([[ 3.2914727, -2.9270098],\n       [ 3.0869112, -2.7461848],\n       [ 3.2496836, -2.9365034],\n       ...,\n       [-2.531946 ,  2.7342494],\n       [ 2.6004195, -2.2583973],\n       [ 3.1634114, -2.79446  ]], dtype=float32), label_ids=array([0, 0, 0, ..., 1, 0, 0]), metrics={'test_loss': 0.019209984689950943, 'test_accuracy': 0.9957389549226284, 'test_runtime': 3.4043, 'test_samples_per_second': 1309.824, 'test_steps_per_second': 81.956})\n\n\n\n트레이너.predict(전처리된검증자료)\n\n\n\n\nPredictionOutput(predictions=array([[-2.5481982,  2.7301414],\n       [ 3.1953466, -2.7654445],\n       [ 3.32271  , -2.99813  ],\n       ...,\n       [ 3.3674207, -3.0081215],\n       [ 3.3854856, -3.0333314],\n       [-2.5486443,  2.7200103]], dtype=float32), label_ids=array([1, 0, 0, ..., 0, 0, 1]), metrics={'test_loss': 0.04448513314127922, 'test_accuracy': 0.9901345291479821, 'test_runtime': 2.1984, 'test_samples_per_second': 507.183, 'test_steps_per_second': 31.841})\n\n\n(3) 아래의 자료에 대한 loss를 계산하라. – 30점\nNote1: 2-(2)를 풀지 않은상태에서 계산해도 정답으로 인정\nNote2: loss가 출력되는 어떠한 형태의 답안도 정답으로 인정, 예를들면 아래와 같은 결과를 얻을 경우도 정답으로 인정.\nPredictionOutput(predictions=array([[-2.3364387,  2.866232 ],\n       [ 3.2778776, -2.9869947],\n       [ 3.0951078, -2.8594062],\n       [ 2.7923744, -2.6019576],\n       [ 3.460438 , -3.1327899],\n       [ 3.395659 , -3.091136 ],\n       [ 3.403764 , -3.1232061],\n       [ 3.2923138, -2.9569058],\n       [ 3.1802518, -2.8368335],\n       [ 3.3302088, -2.9617522],\n       [ 3.394341 , -3.0325623],\n       [ 3.1710148, -2.9316459]], dtype=float32),\nlabel_ids=array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\nmetrics={'test_loss': 0.0024105568882077932, 'test_accuracy': 1.0, 'test_runtime': 1.1632, 'test_samples_per_second': 10.317, 'test_steps_per_second': 0.86})\n(풀이)\n\n트레이너.predict(\n    datasets.Dataset.from_dict(sms_spam_preprocessed['test'][::100]) # 데이터 \n)\n\n\n\n\nPredictionOutput(predictions=array([[-2.5481987,  2.7301414],\n       [ 3.1816514, -2.7806158],\n       [ 3.0223804, -2.736825 ],\n       [ 2.9823759, -2.5838423],\n       [ 3.3489246, -3.040609 ],\n       [ 3.3441825, -3.0298705],\n       [ 3.3488343, -2.9992719],\n       [ 3.2199192, -2.915003 ],\n       [ 3.0861435, -2.7881389],\n       [ 3.23049  , -2.9884613],\n       [ 3.2536855, -2.9340549],\n       [ 3.1646938, -2.8317883]], dtype=float32), label_ids=array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), metrics={'test_loss': 0.0026043050456792116, 'test_accuracy': 1.0, 'test_runtime': 1.4644, 'test_samples_per_second': 8.194, 'test_steps_per_second': 0.683})\n\n\n(어려운풀이1)\n\ndct = sms_spam_preprocessed['test'][::100]\n\n\n#데이터콜렉터(sms_spam_preprocessed['test'][::100])\n\n인공지능(**데이터콜렉터(\n    [\n        dict(label=dct['label'][i], input_ids=dct['input_ids'][i],attention_mask=dct['attention_mask'][i]) \n        for i in range(12)\n    ]\n).to(\"cuda:0\"))\n\nSequenceClassifierOutput(loss=tensor(0.0026, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;), logits=tensor([[-2.5482,  2.7301],\n        [ 3.1817, -2.7806],\n        [ 3.0224, -2.7368],\n        [ 2.9824, -2.5838],\n        [ 3.3489, -3.0406],\n        [ 3.3442, -3.0299],\n        [ 3.3488, -2.9993],\n        [ 3.2199, -2.9150],\n        [ 3.0861, -2.7881],\n        [ 3.2305, -2.9885],\n        [ 3.2537, -2.9341],\n        [ 3.1647, -2.8318]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n(어려운풀이2)\n\ndct2 = dct.copy()\ndel dct2['sms']\n인공지능(**데이터콜렉터(dct2).to(\"cuda:0\"))\n\nSequenceClassifierOutput(loss=tensor(0.0026, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;), logits=tensor([[-2.5482,  2.7301],\n        [ 3.1817, -2.7806],\n        [ 3.0224, -2.7368],\n        [ 2.9824, -2.5838],\n        [ 3.3489, -3.0406],\n        [ 3.3442, -3.0299],\n        [ 3.3488, -2.9993],\n        [ 3.2199, -2.9150],\n        [ 3.0861, -2.7881],\n        [ 3.2305, -2.9885],\n        [ 3.2537, -2.9341],\n        [ 3.1647, -2.8318]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n(어려운풀이3)\n\n입력정보들_원시텍스트 = 데이터['test'][::100]['sms']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['test'][::100]['label']) # 이거 해설할때는 이 line 깜빡하고 빼먹었습니다...\n인공지능(**정리된숫자들_토큰화된자료.to(\"cuda:0\"))\n\nSequenceClassifierOutput(loss=tensor(0.0026, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;), logits=tensor([[-2.5482,  2.7301],\n        [ 3.1817, -2.7806],\n        [ 3.0224, -2.7368],\n        [ 2.9824, -2.5838],\n        [ 3.3489, -3.0406],\n        [ 3.3442, -3.0299],\n        [ 3.3488, -2.9993],\n        [ 3.2199, -2.9150],\n        [ 3.0861, -2.7881],\n        [ 3.2305, -2.9885],\n        [ 3.2537, -2.9341],\n        [ 3.1647, -2.8318]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)"
  },
  {
    "objectID": "quiz/Quiz-6.html",
    "href": "quiz/Quiz-6.html",
    "title": "Quiz-6 (2024.11.12) // 범위: 08wk-2 까지",
    "section": "",
    "text": "항목\n허용 여부\n비고\n\n\n\n\n강의노트 참고\n허용\n수업 중 제공된 강의노트나 본인이 정리한 자료를 참고 가능\n\n\n구글 검색\n허용\n인터넷을 통한 자료 검색 및 정보 확인 가능\n\n\n생성 모형 사용\n허용 안함\n인공지능 기반 도구(GPT 등) 사용 불가\n\n\n\n\n\nimport torch\nimport datasets\nimport transformers\n\n\n1. 모듈 – 10점\n????를 적당히 채워서\nfrom numpy import ???? as ???? \n아래와 같은 동작이 가능하도록 하라.\n\na = arr([1,2,3]) \na\n\narray([1, 2, 3])\n\n\n\ntype(a)\n\nnumpy.ndarray\n\n\n\nb = np.array([1,2,3]) \n\nNameError: name 'np' is not defined\n\n\n\nNote: 함수 arr은 기존의 np.array와 같은 효과를 주는 함수임\n\n(풀이)\n\nfrom numpy import array as arr \n\n\n\n2. 파이토치 – 70점\n(1) 아래의 코드를 수정하여 에러를 고치고 계산가능한 코드로 만들라. – 10점\n\na = torch.tensor([1,2,3]).to(\"cuda:0\")\nb = torch.tensor([1,2,3])\na-b\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n\n\n(풀이)\n\na = torch.tensor([1,2,3]).to(\"cuda:0\")\nb = torch.tensor([1,2,3]).to(\"cuda:0\")\na-b\n\ntensor([0, 0, 0], device='cuda:0')\n\n\n(2) 아래의 코드를 수정하여 에러를 고치고 변수 x를 requires_grad=True 옵션과 함께 올바르게 선언하라. – 10점\n\nx = torch.tensor(3, requires_grad = True)\n\nRuntimeError: Only Tensors of floating point and complex dtype can require gradients\n\n\n(풀이)\n\nx = torch.tensor(3.0, requires_grad = True)\n\n(3) 파이토치의 미분기능을 이용하여 \\(f(x)=\\sin(x)\\) 일때 \\(f'(0)\\)의 값을 구하여라. – 10점\nhint: \\(\\sin(x)\\)는 torch.sin(x)를 이용하여 구현하라.\n(풀이)\n\nx = torch.tensor(0.0, requires_grad = True)\ny = torch.sin(x)\ny.backward()\nx.grad\n\ntensor(1.)\n\n\n(4) 아래와 같은 함수 \\(l(x)\\), \\(a(x)\\)를 고려하자.\n\n\\(l(x)= 0.5x -1\\)\n\\(a(x)= \\frac{\\exp(x)}{1+\\exp(x)}\\)\n\n함수 \\(f(x)\\) 는 \\(l\\)과 \\(a\\)를 연속으로 합성한 함수이다. 즉\n\\[f(x) = a(l(x))\\]\n이다. 이러한 함수 \\(f\\)에 대하여 \\(f'(\\frac{1}{4})\\) 를 계산하라. – 20점\nhint: \\(\\exp(x)\\)는 torch.exp(x)를 이용하여 구현하라.\n(풀이1)\n\nx = torch.tensor(1/4, requires_grad = True)\ny = 0.5*x - 1 \nz = torch.exp(y)/(1+torch.exp(y))\nz.backward()\nx.grad\n\ntensor(0.1038)\n\n\n(풀이2)\n\nx = torch.tensor(1/4, requires_grad = True)\ndef l(x):\n    return 0.5*x -1 \ndef a(x):\n    return torch.exp(x) / (1+torch.exp(x))\ny = a(l(x))\ny.backward()\nx.grad\n\ntensor(0.1038)\n\n\n(5) 아래의 코드를 수정하여 logits에 grad_fn=&lt;AddmmBackward0&gt; 이 포함되지 않도록하라. – 20점\n\ntsr = torch.randn(10,16,3,224,224)\nmodel = transformers.VideoMAEForVideoClassification.from_pretrained(\n    \"MCG-NJU/videomae-base\",\n)\nmodel(tsr)\n\nSome weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nImageClassifierOutput(loss=None, logits=tensor([[0.5141, 0.4869],\n        [0.6019, 0.6288],\n        [0.5439, 0.2437],\n        [0.6385, 0.3787],\n        [0.5531, 0.1795],\n        [0.5203, 0.3516],\n        [0.4432, 0.3479],\n        [0.4908, 0.4129],\n        [0.6014, 0.3230],\n        [0.5718, 0.3518]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n(풀이)\n\ntsr = torch.randn(10,16,3,224,224)\nmodel = transformers.VideoMAEForVideoClassification.from_pretrained(\n    \"MCG-NJU/videomae-base\"\n)\ntorch.set_grad_enabled(False)\nmodel(tsr)\n\nSome weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nImageClassifierOutput(loss=None, logits=tensor([[-0.1791, -0.0549],\n        [ 0.0663,  0.0705],\n        [-0.2028, -0.0002],\n        [-0.1335, -0.0124],\n        [ 0.0198,  0.1779],\n        [-0.0024, -0.0916],\n        [-0.1839, -0.0414],\n        [ 0.0090,  0.0989],\n        [-0.1907,  0.0802],\n        [ 0.0009, -0.0485]]), hidden_states=None, attentions=None)\n\n\n\ntorch.set_grad_enabled(False)\n\n&lt;torch.autograd.grad_mode.set_grad_enabled at 0x7bf7100bcc20&gt;\n\n\n\n\n3. 자료분석 – 20점\n(1) 아래코드의 에러를 적절히 수정하라. – 10점\n\nmodel = transformers.AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert/distilbert-base-uncased\", num_labels=2\n)\nmodel_input = {\n    'input_ids': torch.tensor([[101, 2023, 3185, 2003, 6659, 2021, 2009, 2038, 2070, 2204, 3896, 1012, 102]]),\n    'attention_mask': torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n    'labels': torch.tensor([0.0])\n}\nmodel(**model_input)\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nValueError: Target size (torch.Size([1])) must be the same as input size (torch.Size([1, 2]))\n\n\n(풀이)\n\nmodel = transformers.AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert/distilbert-base-uncased\", num_labels=2\n)\nmodel_input = {\n    'input_ids': torch.tensor([[101, 2023, 3185, 2003, 6659, 2021, 2009, 2038, 2070, 2204, 3896, 1012, 102]]),\n    'attention_mask': torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n    'labels': torch.tensor([0])\n}\nmodel(**model_input)\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nSequenceClassifierOutput(loss=tensor(0.7860), logits=tensor([[-0.0562,  0.1216]]), hidden_states=None, attentions=None)\n\n\n(2) 아래의 코드를 관찰하라.\n\ntsr = torch.randn(10,3,224,224)\nmodel = transformers.AutoModelForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224-in21k\",\n    num_labels=3\n)\nmodel(tsr)\n\nSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nImageClassifierOutput(loss=None, logits=tensor([[-0.0173, -0.1017, -0.0902],\n        [-0.0138, -0.0895, -0.1029],\n        [-0.0285, -0.1240, -0.0857],\n        [-0.0270, -0.1114, -0.0732],\n        [-0.0047, -0.0962, -0.1087],\n        [ 0.0087, -0.1476, -0.0941],\n        [ 0.0340, -0.1530, -0.0872],\n        [-0.0320, -0.0898, -0.0756],\n        [-0.0015, -0.0830, -0.0583],\n        [ 0.0087, -0.1169, -0.0601]]), hidden_states=None, attentions=None)\n\n\n위와 동일한 계산을 cuda에서 수행하라. 즉 위에서 선언한 tsr와 model을 cuda로 이동시킨 후 계산하라. – 10점\n(풀이)\n\nmodel.to(\"cuda\")\nmodel(tsr.to(\"cuda\"))\n\nImageClassifierOutput(loss=None, logits=tensor([[-0.0173, -0.1017, -0.0902],\n        [-0.0138, -0.0895, -0.1029],\n        [-0.0285, -0.1240, -0.0857],\n        [-0.0270, -0.1114, -0.0732],\n        [-0.0047, -0.0962, -0.1087],\n        [ 0.0087, -0.1476, -0.0941],\n        [ 0.0340, -0.1530, -0.0872],\n        [-0.0320, -0.0898, -0.0756],\n        [-0.0015, -0.0830, -0.0583],\n        [ 0.0087, -0.1169, -0.0601]], device='cuda:0'), hidden_states=None, attentions=None)"
  },
  {
    "objectID": "quiz/Quiz-8.html",
    "href": "quiz/Quiz-8.html",
    "title": "Quiz-8 (2024.11.26) // 범위: 10wk-1 까지",
    "section": "",
    "text": "항목\n허용 여부\n비고\n\n\n\n\n강의노트 참고\n허용\n수업 중 제공된 강의노트나 본인이 정리한 자료를 참고 가능\n\n\n구글 검색\n허용\n인터넷을 통한 자료 검색 및 정보 확인 가능\n\n\n생성 모형 사용\n허용 안함\n인공지능 기반 도구(GPT 등) 사용 불가"
  },
  {
    "objectID": "quiz/Quiz-5.html",
    "href": "quiz/Quiz-5.html",
    "title": "Quiz-5 (2024.11.05) // 범위: 07wk-1 까지",
    "section": "",
    "text": "항목\n허용 여부\n비고\n\n\n\n\n강의노트 참고\n허용\n수업 중 제공된 강의노트나 본인이 정리한 자료를 참고 가능\n\n\n구글 검색\n허용\n인터넷을 통한 자료 검색 및 정보 확인 가능\n\n\n생성 모형 사용\n허용 안함\n인공지능 기반 도구(GPT 등) 사용 불가\n\n\n\n\n!pip install pytorchvideo evaluate\n\nimport huggingface_hub\nimport tarfile \nimport transformers\nimport pytorchvideo.data\nimport pytorchvideo.transforms\nimport torchvision.transforms\nimport evaluate\nimport torch\nimport numpy as np\nimport imageio\nimport IPython.display\nimport matplotlib.pyplot as plt \n\n\n1. 동영상자료의 이해 – 40점\n\nfile_path = huggingface_hub.hf_hub_download(\n    repo_id=\"sayakpaul/ucf101-subset\", \n    filename=\"UCF101_subset.tar.gz\", \n    repo_type=\"dataset\"\n)\nwith tarfile.open(file_path) as t:\n    t.extractall(\".\")\n\n(1) 아래의 코드를 이용하여 “UCF101_subset/test/BenchPress/v_BenchPress_g05_c02.avi” 를 텐서의 형태로 불러오라.\n\nvideo_path = \"UCF101_subset/test/BenchPress/v_BenchPress_g05_c02.avi\"\nvideo = pytorchvideo.data.encoded_video.EncodedVideo.from_path(video_path).get_clip(0, float('inf'))['video']\n\n위 코드로 불러온 동영상의 흑백/컬러 여부, 프레임 수, 높이(height), 너비(width) 를 확인하라. – 10점\n(풀이)\n\nvideo.shape # 칼라,프레임67,h240,w320 \n\ntorch.Size([3, 67, 240, 320])\n\n\n(2) video의 마지막 프레임에 해당하는 이미지를 출력하라. – 10점\n(풀이)\n\nplt.imshow(video[:,-1,:,:].permute(1,2,0)/255)\n\n\n\n\n\n\n\n\n(3) 이 동영상을 확인할 수 있는 적당한 함수 display_gif를 선언하고 동영상을 확인하라. – 10점\n(풀이)\n\ndef display_gif(video_cthw):\n    video_thwc = video_cthw.permute(1,2,3,0)\n    frames = [frame.numpy().astype('uint8') for frame in video_thwc]\n    imageio.mimsave(\"sample.gif\",frames)\n    return IPython.display.Image(\"sample.gif\")\n\n\ndisplay_gif(video)\n\n&lt;IPython.core.display.Image object&gt;\n\n\n(4) 주어진 동영상에 아래의 변환을 적용한 후, display_gif 함수를 사용하여 결과를 확인하라. – 10점\n\npytorchvideo.transforms.UniformTemporalSubsample(4): 프레임 수를 4개로\ntorchvision.transforms.Resize((112, 112)): 이미지 크기를 112x112로 조정\n\n(풀이)\n\nf = torchvision.transforms.Compose([\n    pytorchvideo.transforms.UniformTemporalSubsample(4),\n    torchvision.transforms.Resize((112, 112))\n])\n\n\ndisplay_gif(f(video))\n\n&lt;IPython.core.display.Image object&gt;\n\n\n\n\n2. ucf101-subset 분석 – 60점\n(1) 아래의 코드와 설정을 이용하여 VideoMAE 모델을 선언하고, ucf101-subset 데이터셋을 변환하여 학습하라. – 50점\n모델선언\n\n프레임 수: 4\n이미지 크기: 112x112\n클래스 레이블 매핑: label2id와 id2label\n\n\nlabel2id = {\n    'ApplyEyeMakeup': 0,\n    'ApplyLipstick': 1,\n    'Archery': 2,\n    'BabyCrawling': 3,\n    'BalanceBeam': 4,\n    'BandMarching': 5,\n    'BaseballPitch': 6,\n    'Basketball': 7,\n    'BasketballDunk': 8,\n    'BenchPress': 9\n}\nid2label = {\n    0: 'ApplyEyeMakeup',\n    1: 'ApplyLipstick',\n    2: 'Archery',\n    3: 'BabyCrawling',\n    4: 'BalanceBeam',\n    5: 'BandMarching',\n    6: 'BaseballPitch',\n    7: 'Basketball',\n    8: 'BasketballDunk',\n    9: 'BenchPress'\n}\nconfig = transformers.VideoMAEConfig.from_pretrained(\"MCG-NJU/videomae-base\")\nconfig.num_frames = 4  # 프레임 수를 4로 조정\nconfig.label2id = label2id\nconfig.id2label = id2label\nconfig.image_size = 112   # 이미지 크기를 112x112로 조정\n\n# 설정된 config로 모델 선언\nmodel = transformers.VideoMAEForVideoClassification.from_pretrained(\n    \"MCG-NJU/videomae-base\",\n    config=config,\n    ignore_mismatched_sizes=True  # 가중치 크기 불일치 무시\n)\n\nSome weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n데이터변환\n모델 학습을 위해 train_dataset, val_dataset, test_dataset에 아래의 “동일한” 전처리를 적용하라. 각 데이터셋에 적용할 clip_sampler와 transform은 다음과 같다:\n# clip_sampler\npytorchvideo.data.make_clip_sampler(\"random\", 3), \n# transform\npytorchvideo.transforms.UniformTemporalSubsample(?),\ntorchvision.transforms.Lambda(lambda x: x / 255.0),\npytorchvideo.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\ntorchvision.transforms.Resize((???,???))\n위의 ?에 들어갈 값은 주어진 설정에 맞게 채우라.\nTraining Args\n아래와 같은 설정으로 transformers.TrainingArguments를 작성하라.\ntransformers.TrainingArguments(\n    '영상분류학습된모델',\n    remove_unused_columns=False,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_ratio=0.1,\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    push_to_hub=False,\n    max_steps=(train_dataset.num_videos // 8) * 4,\n    report_to=\"none\"\n)\n\n학습시간은 코랩에서 약 4~5분 소요\n\n(풀이)\n\n## Step1 \ntrain_dataset = pytorchvideo.data.labeled_video_dataset(\n    data_path = 'UCF101_subset/train',\n    clip_sampler = pytorchvideo.data.make_clip_sampler(\"random\", 3),\n    decode_audio = False,\n    transform = pytorchvideo.transforms.ApplyTransformToKey(\n            key=\"video\",\n            transform=torchvision.transforms.Compose(\n                [\n                    pytorchvideo.transforms.UniformTemporalSubsample(4),\n                    torchvision.transforms.Lambda(lambda x: x / 255.0),\n                    pytorchvideo.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n                    torchvision.transforms.Resize((112,112))\n                ]\n            ),\n        )\n)\nval_dataset = pytorchvideo.data.labeled_video_dataset(\n    data_path = 'UCF101_subset/val',\n    clip_sampler = pytorchvideo.data.make_clip_sampler(\"uniform\", 2.1333333333333333),\n    decode_audio = False,\n    transform = pytorchvideo.transforms.ApplyTransformToKey(\n            key=\"video\",\n            transform=torchvision.transforms.Compose(\n                [\n                    pytorchvideo.transforms.UniformTemporalSubsample(4),\n                    torchvision.transforms.Lambda(lambda x: x / 255.0),\n                    pytorchvideo.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n                    torchvision.transforms.Resize((112,112))\n                ]\n            ),\n        )\n)\ntest_dataset = pytorchvideo.data.labeled_video_dataset(\n    data_path = 'UCF101_subset/test',\n    clip_sampler = pytorchvideo.data.make_clip_sampler(\"uniform\", 2.1333333333333333),\n    decode_audio = False,\n    transform = pytorchvideo.transforms.ApplyTransformToKey(\n            key=\"video\",\n            transform=torchvision.transforms.Compose(\n                [\n                    pytorchvideo.transforms.UniformTemporalSubsample(4),\n                    torchvision.transforms.Lambda(lambda x: x / 255.0),\n                    pytorchvideo.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n                    torchvision.transforms.Resize((112,112))\n                ]\n            ),\n        )\n)\n## Step2 \nlabel2id = {\n    'ApplyEyeMakeup': 0,\n    'ApplyLipstick': 1,\n    'Archery': 2,\n    'BabyCrawling': 3,\n    'BalanceBeam': 4,\n    'BandMarching': 5,\n    'BaseballPitch': 6,\n    'Basketball': 7,\n    'BasketballDunk': 8,\n    'BenchPress': 9\n}\nid2label = {\n    0: 'ApplyEyeMakeup',\n    1: 'ApplyLipstick',\n    2: 'Archery',\n    3: 'BabyCrawling',\n    4: 'BalanceBeam',\n    5: 'BandMarching',\n    6: 'BaseballPitch',\n    7: 'Basketball',\n    8: 'BasketballDunk',\n    9: 'BenchPress'\n}\nconfig = transformers.VideoMAEConfig.from_pretrained(\"MCG-NJU/videomae-base\")\nconfig.num_frames = 4  # 프레임 수를 4로 조정\nconfig.label2id = label2id\nconfig.id2label = id2label\nconfig.image_size = 112   # 이미지 크기를 112x112로 조정\n\n# 설정된 config로 모델 선언\nmodel = transformers.VideoMAEForVideoClassification.from_pretrained(\n    \"MCG-NJU/videomae-base\",\n    config=config,\n    ignore_mismatched_sizes=True  # 가중치 크기 불일치 무시\n)\n\n## Step3 \nmetric = evaluate.load(\"accuracy\")\ndef compute_metrics(eval_pred):\n    predictions = np.argmax(eval_pred.predictions, axis=1)\n    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\ndef collate_fn(examples): # examples 는 [Dict, Dict, ...]\n    tsr_ntchw = torch.stack([example['video'].permute(1,0,2,3) for example in examples])\n    tsrlb_n = torch.tensor([example['label'] for example in examples])\n    return dict(pixel_values=tsr_ntchw,labels=tsrlb_n)\nargs = transformers.TrainingArguments(\n    '동영상분류학습된모델',\n    remove_unused_columns=False,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_ratio=0.1,\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    push_to_hub=False,\n    max_steps=(train_dataset.num_videos // 8) * 4,\n    report_to=\"none\"\n)\nimage_processor = transformers.VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\ntrainer = transformers.Trainer(\n    model,\n    args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=image_processor,\n    compute_metrics=compute_metrics,\n    data_collator=collate_fn,\n)\ntrain_results = trainer.train()\n\nSome weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nmax_steps is given, it will override any value given in num_train_epochs\n\n\n\n    \n      \n      \n      [148/148 01:09, Epoch 3/9223372036854775807]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n0\n1.852300\n1.640228\n0.357143\n\n\n1\n0.725400\n1.060898\n0.557143\n\n\n2\n0.263300\n0.727038\n0.800000\n\n\n3\n0.140500\n0.632462\n0.785714\n\n\n\n\n\n\n(2) test_dataset에 대한 accuracy를 계산하라. – 10점\n\ntrainer.predict(test_dataset)\n\nPredictionOutput(predictions=array([[-1.0152042 , -0.8373261 , -1.207278  , ..., -1.0499216 ,\n         3.6036227 , -0.4558872 ],\n       [ 3.4979932 ,  2.726558  ,  0.1493614 , ..., -0.68777245,\n        -1.0310154 ,  0.03641261],\n       [ 3.4107704 ,  2.795597  , -0.01008109, ..., -0.7813617 ,\n        -1.0597252 ,  0.15882756],\n       ...,\n       [ 3.6092486 ,  2.3927014 ,  1.4169955 , ..., -0.99317455,\n        -1.3091443 , -0.9473401 ],\n       [-1.0195913 , -1.9668864 ,  0.9362006 , ...,  3.7727222 ,\n        -0.80633396, -1.3327856 ],\n       [-1.4548383 , -1.5094501 ,  2.0332494 , ...,  2.9641314 ,\n        -1.1804373 , -0.60928375]], dtype=float32), label_ids=array([8, 0, 0, 3, 3, 7, 9, 3, 3, 3, 9, 9, 9, 8, 8, 3, 3, 3, 8, 8, 9, 9,\n       5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 6, 5, 5,\n       5, 5, 5, 5, 5, 5, 7, 6, 2, 2, 2, 9, 9, 9, 9, 9, 6, 7, 1, 1, 1, 9,\n       9, 2, 2, 6, 8, 6, 7, 2, 6, 3, 3, 3, 3, 6, 9, 9, 9, 9, 4, 1, 1, 1,\n       5, 0, 0, 0, 5, 7, 3, 3, 7, 7, 0, 0, 0, 1, 1, 1, 5, 5, 5, 5, 9, 2,\n       2, 2, 6, 4, 3, 3, 3, 0, 0, 5, 5, 9, 9, 9, 9, 6, 2, 2, 2, 7, 3, 3,\n       3, 3, 7, 5, 7, 8, 0, 0, 2, 2, 2, 2, 3, 3, 6, 2, 4, 4, 1, 1, 1, 7,\n       7]), metrics={'test_loss': 0.7561460137367249, 'test_accuracy': 0.7096774193548387, 'test_runtime': 5.0448, 'test_samples_per_second': 30.725, 'test_steps_per_second': 3.964})\n\n\n\n\n3. 가산점 – 20점\ntrain_dataset에서 레이블이 2인 데이터만을 선택하여, 이 데이터를 기반으로 모델의 예측 성능을 평가하라.\n(풀이)\n\nmodel.to(\"cpu\")\nlogits = model(**collate_fn([d for d in train_dataset if d['label'] == 2])).logits\n\n\nlogits.argmax(axis=1)\n\ntensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0,\n        2, 2, 2, 2, 2, 2])\n\n\n\n29/30\n\n0.9666666666666667"
  },
  {
    "objectID": "posts/09wk-2.html#a.-텍스트",
    "href": "posts/09wk-2.html#a.-텍스트",
    "title": "09wk-2: model의 입력파악, model의 사용연습",
    "section": "A. 텍스트",
    "text": "A. 텍스트\n\nmodel1 = transformers.AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert/distilbert-base-uncased\", num_labels=2\n)\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n# 예제1 – imdb\n\nimdb = datasets.load_dataset('imdb')\n\n\nd = imdb['train'].select(range(3))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 3\n})\n\n\n(풀이1)\n실패\n\nmodel1.forward(torch.tensor(tokenizer(d['text'])['input_ids']))\n\nValueError: expected sequence of length 363 at dim 1 (got 304)\n\n\n원인분석\n\nmp.show_list(\n    tokenizer(d['text'])['input_ids']\n)\n\nList Overview:\nTotal items: 3\n\n1. list[0]\n   - Type: list\n   - Length: 363\n   - Value: [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102]\n\n2. list[1]\n   - Type: list\n   - Length: 304\n   - Value: [101, 1000, 1045, 2572, 8025, 1024, 3756, 1000, 2003, 1037, 15544, 19307, 1998, 3653, 6528, 20771, 19986, 8632, 1012, 2009, 2987, 1005, 1056, 3043, 2054, 2028, 1005, 1055, 2576, 5328, 2024, 2138, 2023, 2143, 2064, 6684, 2022, 2579, 5667, 2006, 2151, 2504, 1012, 2004, 2005, 1996, 4366, 2008, 19124, 3287, 16371, 25469, 2003, 2019, 6882, 13316, 1011, 2459, 1010, 2008, 3475, 1005, 1056, 2995, 1012, 1045, 1005, 2310, 2464, 1054, 1011, 6758, 3152, 2007, 3287, 16371, 25469, 1012, 4379, 1010, 2027, 2069, 3749, 2070, 25085, 5328, 1010, 2021, 2073, 2024, 1996, 1054, 1011, 6758, 3152, 2007, 21226, 24728, 22144, 2015, 1998, 20916, 4691, 6845, 2401, 1029, 7880, 1010, 2138, 2027, 2123, 1005, 1056, 4839, 1012, 1996, 2168, 3632, 2005, 2216, 10231, 7685, 5830, 3065, 1024, 8040, 7317, 5063, 2015, 11820, 1999, 1996, 9478, 2021, 2025, 1037, 17962, 21239, 1999, 4356, 1012, 1998, 2216, 3653, 6528, 20771, 10271, 5691, 2066, 1996, 2829, 16291, 1010, 1999, 2029, 2057, 1005, 2128, 5845, 2000, 1996, 2609, 1997, 6320, 25624, 1005, 1055, 17061, 3779, 1010, 2021, 2025, 1037, 7637, 1997, 5061, 5710, 2006, 9318, 7367, 5737, 19393, 1012, 2077, 6933, 1006, 2030, 20242, 1007, 1000, 3313, 1011, 3115, 1000, 1999, 5609, 1997, 16371, 25469, 1010, 1996, 10597, 27885, 5809, 2063, 2323, 2202, 2046, 4070, 2028, 14477, 6767, 8524, 6321, 5793, 28141, 4489, 2090, 2273, 1998, 2308, 1024, 2045, 2024, 2053, 8991, 18400, 2015, 2006, 4653, 2043, 19910, 3544, 15287, 1010, 1998, 1996, 2168, 3685, 2022, 2056, 2005, 1037, 2158, 1012, 1999, 2755, 1010, 2017, 3227, 2180, 1005, 1056, 2156, 2931, 8991, 18400, 2015, 1999, 2019, 2137, 2143, 1999, 2505, 2460, 1997, 22555, 2030, 13216, 14253, 2050, 1012, 2023, 6884, 3313, 1011, 3115, 2003, 2625, 1037, 3313, 3115, 2084, 2019, 4914, 2135, 2139, 24128, 3754, 2000, 2272, 2000, 3408, 20547, 2007, 1996, 19008, 1997, 2308, 1005, 1055, 4230, 1012, 102]\n\n3. list[2]\n   - Type: list\n   - Length: 133\n   - Value: [101, 2065, 2069, 2000, 4468, 2437, 2023, 2828, 1997, 2143, 1999, 1996, 2925, 1012, 2023, 2143, 2003, 5875, 2004, 2019, 7551, 2021, 4136, 2053, 2522, 11461, 2466, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2514, 6819, 5339, 8918, 2005, 3564, 27046, 2009, 2138, 2009, 12817, 2006, 2061, 2116, 2590, 3314, 2021, 2009, 2515, 2061, 2302, 2151, 5860, 11795, 3085, 15793, 1012, 1996, 13972, 3310, 2185, 2007, 2053, 2047, 15251, 1006, 4983, 2028, 3310, 2039, 2007, 2028, 2096, 2028, 1005, 1055, 2568, 17677, 2015, 1010, 2004, 2009, 2097, 26597, 2079, 2076, 2023, 23100, 2143, 1007, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2488, 5247, 2028, 1005, 1055, 2051, 4582, 2041, 1037, 3332, 2012, 1037, 3392, 3652, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 102]\n\n\n\nmp.show_list(\n    tokenizer(d['text'], padding=True)['input_ids']\n)\n\nList Overview:\nTotal items: 3\n\n1. list[0]\n   - Type: list\n   - Length: 363\n   - Value: [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102]\n\n2. list[1]\n   - Type: list\n   - Length: 363\n   - Value: [101, 1000, 1045, 2572, 8025, 1024, 3756, 1000, 2003, 1037, 15544, 19307, 1998, 3653, 6528, 20771, 19986, 8632, 1012, 2009, 2987, 1005, 1056, 3043, 2054, 2028, 1005, 1055, 2576, 5328, 2024, 2138, 2023, 2143, 2064, 6684, 2022, 2579, 5667, 2006, 2151, 2504, 1012, 2004, 2005, 1996, 4366, 2008, 19124, 3287, 16371, 25469, 2003, 2019, 6882, 13316, 1011, 2459, 1010, 2008, 3475, 1005, 1056, 2995, 1012, 1045, 1005, 2310, 2464, 1054, 1011, 6758, 3152, 2007, 3287, 16371, 25469, 1012, 4379, 1010, 2027, 2069, 3749, 2070, 25085, 5328, 1010, 2021, 2073, 2024, 1996, 1054, 1011, 6758, 3152, 2007, 21226, 24728, 22144, 2015, 1998, 20916, 4691, 6845, 2401, 1029, 7880, 1010, 2138, 2027, 2123, 1005, 1056, 4839, 1012, 1996, 2168, 3632, 2005, 2216, 10231, 7685, 5830, 3065, 1024, 8040, 7317, 5063, 2015, 11820, 1999, 1996, 9478, 2021, 2025, 1037, 17962, 21239, 1999, 4356, 1012, 1998, 2216, 3653, 6528, 20771, 10271, 5691, 2066, 1996, 2829, 16291, 1010, 1999, 2029, 2057, 1005, 2128, 5845, 2000, 1996, 2609, 1997, 6320, 25624, 1005, 1055, 17061, 3779, 1010, 2021, 2025, 1037, 7637, 1997, 5061, 5710, 2006, 9318, 7367, 5737, 19393, 1012, 2077, 6933, 1006, 2030, 20242, 1007, 1000, 3313, 1011, 3115, 1000, 1999, 5609, 1997, 16371, 25469, 1010, 1996, 10597, 27885, 5809, 2063, 2323, 2202, 2046, 4070, 2028, 14477, 6767, 8524, 6321, 5793, 28141, 4489, 2090, 2273, 1998, 2308, 1024, 2045, 2024, 2053, 8991, 18400, 2015, 2006, 4653, 2043, 19910, 3544, 15287, 1010, 1998, 1996, 2168, 3685, 2022, 2056, 2005, 1037, 2158, 1012, 1999, 2755, 1010, 2017, 3227, 2180, 1005, 1056, 2156, 2931, 8991, 18400, 2015, 1999, 2019, 2137, 2143, 1999, 2505, 2460, 1997, 22555, 2030, 13216, 14253, 2050, 1012, 2023, 6884, 3313, 1011, 3115, 2003, 2625, 1037, 3313, 3115, 2084, 2019, 4914, 2135, 2139, 24128, 3754, 2000, 2272, 2000, 3408, 20547, 2007, 1996, 19008, 1997, 2308, 1005, 1055, 4230, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n3. list[2]\n   - Type: list\n   - Length: 363\n   - Value: [101, 2065, 2069, 2000, 4468, 2437, 2023, 2828, 1997, 2143, 1999, 1996, 2925, 1012, 2023, 2143, 2003, 5875, 2004, 2019, 7551, 2021, 4136, 2053, 2522, 11461, 2466, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2514, 6819, 5339, 8918, 2005, 3564, 27046, 2009, 2138, 2009, 12817, 2006, 2061, 2116, 2590, 3314, 2021, 2009, 2515, 2061, 2302, 2151, 5860, 11795, 3085, 15793, 1012, 1996, 13972, 3310, 2185, 2007, 2053, 2047, 15251, 1006, 4983, 2028, 3310, 2039, 2007, 2028, 2096, 2028, 1005, 1055, 2568, 17677, 2015, 1010, 2004, 2009, 2097, 26597, 2079, 2076, 2023, 23100, 2143, 1007, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2488, 5247, 2028, 1005, 1055, 2051, 4582, 2041, 1037, 3332, 2012, 1037, 3392, 3652, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\n성공\n\nmodel1(torch.tensor(tokenizer(d['text'], padding=True)['input_ids']))\n\nWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.0174,  0.1774],\n        [-0.0269,  0.2127],\n        [-0.0322,  0.2528]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n(풀이2)\n\nmodel1(tokenizer(d['text'], padding=True, return_tensors=\"pt\")['input_ids'])\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.0174,  0.1774],\n        [-0.0269,  0.2127],\n        [-0.0322,  0.2528]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n#\n# 예제2 – emotion\n\nemotion = datasets.load_dataset('emotion')\n\n\nd = emotion['train'].select(range(3))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 3\n})\n\n\n(풀이)\n\nmodel1(torch.tensor(tokenizer(d['text'],padding=True)['input_ids']))\n\nSequenceClassifierOutput(loss=None, logits=tensor([[0.0163, 0.2285],\n        [0.0100, 0.2066],\n        [0.0087, 0.2619]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n#\n# 예제3 – MBTI\n\n# mbti1.csv 파일 다운로드 \n!wget https://raw.githubusercontent.com/guebin/MP2024/refs/heads/main/posts/mbti_1.csv\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n--2024-11-15 19:18:39--  https://raw.githubusercontent.com/guebin/MP2024/refs/heads/main/posts/mbti_1.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 62856486 (60M) [text/plain]\nSaving to: ‘mbti_1.csv.1’\n\nmbti_1.csv.1        100%[===================&gt;]  59.94M   107MB/s    in 0.6s    \n\n2024-11-15 19:18:44 (107 MB/s) - ‘mbti_1.csv.1’ saved [62856486/62856486]\n\n\n\n\nd = datasets.Dataset.from_csv(\"mbti_1.csv\").select(range(3))\nd\n\nGenerating train split: 8675 examples [00:00, 16512.48 examples/s]\n\n\nDataset({\n    features: ['type', 'posts'],\n    num_rows: 3\n})\n\n\n(풀이1)\n실패\n\nmodel1(torch.tensor(tokenizer(d['posts'],padding=True)['input_ids']))\n\nToken indices sequence length is longer than the specified maximum sequence length for this model (2102 &gt; 512). Running this sequence through the model will result in indexing errors\n\n\nRuntimeError: The size of tensor a (2102) must match the size of tensor b (512) at non-singleton dimension 1\n\n\n원인분석\n\nmp.show_list(\n    tokenizer(d['posts'],padding=True)['input_ids']\n)\n\nList Overview:\nTotal items: 3\n\n1. list[0]\n   - Type: list\n   - Length: 2102\n   - Value: [101, 1005, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1053, 2015, 2595, 16257, 8545, 2509, 21638, 2860, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 4601, 1012, 2865, 1012, 10722, 14905, 20974, 1012, 4012, 1013, 10722, 14905, 20974, 1035, 1048, 14876, 26230, 2692, 2509, 9737, 27717, 19062, 2487, 3217, 9541, 2487, 1035, 3156, 1012, 16545, 2290, 1064, 1064, 1064, 4372, 22540, 1998, 20014, 3501, 5312, 16770, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1045, 2480, 2581, 2571, 2487, 2290, 2549, 2595, 2213, 2549, 2998, 13013, 2121, 2025, 2327, 2702, 3248, 16770, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 15384, 20952, 4371, 2487, 12870, 2278, 26418, 2015, 1064, 1064, 1064, 2054, 2038, 2042, 1996, 2087, 2166, 1011, 5278, 3325, 1999, 2115, 2166, 1029, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1058, 2595, 4371, 2100, 2860, 13088, 2094, 2860, 2620, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1057, 2620, 20518, 3286, 2629, 18927, 2509, 2063, 2006, 9377, 2005, 2087, 1997, 2651, 1012, 1064, 1064, 1064, 2089, 1996, 2566, 2278, 3325, 10047, 16862, 2063, 2017, 1012, 1064, 1064, 1064, 1996, 2197, 2518, 2026, 1999, 2546, 3501, 2767, 6866, 2006, 2010, 9130, 2077, 16873, 5920, 1996, 2279, 2154, 1012, 2717, 1999, 3521, 1066, 8299, 1024, 1013, 1013, 6819, 26247, 1012, 4012, 1013, 22238, 20958, 11387, 2575, 1064, 1064, 1064, 7592, 4372, 2546, 3501, 2581, 1012, 3374, 2000, 2963, 1997, 2115, 12893, 1012, 2009, 1005, 1055, 2069, 3019, 2005, 1037, 3276, 2000, 2025, 2022, 15401, 2035, 1996, 2051, 1999, 2296, 2617, 1997, 4598, 1012, 3046, 2000, 3275, 1996, 2524, 2335, 2004, 2335, 1997, 3930, 1010, 2004, 1012, 1012, 1012, 1064, 1064, 1064, 6391, 22025, 2683, 6391, 23499, 2692, 8299, 1024, 1013, 1013, 2813, 23298, 15194, 3258, 1012, 4012, 1013, 2039, 11066, 1013, 23297, 8889, 1013, 6860, 1011, 2879, 1011, 1998, 1011, 2611, 1011, 2813, 23298, 1012, 16545, 2290, 8299, 1024, 1013, 1013, 7045, 1012, 2079, 19139, 2497, 1012, 4012, 1013, 1059, 2361, 1011, 4180, 1013, 2039, 11066, 2015, 1013, 2230, 1013, 5840, 1013, 2461, 1011, 2188, 1011, 2640, 1012, 16545, 2290, 1012, 1012, 1012, 1064, 1064, 1064, 6160, 1998, 4933, 1012, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 2447, 7971, 10127, 1012, 4012, 1013, 1059, 2361, 1011, 4180, 1013, 2039, 11066, 2015, 1013, 2286, 1013, 5511, 1013, 2417, 1011, 2417, 1011, 1996, 1011, 20421, 1011, 3040, 1011, 19652, 16086, 22610, 2549, 1011, 10332, 1011, 27908, 1012, 16545, 2290, 2208, 1012, 2275, 1012, 2674, 1012, 1064, 1064, 1064, 4013, 4143, 2278, 1010, 2092, 19892, 21823, 2078, 1010, 2012, 2560, 4228, 2781, 1997, 3048, 2115, 3456, 1006, 1998, 1045, 2123, 1005, 1056, 2812, 3048, 2068, 2096, 3564, 1999, 2115, 2168, 4624, 3242, 1007, 1010, 17901, 1999, 5549, 8156, 1006, 2672, 3046, 21006, 2015, 2004, 1037, 2740, 3771, 4522, 1012, 1012, 1012, 1064, 1064, 1064, 10468, 2272, 2039, 2007, 2093, 5167, 2017, 1005, 2310, 4340, 2008, 2169, 2828, 1006, 2030, 29221, 4127, 2017, 2215, 2000, 2079, 1007, 2052, 2062, 2084, 3497, 2224, 1010, 2445, 2169, 4127, 1005, 10699, 4972, 1998, 2054, 17048, 1010, 2043, 2187, 2011, 1012, 1012, 1012, 1064, 1064, 1064, 2035, 2477, 1999, 5549, 8156, 1012, 18135, 2003, 5262, 1037, 2678, 2208, 1010, 1998, 1037, 2204, 2028, 2012, 2008, 1012, 3602, 1024, 1037, 2204, 2028, 2012, 2008, 2003, 5399, 20714, 1999, 2008, 1045, 2572, 2025, 3294, 7694, 1996, 2331, 1997, 2151, 2445, 21934, 1012, 1012, 1012, 1064, 1064, 1064, 6203, 4372, 22540, 1024, 2054, 2020, 2115, 5440, 2678, 2399, 3652, 2039, 1998, 2054, 2024, 2115, 2085, 1010, 2783, 5440, 2678, 2399, 1029, 1024, 4658, 1024, 1064, 1064, 1064, 16770, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1053, 22571, 4160, 2102, 2620, 2819, 2480, 8029, 1064, 1064, 1064, 2009, 3544, 2000, 2022, 2205, 2397, 1012, 1024, 6517, 1024, 1064, 1064, 1064, 2045, 1005, 1055, 2619, 2041, 2045, 2005, 3071, 1012, 1064, 1064, 1064, 3524, 1012, 1012, 1012, 1045, 2245, 7023, 2001, 1037, 2204, 2518, 1012, 1064, 1064, 1064, 1045, 2074, 24188, 4509, 1996, 2051, 1997, 22560, 1038, 1013, 1039, 1045, 7065, 2884, 2306, 2026, 5110, 2088, 2062, 6168, 2087, 2060, 2051, 1045, 1005, 1040, 2022, 2147, 2378, 1012, 1012, 1012, 2074, 5959, 1996, 2033, 2051, 2096, 2017, 2064, 1012, 2123, 1005, 1056, 4737, 1010, 2111, 2097, 2467, 2022, 2105, 2000, 1012, 1012, 1012, 1064, 1064, 1064, 10930, 4372, 25856, 6456, 1012, 1012, 1012, 2065, 2017, 1005, 2128, 2046, 1037, 19394, 5649, 6180, 1010, 2092, 1010, 4931, 1012, 1064, 1064, 1064, 1012, 1012, 1012, 2043, 2115, 2364, 2591, 13307, 2003, 12202, 2444, 11450, 1998, 2130, 2059, 2017, 12064, 2135, 16342, 2855, 1012, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1043, 16425, 2100, 2581, 4103, 16715, 16932, 1045, 2428, 10667, 1996, 2112, 2013, 1015, 1024, 4805, 2000, 1016, 1024, 2753, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 5796, 4160, 2595, 4246, 5603, 2581, 2497, 2620, 1064, 1064, 1064, 7917, 2138, 2023, 11689, 5942, 2009, 1997, 2033, 1012, 1064, 1064, 1064, 2131, 2152, 1999, 16125, 1010, 25043, 1998, 4521, 9409, 10199, 8261, 2015, 1999, 16125, 2096, 9530, 14028, 2075, 2058, 2242, 7789, 1010, 2628, 2011, 21881, 2015, 1998, 8537, 1012, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 12464, 2581, 8780, 2226, 2509, 25526, 4783, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1018, 2615, 2475, 26230, 2953, 2232, 4160, 6559, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 22889, 2615, 24798, 2546, 4160, 4160, 2692, 3775, 1064, 1064, 1064, 7917, 2005, 2205, 2116, 1038, 1005, 1055, 1999, 2008, 6251, 1012, 2129, 2071, 2017, 999, 2228, 1997, 1996, 1038, 999, 1064, 1064, 1064, 7917, 2005, 3666, 5691, 1999, 1996, 3420, 2007, 1996, 24654, 9623, 1012, 1064, 1064, 1064, 7917, 2138, 2740, 2465, 4415, 4036, 2017, 2498, 2055, 8152, 3778, 1012, 1064, 1064, 1064, 7917, 2005, 1037, 2878, 3677, 1997, 4436, 999, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 20868, 26775, 2615, 23632, 25619, 2480, 2549, 1064, 1064, 1064, 1015, 1007, 2048, 3336, 8448, 2006, 2187, 1998, 2157, 14163, 12680, 2075, 2006, 1037, 7813, 1999, 1996, 2690, 1012, 1016, 1007, 2478, 2037, 2219, 2668, 1010, 2048, 5430, 3549, 9708, 2651, 1005, 1055, 6745, 6230, 2015, 2006, 2037, 4351, 5430, 9708, 2813, 1012, 1017, 1007, 1045, 2156, 2009, 2004, 1012, 1012, 1012, 1064, 1064, 1064, 1037, 20421, 2088, 2019, 1999, 2546, 3501, 2554, 3071, 4150, 2019, 23569, 27605, 3367, 1064, 1064, 1064, 4749, 16932, 2475, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1062, 19170, 4160, 1035, 1046, 7959, 16715, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 7523, 2863, 3654, 21254, 1012, 4012, 1013, 2262, 1013, 21650, 1011, 15476, 1013, 2322, 1011, 2477, 1011, 2017, 1011, 2134, 2102, 1011, 2113, 1011, 2055, 1011, 28858, 1013, 5532, 1012, 16545, 2290, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 21480, 1012, 16270, 5714, 5620, 1012, 4012, 1013, 2865, 9148, 3211, 1013, 17928, 2015, 1012, 16270, 1012, 4012, 1013, 20421, 1011, 3165, 1011, 2544, 1013, 1040, 1013, 20315, 1013, 4487, 9284, 1012, 21025, 2546, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 7479, 1012, 14262, 15878, 6137, 1012, 5658, 1013, 8962, 2860, 1011, 1040, 2361, 1013, 16596, 6844, 2099, 1012, 16545, 2290, 1064, 1064, 1064, 2025, 2035, 3324, 2024, 3324, 2138, 2027, 4009, 1012, 2009, 1005, 1055, 1996, 2801, 2008, 9294, 1999, 5716, 2242, 1997, 2115, 2219, 1012, 1012, 1012, 2066, 1037, 8085, 1012, 1064, 1064, 1064, 6160, 2000, 1996, 8957, 6938, 1010, 2711, 2040, 20164, 2026, 2969, 1011, 19593, 12731, 2480, 1045, 1005, 1049, 2025, 2019, 18568, 8085, 3063, 2066, 2841, 1012, 1024, 7098, 1024, 1064, 1064, 1064, 7917, 2005, 2635, 2035, 1996, 2282, 2104, 2026, 2793, 1012, 8038, 10657, 4553, 2000, 3745, 2007, 1996, 20997, 2229, 1012, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1059, 2620, 8004, 5714, 2078, 28311, 20784, 1064, 1064, 1064, 7917, 2005, 2108, 2205, 2172, 1997, 1037, 8505, 2075, 1010, 24665, 25438, 2989, 2785, 1997, 4040, 1012, 1012, 1012, 15624, 1012, 1064, 1064, 1064, 6289, 2232, 1012, 1012, 1012, 2214, 2152, 2082, 2189, 1045, 4033, 1005, 1056, 2657, 1999, 5535, 1012, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 5887, 26775, 6279, 19797, 2497, 2487, 2860, 1064, 1064, 1064, 1045, 3478, 1037, 2270, 4092, 2465, 1037, 2261, 2086, 3283, 1998, 1045, 1005, 2310, 4066, 1997, 4342, 2054, 1045, 2071, 2079, 2488, 2020, 1045, 2000, 2022, 1999, 2008, 2597, 2153, 1012, 1037, 2502, 2112, 1997, 2026, 4945, 2001, 2074, 2058, 18570, 2870, 2007, 2205, 1012, 1012, 1012, 1064, 1064, 1064, 1045, 2066, 2023, 2711, 1005, 1055, 5177, 3012, 1012, 2002, 1005, 1055, 1037, 4484, 20014, 3501, 2011, 1996, 2126, 1012, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1044, 2290, 2243, 3669, 1011, 16216, 2278, 2575, 2213, 1064, 1064, 1064, 2693, 2000, 1996, 7573, 2181, 1998, 2707, 1037, 2047, 2166, 2005, 2870, 1012, 1005, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n2. list[1]\n   - Type: list\n   - Length: 2102\n   - Value: [101, 1005, 1045, 1005, 1049, 4531, 1996, 3768, 1997, 2033, 1999, 2122, 8466, 2200, 8598, 2075, 1012, 1064, 1064, 1064, 3348, 2064, 2022, 11771, 2065, 2009, 1005, 1055, 1999, 1996, 2168, 2597, 2411, 1012, 2005, 2742, 2033, 1998, 2026, 6513, 2024, 2747, 1999, 2019, 4044, 2073, 2057, 2031, 2000, 5541, 2135, 2224, 11190, 15239, 1998, 8696, 1012, 2045, 3475, 1005, 1056, 2438, 1012, 1012, 1012, 1064, 1064, 1064, 3228, 2047, 3574, 2000, 1005, 2208, 1005, 3399, 1012, 1064, 1064, 1064, 7592, 1008, 4372, 25856, 5861, 1008, 2008, 1005, 1055, 2035, 2009, 3138, 1012, 2084, 2057, 23705, 1998, 2027, 2079, 2087, 1997, 1996, 20661, 2096, 1045, 13399, 2037, 3739, 1998, 2709, 2037, 2616, 2007, 5744, 2773, 13068, 1998, 2062, 5048, 2100, 20237, 1012, 1064, 1064, 1064, 2023, 1009, 3768, 1997, 5703, 1998, 2192, 3239, 12016, 1012, 1064, 1064, 1064, 2613, 26264, 3231, 1045, 3556, 13029, 1012, 4274, 26264, 5852, 2024, 6057, 1012, 1045, 3556, 8574, 2015, 2030, 3020, 1012, 2085, 1010, 2066, 1996, 2280, 10960, 1997, 2023, 11689, 1045, 2097, 5254, 2008, 1045, 2123, 1005, 1056, 2903, 1999, 1996, 26264, 3231, 1012, 2077, 2017, 7221, 4509, 1012, 1012, 1012, 1064, 1064, 1064, 2017, 2113, 2017, 1005, 2128, 2019, 4372, 25856, 2043, 2017, 25887, 2013, 1037, 2609, 2005, 1037, 2095, 1998, 1037, 2431, 1010, 2709, 1010, 1998, 2424, 2111, 2024, 2145, 15591, 2006, 2115, 8466, 1998, 16663, 2115, 4784, 1013, 4301, 1012, 2017, 2113, 2017, 1005, 2128, 2019, 4372, 25856, 2043, 2017, 1012, 1012, 1012, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 10047, 2290, 15136, 2620, 1012, 4871, 3270, 3600, 1012, 2149, 1013, 10047, 2290, 15136, 2620, 1013, 4185, 19317, 1013, 3438, 11387, 2094, 2487, 2546, 2683, 2850, 2575, 2683, 22932, 2050, 2575, 2497, 2581, 2487, 19473, 2575, 1012, 16545, 2290, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 10047, 2290, 1012, 4639, 2094, 16872, 28014, 1012, 4012, 1013, 6282, 2509, 2050, 2692, 2278, 2575, 18827, 22025, 16932, 3540, 2497, 2620, 2549, 2278, 22203, 1064, 1064, 1064, 1045, 2058, 2228, 2477, 2823, 1012, 1045, 2175, 2011, 1996, 2214, 20052, 9106, 14686, 1012, 3383, 1010, 2043, 1037, 2158, 2038, 2569, 3716, 1998, 2569, 4204, 2066, 2026, 2219, 1010, 2009, 2738, 16171, 2032, 2000, 6148, 1037, 3375, 1012, 1012, 1012, 1064, 1064, 1064, 13789, 12155, 10270, 1012, 10722, 14905, 20974, 1012, 4012, 2061, 2003, 1045, 1024, 1040, 1064, 1064, 1064, 4278, 1010, 2199, 1009, 2695, 1064, 1064, 1064, 2025, 2428, 1025, 1045, 1005, 2310, 2196, 2245, 1997, 1041, 1013, 1045, 2030, 1046, 1013, 1052, 2004, 2613, 4972, 1012, 1045, 3648, 2870, 2006, 2054, 1045, 2224, 1012, 1045, 2224, 11265, 1998, 14841, 2004, 2026, 29532, 1012, 10768, 2005, 6699, 1998, 6524, 9033, 1012, 1045, 2036, 2224, 9152, 2349, 2000, 2033, 3997, 1012, 1012, 1012, 1064, 1064, 1064, 2017, 2113, 2295, 1012, 2008, 2001, 13749, 18595, 3560, 1012, 2044, 3038, 2009, 1045, 2428, 2215, 2000, 3046, 2009, 1998, 2156, 2054, 6433, 2007, 2033, 2652, 1037, 2034, 2711, 13108, 1999, 1996, 2067, 2096, 2057, 3298, 2105, 1012, 1045, 2215, 2000, 2156, 1996, 2298, 2006, 1012, 1012, 1012, 1064, 1064, 1064, 2041, 1997, 2035, 1997, 2068, 1996, 2600, 3259, 2028, 2003, 1996, 2190, 1012, 2009, 3084, 2033, 8840, 2140, 1012, 2017, 4364, 2024, 5341, 1024, 1040, 1045, 1005, 1049, 2428, 2152, 2039, 2006, 1996, 10722, 14905, 20974, 2291, 1012, 1064, 1064, 1064, 2061, 2106, 2017, 2963, 2055, 2008, 2047, 2034, 2711, 13108, 2208, 1029, 1045, 1005, 2310, 2042, 14934, 1996, 3109, 2041, 1997, 1996, 6050, 2006, 2026, 8285, 2614, 3941, 2008, 2097, 6073, 1996, 17223, 1012, 2057, 3266, 2000, 2404, 1037, 3232, 8827, 2509, 1005, 1055, 1999, 1012, 1012, 1012, 1064, 1064, 1064, 2053, 1025, 1996, 2126, 2002, 4198, 2477, 2001, 2200, 11265, 1012, 11265, 29532, 2024, 2074, 2004, 5204, 1997, 2037, 10058, 2004, 7367, 29532, 1012, 2742, 1024, 13218, 7084, 2030, 4754, 4869, 1025, 2119, 4372, 25856, 2015, 1012, 1064, 1064, 1064, 2092, 4918, 1045, 2097, 2022, 1996, 2034, 2000, 6449, 1045, 2079, 2131, 9981, 2066, 2017, 2079, 1012, 1045, 16833, 2009, 2039, 2000, 2026, 1018, 2860, 2509, 2540, 3816, 2007, 2026, 16083, 1021, 2860, 2620, 1012, 1021, 2015, 1998, 1022, 2015, 2119, 2066, 2000, 2022, 4384, 1012, 1018, 1005, 1055, 2066, 2000, 2022, 2124, 1006, 2025, 1996, 2168, 1012, 1012, 1012, 1064, 1064, 1064, 1025, 1040, 1045, 1005, 2222, 2039, 11066, 1996, 2168, 12528, 2007, 1996, 23025, 2185, 2013, 2026, 2677, 1012, 2084, 2017, 2180, 1005, 1056, 2963, 2505, 1012, 14104, 12025, 2806, 2021, 2007, 11867, 20051, 3334, 1012, 1064, 1064, 1064, 14841, 2243, 2000, 2243, 2003, 1037, 2428, 2307, 2299, 1012, 2004, 2146, 2004, 2017, 2064, 5177, 3796, 2041, 1996, 3220, 1012, 1045, 2293, 1996, 3786, 2009, 3084, 2033, 17523, 1012, 1064, 1064, 1064, 4530, 1012, 22834, 1058, 2487, 26760, 3600, 2692, 1024, 1040, 23025, 2428, 2485, 2000, 2026, 2677, 1998, 15488, 23212, 2078, 20232, 1024, 18364, 3608, 2652, 1999, 1996, 4281, 1012, 1064, 1064, 1064, 27084, 19210, 1027, 1013, 1027, 4654, 13181, 16874, 1025, 1045, 1005, 1049, 2019, 4654, 13181, 16874, 1998, 1045, 1005, 1049, 2025, 27084, 19210, 1012, 1024, 1007, 1064, 1064, 1064, 20052, 1999, 1996, 3185, 2001, 2019, 4372, 25856, 1012, 5373, 2002, 1005, 1055, 2209, 2004, 1037, 4654, 2102, 3501, 1012, 1999, 1996, 2808, 2002, 1005, 1055, 2019, 9765, 3501, 1012, 2004, 1045, 2056, 1012, 1996, 3185, 2246, 2204, 3272, 2005, 2009, 2108, 2170, 20052, 9106, 1012, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 1045, 2620, 16576, 1012, 6302, 24204, 3388, 1012, 4012, 1013, 4042, 1013, 1062, 2480, 2683, 2575, 1013, 27829, 3695, 2080, 1013, 6530, 10105, 2818, 1012, 1052, 3070, 1064, 1064, 1064, 2821, 1010, 1045, 2196, 2018, 3571, 1997, 7618, 1037, 3124, 1012, 1045, 2097, 3610, 2019, 4111, 2205, 1012, 2061, 2045, 2001, 2498, 2000, 25887, 1012, 2074, 3167, 5510, 1998, 2033, 2025, 16663, 2009, 1012, 1996, 3124, 1045, 4782, 2134, 1005, 1056, 2113, 2033, 1012, 2009, 2001, 2028, 1997, 2216, 1012, 1012, 1012, 1064, 1064, 1064, 4165, 3492, 2172, 2066, 2026, 2181, 1998, 2054, 1045, 1005, 1049, 2183, 2083, 2157, 2085, 2667, 2000, 3275, 2041, 2029, 2126, 1045, 2215, 2000, 2202, 2026, 2166, 1012, 1045, 2215, 2000, 2079, 2061, 2116, 2477, 1012, 1996, 5221, 3291, 2003, 2008, 1045, 2113, 2065, 1045, 2123, 1005, 1056, 1012, 1012, 1012, 1064, 1064, 1064, 1025, 1040, 1045, 2001, 4082, 2104, 1996, 8605, 2008, 2017, 2020, 2931, 1012, 1045, 2196, 2246, 2012, 2115, 3482, 2100, 1012, 3100, 1010, 1045, 2393, 2041, 2026, 5637, 2814, 2035, 1996, 2051, 1998, 2028, 1997, 2068, 2038, 2764, 1037, 2210, 10188, 2006, 2033, 1012, 1045, 2131, 2417, 1012, 1012, 1012, 1064, 1064, 1064, 1056, 1035, 1056, 2017, 2074, 2649, 2033, 1998, 1045, 1005, 1049, 2542, 1996, 5409, 10103, 1012, 1045, 1005, 1049, 7567, 1999, 2028, 2173, 2007, 2028, 2028, 2105, 1012, 2069, 10634, 5249, 1012, 2065, 1045, 2001, 1037, 7642, 6359, 2023, 2052, 2022, 1996, 3819, 2173, 2021, 13718, 1045, 1005, 1049, 1012, 1012, 1012, 1064, 1064, 1064, 26419, 2232, 1010, 1998, 25352, 1010, 4165, 2066, 1037, 25843, 1999, 22540, 1012, 1045, 2228, 2672, 2002, 2001, 3480, 1998, 2357, 9765, 3501, 1012, 1045, 2064, 2425, 2138, 2002, 2038, 2070, 1997, 1996, 5171, 1999, 22540, 12955, 2187, 2058, 1012, 1064, 1064, 1064, 1008, 14148, 2862, 1008, 1045, 1005, 1049, 3374, 1012, 2009, 3849, 2008, 2017, 2031, 2234, 2012, 1037, 2919, 2051, 1012, 2057, 1005, 2310, 2525, 2584, 2256, 20563, 1997, 1999, 2546, 22578, 1012, 2174, 1010, 2108, 2017, 1005, 2128, 2931, 1998, 1045, 2066, 3801, 1045, 2097, 2191, 2017, 1037, 3066, 1012, 1045, 2097, 5926, 2028, 1012, 1012, 1012, 1064, 1064, 1064, 1045, 1005, 1049, 14405, 2361, 1006, 6729, 2646, 1041, 1007, 1012, 1045, 1005, 1049, 3733, 2005, 2119, 4372, 25856, 2015, 1998, 20014, 4523, 2000, 6709, 2007, 1012, 1024, 1007, 1064, 1064, 1064, 1045, 2036, 5674, 4372, 25856, 1005, 1055, 16871, 2015, 2052, 2175, 1037, 2210, 2978, 2066, 2990, 1005, 1055, 2013, 2484, 3272, 2062, 6228, 1012, 19838, 4726, 2039, 5213, 3949, 3941, 1999, 2019, 4704, 2311, 2041, 1997, 2019, 2214, 2482, 7151, 3723, 1010, 21097, 1012, 1012, 1012, 1064, 1064, 1064, 2009, 2001, 1037, 19394, 1024, 1007, 3404, 2033, 1012, 1045, 1005, 1049, 2074, 2004, 18224, 25940, 1024, 1040, 3272, 1045, 2031, 7861, 20214, 5644, 1012, 2027, 1005, 2128, 2074, 6881, 3924, 1012, 2066, 5870, 2043, 1045, 2131, 3480, 2030, 2012, 2111, 2770, 3209, 2058, 2007, 2037, 10168, 9587, 13777, 1012, 1012, 1012, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 1045, 2620, 16576, 1012, 6302, 24204, 3388, 1012, 4012, 1013, 4042, 1013, 1062, 2480, 2683, 2575, 1013, 27829, 3695, 2080, 1013, 8505, 19718, 1012, 1052, 3070, 11039, 25856, 1024, 1013, 1013, 1045, 2620, 16576, 1012, 6302, 24204, 3388, 1012, 4012, 1013, 4042, 1013, 1062, 2480, 2683, 2575, 1013, 27829, 3695, 2080, 1013, 8505, 19718, 2497, 2860, 1012, 1052, 3070, 8299, 1024, 1013, 1013, 1045, 2620, 16576, 1012, 6302, 24204, 3388, 1012, 4012, 1013, 4042, 1013, 1062, 2480, 2683, 2575, 1013, 27829, 3695, 2080, 1013, 14448, 19718, 1012, 1052, 3070, 1064, 1064, 1064, 2053, 1012, 2009, 1005, 1055, 2066, 1037, 4323, 2005, 2073, 1045, 2444, 1998, 2008, 2003, 2339, 1045, 2113, 2009, 2011, 2540, 1012, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1046, 2629, 2860, 2581, 2509, 3270, 2615, 4160, 2497, 2290, 1064, 1064, 1064, 1998, 1045, 5156, 2123, 1005, 1056, 2681, 2127, 1996, 2518, 4515, 1012, 2021, 1999, 1996, 2812, 2051, 1012, 1999, 2090, 2335, 1012, 2017, 2147, 2115, 2518, 1012, 1045, 1005, 2222, 2147, 3067, 1024, 1040, 1025, 1040, 1045, 1005, 1049, 1996, 16914, 2361, 1025, 5165, 2000, 3113, 2017, 1012, 1064, 1064, 1064, 4365, 1010, 2342, 2000, 3404, 2026, 16160, 2062, 1045, 2052, 2031, 2042, 3553, 1045, 2001, 2183, 2000, 2360, 1999, 22540, 1012, 1064, 1064, 1064, 4654, 22540, 1029, 6729, 2646, 1055, 2007, 1996, 2126, 2016, 5838, 1012, 1024, 1040, 2026, 2814, 1010, 2130, 2026, 5637, 1998, 11690, 3924, 1010, 2467, 2272, 2000, 2033, 2005, 6040, 1012, 1064, 1064, 1064, 1045, 6812, 2000, 2026, 4372, 25856, 5972, 4372, 25856, 2015, 2024, 2061, 2307, 1012, 2065, 2009, 2347, 1005, 1056, 2005, 4372, 25856, 2015, 1045, 2876, 1005, 1056, 2031, 2042, 2583, 2000, 3857, 2054, 1045, 1005, 1049, 2311, 9457, 9457, 9457, 13305, 1064, 1064, 1064, 2054, 1029, 2033, 1029, 1045, 2196, 2079, 2008, 1028, 1012, 1028, 1026, 1012, 1026, 1064, 1064, 1064, 2138, 2049, 2524, 2000, 2022, 6517, 2055, 3974, 2619, 2017, 2066, 2043, 2017, 2354, 2017, 2020, 2157, 1998, 2507, 4426, 1037, 2502, 6986, 2006, 1996, 2067, 2138, 2017, 1005, 2128, 12476, 1998, 2467, 6149, 1012, 1064, 1064, 1064, 2821, 1010, 2017, 2123, 1005, 1056, 2031, 2000, 2425, 2033, 2008, 2087, 1997, 2068, 2024, 5236, 1012, 1045, 2113, 2023, 1012, 2008, 2003, 2339, 1045, 2377, 2007, 2068, 1998, 2009, 3084, 2033, 4756, 1012, 1024, 1040, 2004, 1045, 1005, 1049, 2183, 2000, 2202, 11265, 10976, 18075, 9905, 6483, 1998, 1045, 2031, 1037, 2261, 15034, 1012, 1012, 1012, 1064, 1064, 1064, 1024, 1040, 1045, 1005, 1049, 1037, 2305, 5004, 2140, 1012, 1045, 5256, 2039, 2090, 1020, 1011, 1021, 9737, 1998, 2994, 8300, 6229, 2184, 1011, 2340, 1024, 2382, 3286, 1012, 1064, 1064, 1064, 3167, 5448, 6153, 2011, 3399, 2052, 6592, 2008, 20014, 4523, 2024, 1996, 2087, 14286, 3697, 1012, 2096, 20014, 22578, 2064, 2022, 14286, 24436, 2021, 2027, 2097, 2036, 2224, 2591, 8146, 2065, 1996, 1996, 2342, 18653, 1012, 1012, 1012, 1012, 1064, 1064, 1064, 3167, 15768, 2008, 1045, 2031, 2006, 2026, 15363, 2008, 1045, 1005, 2310, 22817, 2013, 6721, 4518, 4573, 1998, 4518, 6302, 24204, 8454, 1012, 1064, 1064, 1064, 1045, 1005, 2222, 2425, 2017, 2043, 1045, 2330, 7760, 18471, 1012, 1024, 1007, 5580, 2017, 2066, 2009, 10763, 1012, 1064, 1064, 1064, 1024, 1040, 4283, 1012, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 1045, 2620, 16576, 1012, 6302, 24204, 3388, 1012, 4012, 1013, 4042, 1013, 1062, 2480, 2683, 2575, 1013, 27829, 3695, 2080, 1013, 2331, 16523, 11514, 1012, 1052, 3070, 8299, 1024, 1013, 1013, 1045, 2620, 16576, 1012, 6302, 24204, 3388, 1012, 4012, 1013, 4042, 1013, 1062, 2480, 2683, 2575, 1013, 27829, 3695, 2080, 1013, 2331, 16523, 11514, 2497, 2860, 1012, 1052, 3070, 2081, 2005, 1037, 2767, 1012, 2195, 2847, 1997, 2147, 1012, 1045, 3833, 2296, 2240, 2011, 1012, 1012, 1012, 1064, 1064, 1064, 1024, 1007, 10763, 1024, 8299, 1024, 1013, 1013, 1045, 2620, 16576, 1012, 6302, 24204, 3388, 1012, 4012, 1013, 4042, 1013, 1062, 2480, 2683, 2575, 1013, 27829, 3695, 2080, 1013, 10763, 23615, 6528, 1012, 1052, 3070, 1045, 1005, 2222, 2031, 2000, 2131, 2000, 2115, 22128, 2101, 2065, 2028, 1997, 2026, 3507, 13220, 2987, 1005, 1056, 1012, 1064, 1064, 1064, 15034, 2123, 1005, 1056, 2562, 2033, 2105, 2146, 2438, 2000, 11616, 2033, 1012, 1045, 2066, 2000, 9121, 2007, 2068, 1012, 2054, 1045, 2031, 11616, 2870, 2007, 1998, 2018, 1037, 2261, 15034, 2814, 1006, 1009, 1037, 2261, 2060, 2814, 1007, 2425, 2033, 1045, 2031, 2003, 1012, 1012, 1012, 1005, 102]\n\n3. list[2]\n   - Type: list\n   - Length: 2102\n   - Value: [101, 1005, 2204, 2028, 1035, 1035, 1035, 1035, 1035, 16770, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1042, 4048, 18259, 28027, 2546, 2290, 2860, 1064, 1064, 1064, 1997, 2607, 1010, 2000, 2029, 1045, 2360, 1045, 2113, 1025, 2008, 1005, 1055, 2026, 13301, 1998, 2026, 8364, 1012, 1064, 1064, 1064, 2515, 2108, 7078, 3893, 2008, 2017, 1998, 2115, 2190, 2767, 2071, 2022, 2019, 6429, 3232, 4175, 1029, 2065, 2061, 1010, 2084, 2748, 1012, 2030, 2009, 1005, 1055, 2062, 1045, 2071, 2022, 29179, 1999, 2293, 1999, 2553, 1045, 28348, 2026, 5346, 1006, 2029, 2012, 1012, 1012, 1012, 1064, 1064, 1064, 2053, 1010, 1045, 2134, 1005, 1056, 1025, 4067, 2017, 2005, 1037, 4957, 999, 1064, 1064, 1064, 2061, 1011, 2170, 14841, 1011, 9033, 7077, 1006, 1998, 2009, 2064, 7872, 2013, 2151, 2783, 8476, 1013, 17418, 1007, 2064, 2022, 9252, 1012, 2009, 1005, 1055, 2066, 2043, 2017, 1005, 2128, 5881, 1999, 2115, 2219, 4301, 1010, 1998, 2115, 2568, 2074, 17677, 2015, 1999, 7925, 1012, 5683, 5621, 6659, 1012, 1012, 1012, 1012, 1064, 1064, 1064, 2031, 2017, 4384, 2129, 14099, 10072, 2064, 2022, 1029, 2035, 2017, 2031, 2000, 2079, 2003, 2298, 2091, 2012, 1996, 5568, 1024, 9877, 1997, 2367, 3269, 2427, 2045, 1012, 1998, 2085, 5674, 2008, 5606, 1997, 2086, 2101, 1006, 2043, 1013, 2065, 5800, 1012, 1012, 1012, 1064, 1064, 1064, 1996, 3044, 2015, 1516, 2196, 2018, 2053, 2028, 2412, 1064, 1064, 1064, 1045, 2411, 2424, 2870, 27963, 5344, 2006, 7720, 13262, 1013, 3536, 1012, 1064, 1064, 1064, 2023, 1019, 2095, 1011, 2214, 6251, 2003, 2019, 11757, 8321, 1998, 3376, 6412, 1012, 1064, 1064, 1064, 1045, 4033, 1005, 1056, 4716, 2023, 4037, 1999, 1996, 2197, 1017, 2086, 1012, 2061, 9444, 9631, 2023, 1006, 1998, 2672, 2130, 17749, 2033, 1010, 2029, 1045, 3811, 4797, 1007, 1024, 7632, 1012, 6352, 2692, 26224, 6352, 2692, 28311, 1064, 1064, 1064, 2043, 2017, 4133, 1999, 2115, 3871, 2127, 2184, 1024, 2382, 7610, 3015, 2774, 1010, 1998, 6170, 2068, 1006, 2362, 2007, 9877, 1997, 4533, 2015, 1007, 2096, 2652, 2115, 6490, 2858, 1012, 1064, 1064, 1064, 2023, 2003, 1996, 2087, 20014, 2361, 1011, 2003, 2232, 11689, 1045, 1005, 2310, 2412, 2464, 1012, 1064, 1064, 1064, 1045, 2876, 1005, 1056, 2022, 2583, 2000, 2298, 2012, 1996, 4169, 2005, 1996, 2972, 2166, 2065, 1045, 2354, 2008, 1045, 3856, 2009, 2058, 1996, 2529, 2108, 1012, 1064, 1064, 1064, 1045, 2001, 5059, 1037, 4281, 2005, 2026, 7284, 2006, 2029, 1045, 1005, 1049, 2551, 2157, 2085, 1011, 2009, 2323, 2031, 2042, 7733, 1012, 1012, 2021, 1045, 2371, 27885, 14715, 3064, 2000, 2191, 2928, 28194, 5420, 2595, 2683, 2475, 2015, 2695, 11522, 2013, 2009, 1024, 1040, 2065, 2017, 3191, 1996, 2338, 1012, 1012, 1012, 1064, 1064, 1064, 1045, 2318, 2000, 2191, 5888, 2055, 13170, 5146, 1998, 21830, 9610, 7834, 1011, 2182, 2017, 2064, 2156, 2048, 2034, 3441, 1024, 16770, 1024, 1013, 1013, 7479, 1012, 10722, 14905, 20974, 1012, 4012, 1013, 9927, 1013, 1011, 4074, 20348, 29159, 1011, 1064, 1064, 1064, 20014, 3501, 3728, 1045, 2318, 2000, 2695, 2026, 5888, 2055, 2048, 2814, 1011, 13170, 5146, 1998, 21830, 9610, 7834, 1012, 2077, 2008, 1010, 1045, 2074, 6866, 4933, 2008, 4699, 2033, 1010, 2021, 2013, 2085, 2006, 1045, 1005, 2222, 3046, 2000, 2421, 2069, 2026, 2573, 1012, 1012, 1012, 1064, 1064, 1064, 2763, 2057, 2071, 2147, 2362, 2006, 1037, 2047, 2944, 1011, 1045, 1005, 1049, 2019, 6739, 1999, 18772, 18217, 1997, 7239, 2588, 2536, 6881, 4933, 1012, 2008, 6433, 2138, 1997, 14099, 3168, 1997, 8562, 1011, 2061, 14099, 2008, 2025, 2172, 1012, 1012, 1012, 1064, 1064, 1064, 7592, 9541, 20976, 1010, 2017, 2064, 3543, 2009, 1012, 3071, 6732, 2008, 2009, 1005, 1055, 6015, 2030, 6517, 1010, 2021, 2008, 1005, 1055, 2025, 2995, 1011, 1999, 2755, 2009, 2038, 2019, 7078, 8699, 2227, 1012, 1998, 2023, 18401, 2941, 2428, 7777, 26085, 1998, 24459, 1006, 2069, 1012, 1012, 1012, 1064, 1064, 1064, 2092, 1012, 1012, 2785, 1997, 1025, 2004, 2009, 2001, 2525, 3855, 1010, 2823, 2138, 1997, 9152, 2009, 1005, 1055, 2524, 2000, 16636, 3375, 4933, 2029, 16949, 2039, 1999, 2115, 2132, 1999, 1059, 14341, 19570, 2389, 22754, 1997, 10466, 1998, 4620, 2069, 2007, 2616, 1012, 1012, 1012, 1012, 1064, 1064, 1064, 1045, 2228, 2023, 18401, 2052, 2022, 2200, 6413, 2182, 1012, 4261, 26187, 2575, 2475, 1064, 1064, 1064, 4029, 19841, 22022, 1064, 1064, 1064, 2204, 2305, 3071, 2041, 2045, 999, 2130, 2065, 2005, 2619, 2045, 2003, 2851, 2157, 2085, 1011, 6385, 2467, 3565, 6924, 2063, 16956, 1012, 1012, 1998, 2111, 2360, 2204, 2305, 1999, 2344, 2000, 3113, 2279, 2154, 1024, 1007, 1064, 1064, 1064, 2821, 1010, 2008, 3185, 1024, 1007, 2009, 1005, 1055, 12476, 4067, 2017, 999, 3246, 2017, 2018, 2204, 3637, 1999, 1996, 2250, 1025, 4312, 1010, 1045, 1005, 1049, 10261, 2017, 2204, 2305, 2005, 1996, 2279, 2305, 3805, 999, 1006, 11504, 2009, 2097, 2022, 2006, 2455, 1007, 2204, 2111, 10107, 2204, 1012, 1012, 1012, 1064, 1064, 1064, 3486, 2620, 2620, 2620, 2475, 3486, 2620, 2620, 21057, 1064, 1064, 1064, 2092, 1010, 2060, 2111, 2040, 2089, 2022, 6603, 2055, 2019, 3277, 2013, 1996, 2171, 1997, 1996, 8476, 2097, 2424, 2115, 3433, 14044, 4312, 1024, 1007, 1064, 1064, 1064, 2023, 1012, 2633, 2619, 3855, 2008, 1024, 1007, 1064, 1064, 1064, 1045, 2145, 2156, 7329, 1013, 5344, 1999, 1037, 15079, 1997, 2536, 6721, 7060, 1012, 2009, 2064, 2022, 19142, 2823, 1012, 2009, 1005, 1055, 1037, 2200, 18801, 8066, 2043, 2017, 1005, 2128, 11471, 1012, 1064, 1064, 1064, 2821, 1010, 1045, 2134, 1005, 1056, 2113, 2008, 1012, 1012, 2054, 1037, 12063, 1012, 2339, 2025, 8688, 2878, 17006, 1010, 2059, 1029, 2057, 2064, 5630, 2029, 24547, 22345, 2097, 2022, 1996, 2190, 1006, 1045, 2228, 1996, 5221, 2028, 2052, 2022, 2307, 1007, 1012, 1064, 1064, 1064, 12316, 27659, 2182, 2017, 2175, 26231, 8889, 2475, 2002, 6732, 2008, 1996, 2543, 2003, 12090, 1012, 2323, 1045, 8688, 2000, 11263, 1029, 1045, 2123, 1005, 1056, 2066, 2000, 5949, 2833, 1012, 1064, 1064, 1064, 1045, 2123, 1005, 1056, 2228, 2008, 1996, 8543, 1997, 2023, 11689, 14977, 2054, 1005, 1055, 2183, 2006, 2182, 2044, 1017, 2086, 1024, 1007, 1064, 1064, 1064, 2002, 2232, 1010, 1045, 3305, 2017, 1024, 1007, 2007, 2122, 2168, 2445, 4155, 1007, 1007, 1007, 1064, 1064, 1064, 2748, 4757, 2015, 1010, 6172, 2051, 1024, 1040, 1064, 1064, 1064, 1045, 2131, 4854, 3243, 6524, 1010, 2021, 2043, 1045, 2079, 1010, 2009, 1005, 1055, 13726, 2005, 4193, 2111, 2000, 2175, 4873, 2842, 1012, 2009, 1005, 1055, 5263, 2005, 2033, 2000, 5342, 2030, 16081, 4963, 1025, 1996, 2069, 2126, 2000, 2131, 9436, 1997, 2023, 3110, 2003, 2000, 6532, 1012, 1012, 1012, 1064, 1064, 1064, 1045, 1005, 2310, 2196, 4669, 2009, 2505, 8275, 2003, 2919, 1010, 2941, 1012, 1064, 1064, 1064, 24459, 2323, 2022, 2445, 2069, 2000, 4217, 3924, 1012, 4217, 1012, 2045, 2024, 3243, 2261, 1997, 2068, 1010, 2295, 1012, 1064, 1064, 1064, 4090, 2683, 2620, 21057, 1064, 1064, 1064, 9805, 2361, 1010, 2017, 1005, 2128, 2725, 2009, 2157, 1024, 1007, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 1011, 4074, 20348, 29159, 1011, 1012, 10722, 14905, 20974, 1012, 4012, 1013, 1064, 1064, 1064, 17273, 2620, 15136, 1064, 1064, 1064, 1997, 2607, 2009, 1005, 1055, 2025, 2200, 6625, 1012, 2021, 1012, 2529, 2679, 5175, 16047, 2308, 1005, 1055, 3754, 2000, 2507, 4182, 2000, 2060, 2529, 9552, 1012, 2009, 2499, 2005, 5190, 1997, 2086, 1012, 2339, 2689, 2009, 1029, 4661, 1010, 2045, 2024, 1012, 1012, 1012, 1064, 1064, 1064, 2008, 6433, 1012, 1998, 2009, 5158, 2138, 2087, 2411, 2111, 2224, 3463, 1997, 5186, 10480, 1998, 9603, 3784, 5852, 2004, 1037, 3978, 1997, 12515, 2028, 1005, 1055, 2828, 1012, 2119, 5107, 1998, 2653, 2840, 1006, 2062, 1012, 1012, 1012, 1064, 1064, 1064, 22376, 22025, 2575, 1064, 1064, 1064, 1045, 2817, 8425, 2640, 2085, 1010, 2029, 1045, 2428, 5959, 1012, 2054, 2003, 5875, 2055, 2023, 2492, 1010, 2003, 2008, 1996, 3754, 2000, 9699, 4784, 1998, 9611, 3471, 2003, 2172, 2062, 2590, 2084, 6664, 1997, 1037, 3563, 1012, 1012, 1012, 1064, 1064, 1064, 4074, 20348, 29159, 2683, 2581, 1011, 14386, 26802, 5339, 1064, 1064, 1064, 23593, 2683, 2683, 2549, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1016, 8950, 2278, 2078, 2620, 2213, 2683, 2213, 2692, 2213, 1064, 1064, 1064, 1045, 2572, 2467, 3201, 2000, 9009, 1006, 2000, 20014, 27605, 13701, 1010, 2000, 2022, 10480, 1007, 2026, 22941, 1005, 1055, 25042, 1012, 1064, 1064, 1064, 2088, 17882, 1029, 5008, 2111, 1999, 1996, 2132, 1029, 2339, 1029, 2821, 1010, 2157, 1010, 20014, 22578, 2467, 2442, 2022, 17253, 2069, 2007, 2122, 2616, 1012, 1045, 2215, 2000, 2265, 2061, 6649, 2026, 4668, 2000, 2023, 1024, 19594, 19317, 2575, 1064, 1064, 1064, 20741, 10790, 2575, 1064, 1064, 1064, 21541, 2361, 1029, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1021, 5603, 4160, 6977, 2595, 2863, 5657, 1005, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\n\nmp.show_list(\n    tokenizer(d['posts'],truncation=True)['input_ids']\n)\n\nList Overview:\nTotal items: 3\n\n1. list[0]\n   - Type: list\n   - Length: 512\n   - Value: [101, 1005, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1053, 2015, 2595, 16257, 8545, 2509, 21638, 2860, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 4601, 1012, 2865, 1012, 10722, 14905, 20974, 1012, 4012, 1013, 10722, 14905, 20974, 1035, 1048, 14876, 26230, 2692, 2509, 9737, 27717, 19062, 2487, 3217, 9541, 2487, 1035, 3156, 1012, 16545, 2290, 1064, 1064, 1064, 4372, 22540, 1998, 20014, 3501, 5312, 16770, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1045, 2480, 2581, 2571, 2487, 2290, 2549, 2595, 2213, 2549, 2998, 13013, 2121, 2025, 2327, 2702, 3248, 16770, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 15384, 20952, 4371, 2487, 12870, 2278, 26418, 2015, 1064, 1064, 1064, 2054, 2038, 2042, 1996, 2087, 2166, 1011, 5278, 3325, 1999, 2115, 2166, 1029, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1058, 2595, 4371, 2100, 2860, 13088, 2094, 2860, 2620, 8299, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1057, 2620, 20518, 3286, 2629, 18927, 2509, 2063, 2006, 9377, 2005, 2087, 1997, 2651, 1012, 1064, 1064, 1064, 2089, 1996, 2566, 2278, 3325, 10047, 16862, 2063, 2017, 1012, 1064, 1064, 1064, 1996, 2197, 2518, 2026, 1999, 2546, 3501, 2767, 6866, 2006, 2010, 9130, 2077, 16873, 5920, 1996, 2279, 2154, 1012, 2717, 1999, 3521, 1066, 8299, 1024, 1013, 1013, 6819, 26247, 1012, 4012, 1013, 22238, 20958, 11387, 2575, 1064, 1064, 1064, 7592, 4372, 2546, 3501, 2581, 1012, 3374, 2000, 2963, 1997, 2115, 12893, 1012, 2009, 1005, 1055, 2069, 3019, 2005, 1037, 3276, 2000, 2025, 2022, 15401, 2035, 1996, 2051, 1999, 2296, 2617, 1997, 4598, 1012, 3046, 2000, 3275, 1996, 2524, 2335, 2004, 2335, 1997, 3930, 1010, 2004, 1012, 1012, 1012, 1064, 1064, 1064, 6391, 22025, 2683, 6391, 23499, 2692, 8299, 1024, 1013, 1013, 2813, 23298, 15194, 3258, 1012, 4012, 1013, 2039, 11066, 1013, 23297, 8889, 1013, 6860, 1011, 2879, 1011, 1998, 1011, 2611, 1011, 2813, 23298, 1012, 16545, 2290, 8299, 1024, 1013, 1013, 7045, 1012, 2079, 19139, 2497, 1012, 4012, 1013, 1059, 2361, 1011, 4180, 1013, 2039, 11066, 2015, 1013, 2230, 1013, 5840, 1013, 2461, 1011, 2188, 1011, 2640, 1012, 16545, 2290, 1012, 1012, 1012, 1064, 1064, 1064, 6160, 1998, 4933, 1012, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 2447, 7971, 10127, 1012, 4012, 1013, 1059, 2361, 1011, 4180, 1013, 2039, 11066, 2015, 1013, 2286, 1013, 5511, 1013, 2417, 1011, 2417, 1011, 1996, 1011, 20421, 1011, 3040, 1011, 19652, 16086, 22610, 2549, 1011, 10332, 1011, 27908, 1012, 16545, 2290, 2208, 1012, 2275, 1012, 2674, 1012, 1064, 1064, 1064, 4013, 4143, 2278, 1010, 2092, 19892, 21823, 2078, 1010, 2012, 2560, 4228, 2781, 1997, 3048, 2115, 3456, 1006, 1998, 1045, 2123, 1005, 1056, 2812, 3048, 2068, 2096, 3564, 1999, 2115, 2168, 4624, 3242, 1007, 1010, 17901, 1999, 5549, 8156, 1006, 2672, 3046, 21006, 2015, 2004, 1037, 2740, 3771, 4522, 1012, 1012, 1012, 1064, 1064, 1064, 10468, 2272, 2039, 2007, 2093, 5167, 2017, 1005, 2310, 4340, 2008, 2169, 2828, 1006, 2030, 29221, 4127, 2017, 2215, 102]\n\n2. list[1]\n   - Type: list\n   - Length: 512\n   - Value: [101, 1005, 1045, 1005, 1049, 4531, 1996, 3768, 1997, 2033, 1999, 2122, 8466, 2200, 8598, 2075, 1012, 1064, 1064, 1064, 3348, 2064, 2022, 11771, 2065, 2009, 1005, 1055, 1999, 1996, 2168, 2597, 2411, 1012, 2005, 2742, 2033, 1998, 2026, 6513, 2024, 2747, 1999, 2019, 4044, 2073, 2057, 2031, 2000, 5541, 2135, 2224, 11190, 15239, 1998, 8696, 1012, 2045, 3475, 1005, 1056, 2438, 1012, 1012, 1012, 1064, 1064, 1064, 3228, 2047, 3574, 2000, 1005, 2208, 1005, 3399, 1012, 1064, 1064, 1064, 7592, 1008, 4372, 25856, 5861, 1008, 2008, 1005, 1055, 2035, 2009, 3138, 1012, 2084, 2057, 23705, 1998, 2027, 2079, 2087, 1997, 1996, 20661, 2096, 1045, 13399, 2037, 3739, 1998, 2709, 2037, 2616, 2007, 5744, 2773, 13068, 1998, 2062, 5048, 2100, 20237, 1012, 1064, 1064, 1064, 2023, 1009, 3768, 1997, 5703, 1998, 2192, 3239, 12016, 1012, 1064, 1064, 1064, 2613, 26264, 3231, 1045, 3556, 13029, 1012, 4274, 26264, 5852, 2024, 6057, 1012, 1045, 3556, 8574, 2015, 2030, 3020, 1012, 2085, 1010, 2066, 1996, 2280, 10960, 1997, 2023, 11689, 1045, 2097, 5254, 2008, 1045, 2123, 1005, 1056, 2903, 1999, 1996, 26264, 3231, 1012, 2077, 2017, 7221, 4509, 1012, 1012, 1012, 1064, 1064, 1064, 2017, 2113, 2017, 1005, 2128, 2019, 4372, 25856, 2043, 2017, 25887, 2013, 1037, 2609, 2005, 1037, 2095, 1998, 1037, 2431, 1010, 2709, 1010, 1998, 2424, 2111, 2024, 2145, 15591, 2006, 2115, 8466, 1998, 16663, 2115, 4784, 1013, 4301, 1012, 2017, 2113, 2017, 1005, 2128, 2019, 4372, 25856, 2043, 2017, 1012, 1012, 1012, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 10047, 2290, 15136, 2620, 1012, 4871, 3270, 3600, 1012, 2149, 1013, 10047, 2290, 15136, 2620, 1013, 4185, 19317, 1013, 3438, 11387, 2094, 2487, 2546, 2683, 2850, 2575, 2683, 22932, 2050, 2575, 2497, 2581, 2487, 19473, 2575, 1012, 16545, 2290, 1064, 1064, 1064, 8299, 1024, 1013, 1013, 10047, 2290, 1012, 4639, 2094, 16872, 28014, 1012, 4012, 1013, 6282, 2509, 2050, 2692, 2278, 2575, 18827, 22025, 16932, 3540, 2497, 2620, 2549, 2278, 22203, 1064, 1064, 1064, 1045, 2058, 2228, 2477, 2823, 1012, 1045, 2175, 2011, 1996, 2214, 20052, 9106, 14686, 1012, 3383, 1010, 2043, 1037, 2158, 2038, 2569, 3716, 1998, 2569, 4204, 2066, 2026, 2219, 1010, 2009, 2738, 16171, 2032, 2000, 6148, 1037, 3375, 1012, 1012, 1012, 1064, 1064, 1064, 13789, 12155, 10270, 1012, 10722, 14905, 20974, 1012, 4012, 2061, 2003, 1045, 1024, 1040, 1064, 1064, 1064, 4278, 1010, 2199, 1009, 2695, 1064, 1064, 1064, 2025, 2428, 1025, 1045, 1005, 2310, 2196, 2245, 1997, 1041, 1013, 1045, 2030, 1046, 1013, 1052, 2004, 2613, 4972, 1012, 1045, 3648, 2870, 2006, 2054, 1045, 2224, 1012, 1045, 2224, 11265, 1998, 14841, 2004, 2026, 29532, 1012, 10768, 2005, 6699, 1998, 6524, 9033, 1012, 1045, 2036, 2224, 9152, 2349, 2000, 2033, 3997, 1012, 1012, 1012, 1064, 1064, 1064, 2017, 2113, 2295, 1012, 2008, 2001, 13749, 18595, 3560, 1012, 2044, 3038, 2009, 1045, 2428, 2215, 2000, 3046, 2009, 1998, 2156, 2054, 6433, 2007, 2033, 2652, 1037, 2034, 2711, 13108, 1999, 1996, 2067, 2096, 2057, 3298, 2105, 1012, 1045, 2215, 2000, 2156, 1996, 2298, 2006, 1012, 1012, 1012, 1064, 1064, 1064, 2041, 1997, 2035, 1997, 2068, 1996, 2600, 3259, 2028, 102]\n\n3. list[2]\n   - Type: list\n   - Length: 512\n   - Value: [101, 1005, 2204, 2028, 1035, 1035, 1035, 1035, 1035, 16770, 1024, 1013, 1013, 7479, 1012, 7858, 1012, 4012, 1013, 3422, 1029, 1058, 1027, 1042, 4048, 18259, 28027, 2546, 2290, 2860, 1064, 1064, 1064, 1997, 2607, 1010, 2000, 2029, 1045, 2360, 1045, 2113, 1025, 2008, 1005, 1055, 2026, 13301, 1998, 2026, 8364, 1012, 1064, 1064, 1064, 2515, 2108, 7078, 3893, 2008, 2017, 1998, 2115, 2190, 2767, 2071, 2022, 2019, 6429, 3232, 4175, 1029, 2065, 2061, 1010, 2084, 2748, 1012, 2030, 2009, 1005, 1055, 2062, 1045, 2071, 2022, 29179, 1999, 2293, 1999, 2553, 1045, 28348, 2026, 5346, 1006, 2029, 2012, 1012, 1012, 1012, 1064, 1064, 1064, 2053, 1010, 1045, 2134, 1005, 1056, 1025, 4067, 2017, 2005, 1037, 4957, 999, 1064, 1064, 1064, 2061, 1011, 2170, 14841, 1011, 9033, 7077, 1006, 1998, 2009, 2064, 7872, 2013, 2151, 2783, 8476, 1013, 17418, 1007, 2064, 2022, 9252, 1012, 2009, 1005, 1055, 2066, 2043, 2017, 1005, 2128, 5881, 1999, 2115, 2219, 4301, 1010, 1998, 2115, 2568, 2074, 17677, 2015, 1999, 7925, 1012, 5683, 5621, 6659, 1012, 1012, 1012, 1012, 1064, 1064, 1064, 2031, 2017, 4384, 2129, 14099, 10072, 2064, 2022, 1029, 2035, 2017, 2031, 2000, 2079, 2003, 2298, 2091, 2012, 1996, 5568, 1024, 9877, 1997, 2367, 3269, 2427, 2045, 1012, 1998, 2085, 5674, 2008, 5606, 1997, 2086, 2101, 1006, 2043, 1013, 2065, 5800, 1012, 1012, 1012, 1064, 1064, 1064, 1996, 3044, 2015, 1516, 2196, 2018, 2053, 2028, 2412, 1064, 1064, 1064, 1045, 2411, 2424, 2870, 27963, 5344, 2006, 7720, 13262, 1013, 3536, 1012, 1064, 1064, 1064, 2023, 1019, 2095, 1011, 2214, 6251, 2003, 2019, 11757, 8321, 1998, 3376, 6412, 1012, 1064, 1064, 1064, 1045, 4033, 1005, 1056, 4716, 2023, 4037, 1999, 1996, 2197, 1017, 2086, 1012, 2061, 9444, 9631, 2023, 1006, 1998, 2672, 2130, 17749, 2033, 1010, 2029, 1045, 3811, 4797, 1007, 1024, 7632, 1012, 6352, 2692, 26224, 6352, 2692, 28311, 1064, 1064, 1064, 2043, 2017, 4133, 1999, 2115, 3871, 2127, 2184, 1024, 2382, 7610, 3015, 2774, 1010, 1998, 6170, 2068, 1006, 2362, 2007, 9877, 1997, 4533, 2015, 1007, 2096, 2652, 2115, 6490, 2858, 1012, 1064, 1064, 1064, 2023, 2003, 1996, 2087, 20014, 2361, 1011, 2003, 2232, 11689, 1045, 1005, 2310, 2412, 2464, 1012, 1064, 1064, 1064, 1045, 2876, 1005, 1056, 2022, 2583, 2000, 2298, 2012, 1996, 4169, 2005, 1996, 2972, 2166, 2065, 1045, 2354, 2008, 1045, 3856, 2009, 2058, 1996, 2529, 2108, 1012, 1064, 1064, 1064, 1045, 2001, 5059, 1037, 4281, 2005, 2026, 7284, 2006, 2029, 1045, 1005, 1049, 2551, 2157, 2085, 1011, 2009, 2323, 2031, 2042, 7733, 1012, 1012, 2021, 1045, 2371, 27885, 14715, 3064, 2000, 2191, 2928, 28194, 5420, 2595, 2683, 2475, 2015, 2695, 11522, 2013, 2009, 1024, 1040, 2065, 2017, 3191, 1996, 2338, 1012, 1012, 1012, 1064, 1064, 1064, 1045, 2318, 2000, 2191, 5888, 2055, 13170, 5146, 1998, 21830, 9610, 7834, 1011, 2182, 2017, 2064, 2156, 2048, 2034, 3441, 1024, 16770, 1024, 1013, 1013, 7479, 1012, 10722, 14905, 20974, 1012, 4012, 1013, 9927, 1013, 1011, 4074, 20348, 29159, 1011, 1064, 1064, 1064, 20014, 3501, 3728, 1045, 2318, 2000, 2695, 2026, 5888, 2055, 2048, 2814, 1011, 13170, 5146, 1998, 21830, 9610, 7834, 1012, 2077, 102]\n\n\n성공\n\nmodel1(torch.tensor(tokenizer(d['posts'],truncation=True)['input_ids']))\n#model1(tokenizer(d['posts'],truncation=True,return_tensors=\"pt\")['input_ids'])\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.0450,  0.1728],\n        [-0.0259,  0.1809],\n        [-0.0611,  0.1994]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n(풀이2) –모델설정변경 (퀴즈5, 모델의 프레임수를 4로 바꾸는 예제에서 사용한 테크닉)\ndistilbert/distilbert-base-uncased 설정값 부르기\n\nconfig = transformers.AutoConfig.from_pretrained(\n    \"distilbert/distilbert-base-uncased\"\n)\nconfig\n\nDistilBertConfig {\n  \"_name_or_path\": \"distilbert/distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.46.2\",\n  \"vocab_size\": 30522\n}\n\n\n설정값변경\n\nconfig.max_position_embeddings = 2200\n\n설정값으로 모델불러오기\n\nmodel1_large = transformers.AutoModelForSequenceClassification.from_config(\n    config=config\n)\n\n모델사용\n\nmodel1_large(torch.tensor(tokenizer(d['posts'],padding=True)['input_ids']))\n\nSequenceClassifierOutput(loss=None, logits=tensor([[ 0.1023,  0.1124],\n        [ 0.1443, -0.0719],\n        [ 0.2653, -0.0368]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n\nmodel1_large(**tokenizer(d['posts'],padding=True,return_tensors=\"pt\"))\n\nSequenceClassifierOutput(loss=None, logits=tensor([[0.1730, 0.0042],\n        [0.2185, 0.0896],\n        [0.3406, 0.0155]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n#\n# 예제4 – sms_spam\n\nsms_spam = datasets.load_dataset('sms_spam')['train'].train_test_split(test_size=0.2, seed=42)\nsms_spam\n\nDatasetDict({\n    train: Dataset({\n        features: ['sms', 'label'],\n        num_rows: 4459\n    })\n    test: Dataset({\n        features: ['sms', 'label'],\n        num_rows: 1115\n    })\n})\n\n\n\nd = sms_spam['train'].select(range(3))\nd\n\nDataset({\n    features: ['sms', 'label'],\n    num_rows: 3\n})\n\n\n(풀이)\n\nmodel1(**tokenizer(d['sms'],padding=True,return_tensors=\"pt\"))\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.0118,  0.1699],\n        [ 0.0273,  0.1815],\n        [-0.0208,  0.2202]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n#"
  },
  {
    "objectID": "posts/09wk-2.html#b.-이미지",
    "href": "posts/09wk-2.html#b.-이미지",
    "title": "09wk-2: model의 입력파악, model의 사용연습",
    "section": "B. 이미지",
    "text": "B. 이미지\n\nmodel2 = transformers.AutoModelForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224-in21k\",\n    num_labels=3 # 그냥 대충 3이라고 했음.. 별 이유는 없음\n)\n\nSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n# 예제1 – food101\n\nd = datasets.load_dataset(\"food101\", split=\"train[:4]\")\nd\n\nDataset({\n    features: ['image', 'label'],\n    num_rows: 4\n})\n\n\n(예비학습) – torchvision.transforms 에서 제공하는 기능들은 배치처리가 가능한가?\n\nto_tensor = torchvision.transforms.ToTensor()\n\n\nto_tensor(d['image'][0])\n\ntensor([[[0.1216, 0.1137, 0.1098,  ..., 0.0039, 0.0039, 0.0000],\n         [0.1255, 0.1216, 0.1176,  ..., 0.0039, 0.0039, 0.0000],\n         [0.1294, 0.1255, 0.1255,  ..., 0.0039, 0.0000, 0.0000],\n         ...,\n         [0.2588, 0.2745, 0.2863,  ..., 0.3765, 0.3882, 0.3922],\n         [0.2353, 0.2471, 0.2667,  ..., 0.3373, 0.3373, 0.3373],\n         [0.2235, 0.2275, 0.2471,  ..., 0.3333, 0.3176, 0.3059]],\n\n        [[0.1373, 0.1294, 0.1255,  ..., 0.1020, 0.1020, 0.0980],\n         [0.1412, 0.1373, 0.1333,  ..., 0.1020, 0.1020, 0.0980],\n         [0.1451, 0.1412, 0.1412,  ..., 0.1020, 0.0980, 0.0980],\n         ...,\n         [0.2471, 0.2627, 0.2745,  ..., 0.3647, 0.3765, 0.3882],\n         [0.2235, 0.2353, 0.2549,  ..., 0.3255, 0.3333, 0.3333],\n         [0.2118, 0.2157, 0.2353,  ..., 0.3216, 0.3137, 0.3020]],\n\n        [[0.1412, 0.1333, 0.1294,  ..., 0.0902, 0.0902, 0.0863],\n         [0.1451, 0.1412, 0.1451,  ..., 0.0902, 0.0902, 0.0863],\n         [0.1490, 0.1451, 0.1529,  ..., 0.0902, 0.0863, 0.0863],\n         ...,\n         [0.1725, 0.1882, 0.2000,  ..., 0.2431, 0.2549, 0.2667],\n         [0.1490, 0.1608, 0.1804,  ..., 0.2039, 0.2118, 0.2118],\n         [0.1373, 0.1412, 0.1608,  ..., 0.2000, 0.1922, 0.1804]]])\n\n\n\nto_tensor(d['image'])\n\nTypeError: pic should be PIL Image or ndarray. Got &lt;class 'list'&gt;\n\n\n(풀이)\n\ncompose = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Resize((224,224))\n])\n\n\ntorch.stack(list(map(compose,d['image'])),axis=0).shape\n\ntorch.Size([4, 3, 224, 224])\n\n\n\nmodel2.forward(\n    torch.stack(list(map(compose,d['image'])),axis=0)\n)\n\nImageClassifierOutput(loss=None, logits=tensor([[ 0.0472,  0.0317, -0.1932],\n        [ 0.2093,  0.1209, -0.0129],\n        [ 0.0555,  0.0957, -0.0811],\n        [-0.0682,  0.0363,  0.0317]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n#\n# 예제2\n\nbeans = datasets.load_dataset('beans')\nd = beans['train'].select(range(4))\nd\n\nDataset({\n    features: ['image_file_path', 'image', 'labels'],\n    num_rows: 4\n})\n\n\n(풀이)\n\nmodel2(torch.stack(list(map(compose,d['image'])),axis=0))\n\nImageClassifierOutput(loss=None, logits=tensor([[ 0.0545, -0.0993, -0.0386],\n        [ 0.0576, -0.1576, -0.0019],\n        [ 0.0982, -0.1531, -0.1147],\n        [ 0.0570, -0.0575, -0.1455]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n#"
  },
  {
    "objectID": "posts/09wk-2.html#c.-동영상",
    "href": "posts/09wk-2.html#c.-동영상",
    "title": "09wk-2: model의 입력파악, model의 사용연습",
    "section": "C. 동영상",
    "text": "C. 동영상\n\nmodel3 = transformers.VideoMAEForVideoClassification.from_pretrained(\n    \"MCG-NJU/videomae-base\",\n)\n\nSome weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n# 예제1 – UCF101_subset\n\nfile_path = huggingface_hub.hf_hub_download(\n    repo_id=\"sayakpaul/ucf101-subset\",\n    filename=\"UCF101_subset.tar.gz\",\n    repo_type=\"dataset\"\n)\n# file_path는 다운로드한 압축파일이 존재하는 경로와 파일명이 string으로 저장되어있음.\nwith tarfile.open(file_path) as t:\n     t.extractall(\"./data\") # 여기에서 \".\"은 현재폴더라는 의미\n\n\nmp.tree(\"./data\")\n\n└── UCF101_subset\n    ├── test\n    │   ├── ApplyEyeMakeup\n    │   │   ├── UCF101\n    │   │   ├── v_ApplyEyeMakeup_g03_c01.avi\n    │   │   └── ...\n    │   │   └── v_ApplyEyeMakeup_g23_c06.avi\n    │   ├── ApplyLipstick\n    │   │   ├── UCF101\n    │   │   ├── v_ApplyLipstick_g14_c01.avi\n    │   │   └── ...\n    │   │   └── v_ApplyLipstick_g16_c04.avi\n    │   └── ...\n    │   └── BenchPress\n    │       ├── UCF101\n    │       ├── v_BenchPress_g05_c02.avi\n    │       └── ...\n    │       └── v_BenchPress_g25_c06.avi\n    ├── train\n    │   ├── ApplyEyeMakeup\n    │   │   ├── UCF101\n    │   │   ├── v_ApplyEyeMakeup_g02_c03.avi\n    │   │   └── ...\n    │   │   └── v_ApplyEyeMakeup_g25_c07.avi\n    │   ├── ApplyLipstick\n    │   │   ├── UCF101\n    │   │   ├── v_ApplyLipstick_g01_c02.avi\n    │   │   └── ...\n    │   │   └── v_ApplyLipstick_g24_c05.avi\n    │   └── ...\n    │   └── BenchPress\n    │       ├── UCF101\n    │       ├── v_BenchPress_g01_c05.avi\n    │       └── ...\n    │       └── v_BenchPress_g24_c05.avi\n    └── val\n        ├── ApplyEyeMakeup\n        │   ├── UCF101\n        │   ├── v_ApplyEyeMakeup_g01_c01.avi\n        │   ├── v_ApplyEyeMakeup_g14_c05.avi\n        │   └── v_ApplyEyeMakeup_g20_c04.avi\n        ├── ApplyLipstick\n        │   ├── UCF101\n        │   ├── v_ApplyLipstick_g10_c04.avi\n        │   ├── v_ApplyLipstick_g20_c04.avi\n        │   └── v_ApplyLipstick_g25_c02.avi\n        └── ...\n        └── BenchPress\n            ├── UCF101\n            ├── v_BenchPress_g11_c05.avi\n            ├── v_BenchPress_g17_c02.avi\n            └── v_BenchPress_g17_c06.avi\n\n\n\nvideo_path = \"./data/UCF101_subset/test/BenchPress/v_BenchPress_g05_c02.avi\"\nvideo = pytorchvideo.data.encoded_video.EncodedVideo.from_path(video_path).get_clip(0, float('inf'))['video']\nvideo.shape\n\ntorch.Size([3, 67, 240, 320])\n\n\n(풀이)\n\nmodel3(\n    #video.permute(1,0,2,3)[:16,:,:224,:224].unsqueeze(0)\n    #video.permute(1,0,2,3)[:16,:,:224,:224].reshape(1,16,3,224,224)\n    torch.stack([video.permute(1,0,2,3)[:16,:,:224,:224]])\n)\n\nImageClassifierOutput(loss=None, logits=tensor([[-0.1375,  0.4270]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)"
  },
  {
    "objectID": "posts/08wk-1.html#a.-위치",
    "href": "posts/08wk-1.html#a.-위치",
    "title": "08wk-1: 모듈설치 및 변경",
    "section": "A. 위치",
    "text": "A. 위치\n\n#!pip install git+https://github.com/guebin/mp2024pkg.git\n\n- numpy가 설치된 위치를 알아보자.\n\n# '/usr/local/lib/python3.10/dist-packages/numpy'\n# 여기에 있음..\n\n- numpy폴더는 있고 mp2024pkg 폴더는 없음..\n- 그래서 numpy는 임포트되는데 mp2024pkg는 임포트되지 않음\n\nimport mp2024pkg\n\nModuleNotFoundError: No module named 'mp2024pkg'"
  },
  {
    "objectID": "posts/08wk-1.html#b.-설치",
    "href": "posts/08wk-1.html#b.-설치",
    "title": "08wk-1: 모듈설치 및 변경",
    "section": "B. 설치",
    "text": "B. 설치\n- mp2024pkg install (=download)\n\n!pip install git+https://github.com/guebin/mp2024pkg.git\n# 이걸 실행하면\n# '/usr/local/lib/python3.10/dist-packages'\n# 여기에서 mp2024pkg라는 폴더가 생김\n\nCollecting git+https://github.com/guebin/mp2024pkg.git\n  Cloning https://github.com/guebin/mp2024pkg.git to /tmp/pip-req-build-ctyecrwe\n  Running command git clone --filter=blob:none --quiet https://github.com/guebin/mp2024pkg.git /tmp/pip-req-build-ctyecrwe\n  Resolved https://github.com/guebin/mp2024pkg.git to commit bd3e63bfe6a50d78c955b42236511e7c90786291\n  Preparing metadata (setup.py) ... done\n\n\n\n#!pip uninstall mp2024pkg -y\n# 이걸 실행하면 위에서 생긴 폴더가 삭제\n\nFound existing installation: mp2024pkg 1.0\nUninstalling mp2024pkg-1.0:\n  Successfully uninstalled mp2024pkg-1.0\n\n\n- 생성된 mp2024pkg 폴더의 내용 – 깃허브에 있는 폴더 그대로 다운로드됨..\n- import\n\nimport mp2024pkg\n\n- 사용\n\nmp2024pkg.show_list(lst)\n\nLevel 1 - Type: list, Length: 4, Content: [[1, 1, 1, 1, 1, 1, 1, 1, ... 'python', (1, 2, [3, 4])]\n     Level 2 - Type: list, Length: 500, Content: [1, 1, 1, 1, 1, 1, 1, 1,  ... , 1, 1, 1, 1, 1, 1, 1, 1]\n     Level 2 - Type: dict, Length: 2, Content: {'a': [2, 2, 2, 2, 2, 2,  ...  2, 2, 2, 2], 'b': '123'}\n     Level 2 - Type: str, Length: 6, Content: 'python'\n     Level 2 - Type: tuple, Length: 3, Content: (1, 2, [3, 4])\n\n\n\nmp2024pkg.show_dict(dct)\n\nDictionary Overview:\nTotal keys: 4\nKeys: ['lst', 'tpl', 'np_array', 'torch']\n\n1. Key: 'lst'\n   - Type: list\n   - Length: 300\n   - Values: [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, ...\n\n2. Key: 'tpl'\n   - Type: tuple\n   - Length: 3\n   - Values: (2, 3, 4)\n\n3. Key: 'np_array'\n   - Type: ndarray\n   - Length: 100\n   - Values: [-7.17493471e-01  6.35787670e-01 -8.11273743e-01  1.49924972e+00\n -5.57618169e-01  7.19155061e-01  2...\n\n4. Key: 'torch'\n   - Type: Tensor\n   - Length: 1\n   - Values: tensor([1])"
  },
  {
    "objectID": "posts/08wk-1.html#c.-변경",
    "href": "posts/08wk-1.html#c.-변경",
    "title": "08wk-1: 모듈설치 및 변경",
    "section": "C. 변경",
    "text": "C. 변경\n- mp2024pkg 폴더로 show_list 함수가 정의된 부분 변경\n\nmp2024pkg.show_list([1,2,[3,4]])\n\n이거 최규빈이 만들었음 2024.11.1\nLevel 1 - Type: list, Length: 3, Content: [1, 2, [3, 4]]\n     Level 2 - Type: int, Content: 1\n     Level 2 - Type: int, Content: 2\n     Level 2 - Type: list, Length: 2, Content: [3, 4]\n\n\n\n\n참고: 촬영당시에는 (2024년11월1일) mp2024pkg에 정의한 함수가 2개였지만 이후에 몇가지 함수가 더 추가되었습니다.\n\n- 커널재시작후 다시 import\n\nimport mp2024pkg\n\n- 변경된 내용 확인\n\nmp2024pkg.show_list([1,2,[3,4]])\n\n이거 최규빈이 만들었음 2024.11.1\nLevel 1 - Type: list, Length: 3, Content: [1, 2, [3, 4]]\n     Level 2 - Type: int, Content: 1\n     Level 2 - Type: int, Content: 2\n     Level 2 - Type: list, Length: 2, Content: [3, 4]"
  },
  {
    "objectID": "posts/08wk-1.html#d.-로드",
    "href": "posts/08wk-1.html#d.-로드",
    "title": "08wk-1: 모듈설치 및 변경",
    "section": "D. 로드",
    "text": "D. 로드\n\n# !rm -rf /usr/local/lib/python3.10/dist-packages/mp2024pkg\n# !rm -rf /usr/local/lib/python3.10/dist-packages/mp2024pkg-1.0.dist-info\n\n\n#!pip install git+https://github.com/guebin/mp2024pkg.git\n\n# 예시1\n\nimport mp2024pkg\n\n\nmp2024pkg.show_dict({'a':[1,2,3],'b':[2,3,4]})\n\nDictionary Overview:\nTotal keys: 2\nKeys: ['a', 'b']\n\n1. Key: 'a'\n   - Type: list\n   - Length: 3\n   - Values: [1, 2, 3]\n\n2. Key: 'b'\n   - Type: list\n   - Length: 3\n   - Values: [2, 3, 4]\n\n\n\n\nmp2024pkg.show_list([[1],[1,2]])\n\nLevel 1 - Type: list, Length: 2, Content: [[1], [1, 2]]\n     Level 2 - Type: list, Length: 1, Content: [1]\n     Level 2 - Type: list, Length: 2, Content: [1, 2]\n\n\n\nshow_list([[1],[1,2]])\n\nNameError: name 'show_list' is not defined\n\n\n#\n# 예시2\n\nfrom mp2024pkg import show_list\n\n\nmp2024pkg.show_list([[1],[1,2]])\n\nNameError: name 'mp2024pkg' is not defined\n\n\n\nshow_list([[1],[1,2]])\n\nLevel 1 - Type: list, Length: 2, Content: [[1], [1, 2]]\n     Level 2 - Type: list, Length: 1, Content: [1]\n     Level 2 - Type: list, Length: 2, Content: [1, 2]\n\n\n#\n# 예시3\n\nimport mp2024pkg as mp\n\n\nmp2024pkg.show_list([[1],[1,2]])\n\nNameError: name 'mp2024pkg' is not defined\n\n\n\nshow_list([[1],[1,2]])\n\nNameError: name 'show_list' is not defined\n\n\n\nmp.show_dict({'a':[1,2,3],'b':[2,3,4]})\n\nDictionary Overview:\nTotal keys: 2\nKeys: ['a', 'b']\n\n1. Key: 'a'\n   - Type: list\n   - Length: 3\n   - Values: [1, 2, 3]\n\n2. Key: 'b'\n   - Type: list\n   - Length: 3\n   - Values: [2, 3, 4]\n\n\n\n\nmp.show_list([[1],[1,2]])\n\nLevel 1 - Type: list, Length: 2, Content: [[1], [1, 2]]\n     Level 2 - Type: list, Length: 1, Content: [1]\n     Level 2 - Type: list, Length: 2, Content: [1, 2]\n\n\n#\n# 예시5\n\nfrom mp2024pkg import *\n\n\nshow_dict({'a':[1,2,3],'b':[2,3,4]})\n\nDictionary Overview:\nTotal keys: 2\nKeys: ['a', 'b']\n\n1. Key: 'a'\n   - Type: list\n   - Length: 3\n   - Values: [1, 2, 3]\n\n2. Key: 'b'\n   - Type: list\n   - Length: 3\n   - Values: [2, 3, 4]\n\n\n\n\nshow_list([[1],[1,2]])\n\nLevel 1 - Type: list, Length: 2, Content: [[1], [1, 2]]\n     Level 2 - Type: list, Length: 1, Content: [1]\n     Level 2 - Type: list, Length: 2, Content: [1, 2]\n\n\n#\n# 예시6\n\nfrom mp2024pkg import show_dict as sd\n\n\nsd({'a':[1,2,3],'b':[2,3,4]})\n\nDictionary Overview:\nTotal keys: 2\nKeys: ['a', 'b']\n\n1. Key: 'a'\n   - Type: list\n   - Length: 3\n   - Values: [1, 2, 3]\n\n2. Key: 'b'\n   - Type: list\n   - Length: 3\n   - Values: [2, 3, 4]\n\n\n\n#\n# 예시7\n\nfrom mp2024pkg import show_dict as sd\nimport mp2024pkg\n\n\nsd({'a':[1,2,3],'b':[2,3,4]})\n\nDictionary Overview:\nTotal keys: 2\nKeys: ['a', 'b']\n\n1. Key: 'a'\n   - Type: list\n   - Length: 3\n   - Values: [1, 2, 3]\n\n2. Key: 'b'\n   - Type: list\n   - Length: 3\n   - Values: [2, 3, 4]\n\n\n\n\nmp2024pkg.show_dict({'a':[1,2,3],'b':[2,3,4]})\n\nDictionary Overview:\nTotal keys: 2\nKeys: ['a', 'b']\n\n1. Key: 'a'\n   - Type: list\n   - Length: 3\n   - Values: [1, 2, 3]\n\n2. Key: 'b'\n   - Type: list\n   - Length: 3\n   - Values: [2, 3, 4]\n\n\n\n\nmp2024pkg.show_list([1,2,[3]])\n\nLevel 1 - Type: list, Length: 3, Content: [1, 2, [3]]\n     Level 2 - Type: int, Content: 1\n     Level 2 - Type: int, Content: 2\n     Level 2 - Type: list, Length: 1, Content: [3]\n\n\n#"
  },
  {
    "objectID": "posts/09wk-1.html#a.-기본사용",
    "href": "posts/09wk-1.html#a.-기본사용",
    "title": "09wk-1: with의 사용",
    "section": "A. 기본사용",
    "text": "A. 기본사용\n- 아래와 같이 with를 사용한다. 여기에서 ???? 자리에 올 수 있는 오브젝트는 __enter__ 와 __exit__을 포함하는 어떠한 오브젝트이다. (그리고 이러한 오브젝트를 “컨텍스트 매니저”라고 부른다)\nwith ????:\n    블라블라~\n    야디야디~\n# 예제1 – 기본예제\n- context manager 를 찍어내는 Dummy 클래스\n\nclass Dummy: \n    def __enter__(self):\n        print(\"enter\")\n    def __exit__(self,*args): # *args는 에러처리 관련된 __exit__의 입력변수들 \n        print(\"exit\") \n\n\nd = Dummy()\nd.__enter__()\nprint(\"context\")\nd.__exit__()\n\nenter\ncontext\nexit\n\n\n\nd = Dummy()\nwith d:\n    print(\"context\")\n\nenter\ncontext\nexit\n\n\n- with 뒤에 올 수 있는 오브젝트는 __enter__ 와 __exit__ 을 포함해야한다.\n\nlst = [1,2,3]\nwith lst:\n    pass \n\nTypeError: 'list' object does not support the context manager protocol\n\n\n\nwith 33:\n    pass \n\nTypeError: 'int' object does not support the context manager protocol\n\n\n# 예제2 – 타이머\n원래 사용패턴\n\nt1 = time.time()\nnp.random.randn(100,100) + 30\nt2 = time.time()\nt2-t1\n\n0.0005712509155273438\n\n\n\nt1 = time.time()\nnp.random.randn(10000,10000) + 30\nt2 = time.time()\nt2-t1\n\n1.5068929195404053\n\n\n\nclass Timer:\n    def __enter__(self):\n        self.t1 = time.time()\n    def __exit__(self,*args):\n        self.t2 = time.time()\n        print(f\"{self.t2 - self.t1:.4f} 초 걸림\")\n\n\nwith Timer():\n    np.random.randn(10000,10000) + 30\n\n1.4989 초 걸림\n\n\n#\n# 예제3\n\nclass 미분꼬리표추적금지:\n    def __enter__(self):\n        torch.set_grad_enabled(False)\n    def __exit__(self,*args):\n        torch.set_grad_enabled(True)\n\n\na = torch.tensor(1.0,requires_grad = True)\na\n\ntensor(1., requires_grad=True)\n\n\n\nb = a+1 \nc = 2*b \nd = c/3\nb,c,d\n\n(tensor(2., grad_fn=&lt;AddBackward0&gt;),\n tensor(4., grad_fn=&lt;MulBackward0&gt;),\n tensor(1.3333, grad_fn=&lt;DivBackward0&gt;))\n\n\n\nwith 미분꼬리표추적금지():\n    b = a+1 \n    c = 2*b \n    d = c/3\nb,c,d\n\n(tensor(2.), tensor(4.), tensor(1.3333))\n\n\n#\n# 예제4\n아래의 코드를 모델을 활용하여 로짓을 계산하라.\n\nmodel = transformers.AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert/distilbert-base-uncased\", num_labels=2\n)\nmodel_input = {\n    'input_ids': torch.tensor([[101, 2023, 3185, 2003, 6659, 2021, 2009, 2038, 2070, 2204, 3896, 1012, 102]]),\n    'attention_mask': torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n    'labels': torch.tensor([0])\n}\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nlogits = model(**model_input).logits\nlogits\n\ntensor([[0.0505, 0.1006]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n(풀이)\n\nwith 미분꼬리표추적금지():\n    logits = model(**model_input).logits\n    print(logits)\n\ntensor([[0.0505, 0.1006]])\n\n\n(풀이2)\n\nwith torch.no_grad():\n    logits = model(**model_input).logits\n    print(logits)\n\ntensor([[0.0505, 0.1006]])\n\n\nref: https://huggingface.co/docs/transformers/tasks/sequence_classification\n#"
  },
  {
    "objectID": "posts/09wk-1.html#b.-약간고급사용",
    "href": "posts/09wk-1.html#b.-약간고급사용",
    "title": "09wk-1: with의 사용",
    "section": "B. 약간고급사용",
    "text": "B. 약간고급사용\n# 예비학습\n- 기본플랏\n\nplt.plot([1,2,4,3],'--o')\n\n\n\n\n\n\n\n\n- 크기를 (10,3)으로 조정\n\nplt.rcParams['figure.figsize'] = [10, 3] \nplt.plot([1,2,4,3],'--o')\n\n\n\n\n\n\n\n\n- 한번 뒤틀린 설정이 그대로 있음\n\nplt.plot([4,3,2,1],'--o')\n\n\n\n\n\n\n\n\n- 설정을 원래대로\n\nplt.rcdefaults()\n\n\nplt.plot([4,3,2,1],'--o')\n\n\n\n\n\n\n\n\n#\n# 예제4 – 크기조정\n\nclass FigureSizeContext:\n    def __enter__(self):\n        def resize(w,h):\n            plt.rcParams['figure.figsize'] = [w, h] \n        return resize\n    def __exit__(self,*args):\n        plt.rcdefaults()\n\n- 사용예시1\n\ncontext_manager = FigureSizeContext()\nxxxx = context_manager.__enter__()\nxxxx(3,3)\nplt.plot([1,2,4,3],'--o')\ncontext_manager.__exit__()\n\n\n\n\n\n\n\n\n\nplt.plot([1,2,4,3],'--o')\n\n\n\n\n\n\n\n\n- 사용예시2\n\nwith FigureSizeContext() as xxxx:\n    xxxx(3,3)\n    plt.plot([1,2,4,3],'--o')\n\n\n\n\n\n\n\n\n\nplt.plot([1,2,4,3],'--o')\n\n\n\n\n\n\n\n\n- 정리하면.. with는 아래와 같이 as와 함께 사용가능한데,\nwith context_manager as xxxx: \n    블라블라~\n    야디야디~\n여기에서 as뒤의 xxxx 자리는 context_manager.__enter__의 리턴값이 차지하게 된다.\n#\n# 예제5 – 샌드위치생성기\n\nclass 샌드위치생성기:\n    def __init__(self,빵=\"기본빵\"):\n        self.빵 = 빵\n        self.재료들 = []\n    def __enter__(self):\n        print(f\"---{self.빵}---\")\n        return self\n    def __exit__(self,*args):\n        print(f\"---{self.빵}---\")\n    def 재료추가하기(self,재료):\n        print(재료)\n        self.재료들.append(재료)\n\n- 사용예시1\n\nwith 샌드위치생성기(빵=\"허니오트\") as 샌드위치:\n    샌드위치.재료추가하기(\"양상추\")\n    샌드위치.재료추가하기(\"치즈\")\n    샌드위치.재료추가하기(\"베이컨\")\n    샌드위치.재료추가하기(\"오이\")\n\n---허니오트---\n양상추\n치즈\n베이컨\n오이\n---허니오트---\n\n\n\n샌드위치.빵, 샌드위치.재료들\n\n('허니오트', ['양상추', '치즈', '베이컨', '오이'])\n\n\n- 사용예시2\n\nwith 샌드위치생성기(빵=\"허니오트\") as 샌드위치:\n    샌드위치.재료추가하기(\"토마토\")\n    샌드위치.재료추가하기(\"치즈\")\n    샌드위치.재료추가하기(\"베이컨\")\n    샌드위치.재료추가하기(\"에그마요\")\n\n---허니오트---\n토마토\n치즈\n베이컨\n에그마요\n---허니오트---\n\n\n\n샌드위치.빵, 샌드위치.재료들\n\n('허니오트', ['토마토', '치즈', '베이컨', '에그마요'])\n\n\n- 아래의 두 코드가 같은 효과를 가진다.\nwith 샌드위치생성기(빵=\"허니오트\") as 샌드위치:\n    샌드위치.재료추가하기(\"토마토\")\n    샌드위치.재료추가하기(\"치즈\")\n    샌드위치.재료추가하기(\"베이컨\")\n    샌드위치.재료추가하기(\"에그마요\")\n샌드위치 = 샌드위치생성기(빵=\"허니오트\")\n샌드위치 = 샌드위치.__enter__()\n샌드위치.재료추가하기(\"토마토\")\n샌드위치.재료추가하기(\"치즈\")\n샌드위치.재료추가하기(\"베이컨\")\n샌드위치.재료추가하기(\"에그마요\")\n샌드위치.__exit__()\n- 따라서 __enter__ 가 self를 리턴하는 경우에는 with 샌드위치생성기(빵=\"허니오트\") as 샌드위치 이 부분을 해석할때 “샌드위치생성기(빵=\"허니오트\") 코드의 실행결과 만들어지는 오브젝트를 샌드위치로 저장”한다고 해석해도 무리가 없다.\n#\n# 예제6\n- example.txt 를 만들고 “asdf” 라는 글자를 넣는 파이썬코드\n\nwith open(\"example.txt\",\"w\") as file: \n    # 대충해석: open(\"example.txt\",\"w\") 을 실행하여 나오는 오브젝트를 file로 받음 \n    # 더 엄밀한 해석: \n    # open(\"example.txt\",\"w\") 을 실행하여 나오는 오브젝트를 ???라고 하자. \n    # 그런데 ??? 에는 `__enter__`가 있을텐데, 그 `__enter__`를 실행하여 나오는 오브젝트를 \n    # file로 받음.\n    file.write(\"asdf\")    \n\n- 분석하기: “대충해석”으로 해석해도 되는지 체크\n\nfile\n\n&lt;_io.TextIOWrapper name='example.txt' mode='w' encoding='UTF-8'&gt;\n\n\n\ncontext_manager = open(\"example.txt\",\"w\")\nfile = context_manager.__enter__()\n\n\ntype(context_manager), type(file)\n\n(_io.TextIOWrapper, _io.TextIOWrapper)\n\n\n- 분석하기2: __enter__ 가 self를 리턴하는지 보자\n\ncontext_manager.__enter__??\n# 코드를 볼 수 없음... -- 아쉬움..\n\n\nDocstring: &lt;no docstring&gt;\nType:      builtin_function_or_method\n\n\n\n#\n# 예제7\n- example.txt 파일을 example.tar.gz로 압축하는 코드\n\nwith tarfile.open(\"example.tar.gz\",\"w:gz\") as tar:\n    tar.add(\"example.txt\", arcname=\"example.txt\")\n\n- 분석하기\n\ncontext_manager = tarfile.open(\"example.tar.gz\",\"w:gz\")\ntar = context_manager.__enter__()\ntype(context_manager), type(tar)\n\n(tarfile.TarFile, tarfile.TarFile)\n\n\n\ncontext_manager.__enter__() 는 self를 리턴하는듯함\n\n- 분석하기2: __enter__ 가 self를 리턴하는지 보자\n\ntar.__enter__??\n\n\nSignature: tar.__enter__()\nDocstring: &lt;no docstring&gt;\nSource:   \n    def __enter__(self):\n        self._check()\n        return self\nFile:      ~/anaconda3/envs/hf/lib/python3.12/tarfile.py\nType:      method\n\n\n\n#"
  },
  {
    "objectID": "posts/02wk-1.html#a.-step1-데이터",
    "href": "posts/02wk-1.html#a.-step1-데이터",
    "title": "02wk-1: IMDB 영화평 감성분석",
    "section": "A. Step1 – 데이터",
    "text": "A. Step1 – 데이터\n- 데이터불러오기\n\nfrom datasets import load_dataset\nimdb = load_dataset(\"imdb\")\n\n/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n\n/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n\n\n뭐하는거?\n\ntokenizer(\"Transformers are really useful for natural language processing.\") # 텍스트 -&gt; 숫자들 \n\n{'input_ids': [101, 19081, 2024, 2428, 6179, 2005, 3019, 2653, 6364, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n- preprocess_function 를 선언\n\ndef preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True)\n\n뭐하는 함수?\n\ntokenizer(\"Transformers are really useful for natural language processing.\",truncation=True) # 텍스트 -&gt; 숫자들 \n\n{'input_ids': [101, 19081, 2024, 2428, 6179, 2005, 3019, 2653, 6364, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\nexamples = imdb['train'][1]\n#tokenizer(examples[\"text\"], truncation=True)\npreprocess_function(examples)\n\n{'input_ids': [101, 1000, 1045, 2572, 8025, 1024, 3756, 1000, 2003, 1037, 15544, 19307, 1998, 3653, 6528, 20771, 19986, 8632, 1012, 2009, 2987, 1005, 1056, 3043, 2054, 2028, 1005, 1055, 2576, 5328, 2024, 2138, 2023, 2143, 2064, 6684, 2022, 2579, 5667, 2006, 2151, 2504, 1012, 2004, 2005, 1996, 4366, 2008, 19124, 3287, 16371, 25469, 2003, 2019, 6882, 13316, 1011, 2459, 1010, 2008, 3475, 1005, 1056, 2995, 1012, 1045, 1005, 2310, 2464, 1054, 1011, 6758, 3152, 2007, 3287, 16371, 25469, 1012, 4379, 1010, 2027, 2069, 3749, 2070, 25085, 5328, 1010, 2021, 2073, 2024, 1996, 1054, 1011, 6758, 3152, 2007, 21226, 24728, 22144, 2015, 1998, 20916, 4691, 6845, 2401, 1029, 7880, 1010, 2138, 2027, 2123, 1005, 1056, 4839, 1012, 1996, 2168, 3632, 2005, 2216, 10231, 7685, 5830, 3065, 1024, 8040, 7317, 5063, 2015, 11820, 1999, 1996, 9478, 2021, 2025, 1037, 17962, 21239, 1999, 4356, 1012, 1998, 2216, 3653, 6528, 20771, 10271, 5691, 2066, 1996, 2829, 16291, 1010, 1999, 2029, 2057, 1005, 2128, 5845, 2000, 1996, 2609, 1997, 6320, 25624, 1005, 1055, 17061, 3779, 1010, 2021, 2025, 1037, 7637, 1997, 5061, 5710, 2006, 9318, 7367, 5737, 19393, 1012, 2077, 6933, 1006, 2030, 20242, 1007, 1000, 3313, 1011, 3115, 1000, 1999, 5609, 1997, 16371, 25469, 1010, 1996, 10597, 27885, 5809, 2063, 2323, 2202, 2046, 4070, 2028, 14477, 6767, 8524, 6321, 5793, 28141, 4489, 2090, 2273, 1998, 2308, 1024, 2045, 2024, 2053, 8991, 18400, 2015, 2006, 4653, 2043, 19910, 3544, 15287, 1010, 1998, 1996, 2168, 3685, 2022, 2056, 2005, 1037, 2158, 1012, 1999, 2755, 1010, 2017, 3227, 2180, 1005, 1056, 2156, 2931, 8991, 18400, 2015, 1999, 2019, 2137, 2143, 1999, 2505, 2460, 1997, 22555, 2030, 13216, 14253, 2050, 1012, 2023, 6884, 3313, 1011, 3115, 2003, 2625, 1037, 3313, 3115, 2084, 2019, 4914, 2135, 2139, 24128, 3754, 2000, 2272, 2000, 3408, 20547, 2007, 1996, 19008, 1997, 2308, 1005, 1055, 4230, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\nexamples = imdb['train'][:2]\n#tokenizer(examples[\"text\"], truncation=True)\npreprocess_function(examples)\n\n{'input_ids': [[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102], [101, 1000, 1045, 2572, 8025, 1024, 3756, 1000, 2003, 1037, 15544, 19307, 1998, 3653, 6528, 20771, 19986, 8632, 1012, 2009, 2987, 1005, 1056, 3043, 2054, 2028, 1005, 1055, 2576, 5328, 2024, 2138, 2023, 2143, 2064, 6684, 2022, 2579, 5667, 2006, 2151, 2504, 1012, 2004, 2005, 1996, 4366, 2008, 19124, 3287, 16371, 25469, 2003, 2019, 6882, 13316, 1011, 2459, 1010, 2008, 3475, 1005, 1056, 2995, 1012, 1045, 1005, 2310, 2464, 1054, 1011, 6758, 3152, 2007, 3287, 16371, 25469, 1012, 4379, 1010, 2027, 2069, 3749, 2070, 25085, 5328, 1010, 2021, 2073, 2024, 1996, 1054, 1011, 6758, 3152, 2007, 21226, 24728, 22144, 2015, 1998, 20916, 4691, 6845, 2401, 1029, 7880, 1010, 2138, 2027, 2123, 1005, 1056, 4839, 1012, 1996, 2168, 3632, 2005, 2216, 10231, 7685, 5830, 3065, 1024, 8040, 7317, 5063, 2015, 11820, 1999, 1996, 9478, 2021, 2025, 1037, 17962, 21239, 1999, 4356, 1012, 1998, 2216, 3653, 6528, 20771, 10271, 5691, 2066, 1996, 2829, 16291, 1010, 1999, 2029, 2057, 1005, 2128, 5845, 2000, 1996, 2609, 1997, 6320, 25624, 1005, 1055, 17061, 3779, 1010, 2021, 2025, 1037, 7637, 1997, 5061, 5710, 2006, 9318, 7367, 5737, 19393, 1012, 2077, 6933, 1006, 2030, 20242, 1007, 1000, 3313, 1011, 3115, 1000, 1999, 5609, 1997, 16371, 25469, 1010, 1996, 10597, 27885, 5809, 2063, 2323, 2202, 2046, 4070, 2028, 14477, 6767, 8524, 6321, 5793, 28141, 4489, 2090, 2273, 1998, 2308, 1024, 2045, 2024, 2053, 8991, 18400, 2015, 2006, 4653, 2043, 19910, 3544, 15287, 1010, 1998, 1996, 2168, 3685, 2022, 2056, 2005, 1037, 2158, 1012, 1999, 2755, 1010, 2017, 3227, 2180, 1005, 1056, 2156, 2931, 8991, 18400, 2015, 1999, 2019, 2137, 2143, 1999, 2505, 2460, 1997, 22555, 2030, 13216, 14253, 2050, 1012, 2023, 6884, 3313, 1011, 3115, 2003, 2625, 1037, 3313, 3115, 2084, 2019, 4914, 2135, 2139, 24128, 3754, 2000, 2272, 2000, 3408, 20547, 2007, 1996, 19008, 1997, 2308, 1005, 1055, 4230, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n\n\n- map 메소드 사용\n\ntokenized_imdb = imdb.map(preprocess_function, batched=True)\n\n\n#imdb['train'][0], tokenized_imdb['train'][0]\n\n\nfor i in range(5):\n    print(preprocess_function(imdb['train'][i]))\n\n{'input_ids': [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n{'input_ids': [101, 1000, 1045, 2572, 8025, 1024, 3756, 1000, 2003, 1037, 15544, 19307, 1998, 3653, 6528, 20771, 19986, 8632, 1012, 2009, 2987, 1005, 1056, 3043, 2054, 2028, 1005, 1055, 2576, 5328, 2024, 2138, 2023, 2143, 2064, 6684, 2022, 2579, 5667, 2006, 2151, 2504, 1012, 2004, 2005, 1996, 4366, 2008, 19124, 3287, 16371, 25469, 2003, 2019, 6882, 13316, 1011, 2459, 1010, 2008, 3475, 1005, 1056, 2995, 1012, 1045, 1005, 2310, 2464, 1054, 1011, 6758, 3152, 2007, 3287, 16371, 25469, 1012, 4379, 1010, 2027, 2069, 3749, 2070, 25085, 5328, 1010, 2021, 2073, 2024, 1996, 1054, 1011, 6758, 3152, 2007, 21226, 24728, 22144, 2015, 1998, 20916, 4691, 6845, 2401, 1029, 7880, 1010, 2138, 2027, 2123, 1005, 1056, 4839, 1012, 1996, 2168, 3632, 2005, 2216, 10231, 7685, 5830, 3065, 1024, 8040, 7317, 5063, 2015, 11820, 1999, 1996, 9478, 2021, 2025, 1037, 17962, 21239, 1999, 4356, 1012, 1998, 2216, 3653, 6528, 20771, 10271, 5691, 2066, 1996, 2829, 16291, 1010, 1999, 2029, 2057, 1005, 2128, 5845, 2000, 1996, 2609, 1997, 6320, 25624, 1005, 1055, 17061, 3779, 1010, 2021, 2025, 1037, 7637, 1997, 5061, 5710, 2006, 9318, 7367, 5737, 19393, 1012, 2077, 6933, 1006, 2030, 20242, 1007, 1000, 3313, 1011, 3115, 1000, 1999, 5609, 1997, 16371, 25469, 1010, 1996, 10597, 27885, 5809, 2063, 2323, 2202, 2046, 4070, 2028, 14477, 6767, 8524, 6321, 5793, 28141, 4489, 2090, 2273, 1998, 2308, 1024, 2045, 2024, 2053, 8991, 18400, 2015, 2006, 4653, 2043, 19910, 3544, 15287, 1010, 1998, 1996, 2168, 3685, 2022, 2056, 2005, 1037, 2158, 1012, 1999, 2755, 1010, 2017, 3227, 2180, 1005, 1056, 2156, 2931, 8991, 18400, 2015, 1999, 2019, 2137, 2143, 1999, 2505, 2460, 1997, 22555, 2030, 13216, 14253, 2050, 1012, 2023, 6884, 3313, 1011, 3115, 2003, 2625, 1037, 3313, 3115, 2084, 2019, 4914, 2135, 2139, 24128, 3754, 2000, 2272, 2000, 3408, 20547, 2007, 1996, 19008, 1997, 2308, 1005, 1055, 4230, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n{'input_ids': [101, 2065, 2069, 2000, 4468, 2437, 2023, 2828, 1997, 2143, 1999, 1996, 2925, 1012, 2023, 2143, 2003, 5875, 2004, 2019, 7551, 2021, 4136, 2053, 2522, 11461, 2466, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2514, 6819, 5339, 8918, 2005, 3564, 27046, 2009, 2138, 2009, 12817, 2006, 2061, 2116, 2590, 3314, 2021, 2009, 2515, 2061, 2302, 2151, 5860, 11795, 3085, 15793, 1012, 1996, 13972, 3310, 2185, 2007, 2053, 2047, 15251, 1006, 4983, 2028, 3310, 2039, 2007, 2028, 2096, 2028, 1005, 1055, 2568, 17677, 2015, 1010, 2004, 2009, 2097, 26597, 2079, 2076, 2023, 23100, 2143, 1007, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2488, 5247, 2028, 1005, 1055, 2051, 4582, 2041, 1037, 3332, 2012, 1037, 3392, 3652, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n{'input_ids': [101, 2023, 2143, 2001, 2763, 4427, 2011, 2643, 4232, 1005, 1055, 16137, 10841, 4115, 1010, 10768, 25300, 2078, 1998, 1045, 9075, 2017, 2000, 2156, 2008, 2143, 2612, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2143, 2038, 2048, 2844, 3787, 1998, 2216, 2024, 1010, 1006, 1015, 1007, 1996, 12689, 3772, 1006, 1016, 1007, 1996, 8052, 1010, 6151, 6810, 2099, 7178, 2135, 2204, 1010, 6302, 1012, 4237, 2013, 2008, 1010, 2054, 9326, 2033, 2087, 2003, 1996, 10866, 5460, 1997, 9033, 21202, 7971, 1012, 14229, 6396, 2386, 2038, 2000, 2022, 2087, 15703, 3883, 1999, 1996, 2088, 1012, 2016, 4490, 2061, 5236, 1998, 2007, 2035, 1996, 16371, 25469, 1999, 2023, 2143, 1010, 1012, 1012, 1012, 2009, 1005, 1055, 14477, 4779, 26884, 1012, 13599, 2000, 2643, 4232, 1005, 1055, 2143, 1010, 7789, 3012, 2038, 2042, 2999, 2007, 28072, 1012, 2302, 2183, 2205, 2521, 2006, 2023, 3395, 1010, 1045, 2052, 2360, 2008, 4076, 2013, 1996, 4489, 1999, 15084, 2090, 1996, 2413, 1998, 1996, 4467, 2554, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1037, 3185, 1997, 2049, 2051, 1010, 1998, 2173, 1012, 1016, 1013, 2184, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n{'input_ids': [101, 2821, 1010, 2567, 1012, 1012, 1012, 2044, 4994, 2055, 2023, 9951, 2143, 2005, 8529, 13876, 12129, 2086, 2035, 1045, 2064, 2228, 1997, 2003, 2008, 2214, 14911, 3389, 2299, 1012, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1000, 2003, 2008, 2035, 2045, 2003, 1029, 1029, 1000, 1012, 1012, 1012, 1045, 2001, 2074, 2019, 2220, 9458, 2043, 2023, 20482, 3869, 2718, 1996, 1057, 1012, 1055, 1012, 1045, 2001, 2205, 2402, 2000, 2131, 1999, 1996, 4258, 1006, 2348, 1045, 2106, 6133, 2000, 13583, 2046, 1000, 9119, 8912, 1000, 1007, 1012, 2059, 1037, 11326, 2012, 1037, 2334, 2143, 2688, 10272, 17799, 1011, 2633, 1045, 2071, 2156, 2023, 2143, 1010, 3272, 2085, 1045, 2001, 2004, 2214, 2004, 2026, 3008, 2020, 2043, 2027, 8040, 7317, 13699, 5669, 2000, 2156, 2009, 999, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2069, 3114, 2023, 2143, 2001, 2025, 10033, 2000, 1996, 10812, 13457, 1997, 2051, 2001, 2138, 1997, 1996, 27885, 11020, 20693, 2553, 13977, 2011, 2049, 1057, 1012, 1055, 1012, 2713, 1012, 8817, 1997, 2111, 19311, 2098, 2000, 2023, 27136, 2121, 1010, 3241, 2027, 2020, 2183, 2000, 2156, 1037, 3348, 2143, 1012, 1012, 1012, 2612, 1010, 2027, 2288, 7167, 1997, 2485, 22264, 1997, 1043, 11802, 2135, 1010, 16360, 23004, 25430, 18352, 1010, 2006, 1011, 2395, 7636, 1999, 20857, 6023, 25943, 1010, 2004, 5498, 2063, 2576, 3653, 29048, 1012, 1012, 1012, 1998, 7408, 3468, 2040, 1011, 14977, 23599, 3348, 5019, 2007, 7842, 22772, 1010, 5122, 5889, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 3451, 12696, 1010, 4151, 24665, 12502, 1010, 3181, 20785, 1012, 1012, 3649, 2023, 2518, 2001, 1010, 14021, 5596, 2009, 1010, 6402, 2009, 1010, 2059, 4933, 1996, 11289, 1999, 1037, 2599, 3482, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 7069, 9765, 27065, 2229, 2145, 26988, 2000, 2424, 3643, 1999, 2049, 11771, 18404, 6208, 2576, 11867, 7974, 8613, 1012, 1012, 2021, 2065, 2009, 4694, 1005, 1056, 2005, 1996, 15657, 9446, 1010, 2009, 2052, 2031, 2042, 6439, 1010, 2059, 6404, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2612, 1010, 1996, 1000, 1045, 2572, 8744, 1010, 8744, 1000, 1054, 10536, 16921, 7583, 2516, 2001, 5567, 10866, 2135, 2005, 2086, 2004, 1037, 14841, 26065, 3508, 2005, 22555, 2080, 3152, 1006, 1045, 2572, 8025, 1010, 20920, 1011, 2005, 5637, 3152, 1010, 1045, 2572, 8025, 1010, 2304, 1011, 2005, 1038, 2721, 2595, 24759, 28100, 3370, 3152, 1010, 4385, 1012, 1012, 1007, 1998, 2296, 2702, 2086, 2030, 2061, 1996, 2518, 9466, 2013, 1996, 2757, 1010, 2000, 2022, 7021, 2011, 1037, 2047, 4245, 1997, 26476, 2015, 2040, 2215, 2000, 2156, 2008, 1000, 20355, 3348, 2143, 1000, 2008, 1000, 4329, 3550, 1996, 2143, 3068, 1000, 1012, 1012, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 6300, 9953, 1010, 4468, 2066, 1996, 11629, 1012, 1012, 2030, 2065, 2017, 2442, 2156, 2009, 1011, 9278, 1996, 2678, 1998, 3435, 2830, 2000, 1996, 1000, 6530, 1000, 3033, 1010, 2074, 2000, 2131, 2009, 2058, 2007, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\nprint(tokenized_imdb['train'][0]['input_ids'])\nprint(tokenized_imdb['train'][1]['input_ids'])\nprint(tokenized_imdb['train'][2]['input_ids'])\nprint(tokenized_imdb['train'][3]['input_ids'])\nprint(tokenized_imdb['train'][4]['input_ids'])\n\n[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102]\n[101, 1000, 1045, 2572, 8025, 1024, 3756, 1000, 2003, 1037, 15544, 19307, 1998, 3653, 6528, 20771, 19986, 8632, 1012, 2009, 2987, 1005, 1056, 3043, 2054, 2028, 1005, 1055, 2576, 5328, 2024, 2138, 2023, 2143, 2064, 6684, 2022, 2579, 5667, 2006, 2151, 2504, 1012, 2004, 2005, 1996, 4366, 2008, 19124, 3287, 16371, 25469, 2003, 2019, 6882, 13316, 1011, 2459, 1010, 2008, 3475, 1005, 1056, 2995, 1012, 1045, 1005, 2310, 2464, 1054, 1011, 6758, 3152, 2007, 3287, 16371, 25469, 1012, 4379, 1010, 2027, 2069, 3749, 2070, 25085, 5328, 1010, 2021, 2073, 2024, 1996, 1054, 1011, 6758, 3152, 2007, 21226, 24728, 22144, 2015, 1998, 20916, 4691, 6845, 2401, 1029, 7880, 1010, 2138, 2027, 2123, 1005, 1056, 4839, 1012, 1996, 2168, 3632, 2005, 2216, 10231, 7685, 5830, 3065, 1024, 8040, 7317, 5063, 2015, 11820, 1999, 1996, 9478, 2021, 2025, 1037, 17962, 21239, 1999, 4356, 1012, 1998, 2216, 3653, 6528, 20771, 10271, 5691, 2066, 1996, 2829, 16291, 1010, 1999, 2029, 2057, 1005, 2128, 5845, 2000, 1996, 2609, 1997, 6320, 25624, 1005, 1055, 17061, 3779, 1010, 2021, 2025, 1037, 7637, 1997, 5061, 5710, 2006, 9318, 7367, 5737, 19393, 1012, 2077, 6933, 1006, 2030, 20242, 1007, 1000, 3313, 1011, 3115, 1000, 1999, 5609, 1997, 16371, 25469, 1010, 1996, 10597, 27885, 5809, 2063, 2323, 2202, 2046, 4070, 2028, 14477, 6767, 8524, 6321, 5793, 28141, 4489, 2090, 2273, 1998, 2308, 1024, 2045, 2024, 2053, 8991, 18400, 2015, 2006, 4653, 2043, 19910, 3544, 15287, 1010, 1998, 1996, 2168, 3685, 2022, 2056, 2005, 1037, 2158, 1012, 1999, 2755, 1010, 2017, 3227, 2180, 1005, 1056, 2156, 2931, 8991, 18400, 2015, 1999, 2019, 2137, 2143, 1999, 2505, 2460, 1997, 22555, 2030, 13216, 14253, 2050, 1012, 2023, 6884, 3313, 1011, 3115, 2003, 2625, 1037, 3313, 3115, 2084, 2019, 4914, 2135, 2139, 24128, 3754, 2000, 2272, 2000, 3408, 20547, 2007, 1996, 19008, 1997, 2308, 1005, 1055, 4230, 1012, 102]\n[101, 2065, 2069, 2000, 4468, 2437, 2023, 2828, 1997, 2143, 1999, 1996, 2925, 1012, 2023, 2143, 2003, 5875, 2004, 2019, 7551, 2021, 4136, 2053, 2522, 11461, 2466, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2514, 6819, 5339, 8918, 2005, 3564, 27046, 2009, 2138, 2009, 12817, 2006, 2061, 2116, 2590, 3314, 2021, 2009, 2515, 2061, 2302, 2151, 5860, 11795, 3085, 15793, 1012, 1996, 13972, 3310, 2185, 2007, 2053, 2047, 15251, 1006, 4983, 2028, 3310, 2039, 2007, 2028, 2096, 2028, 1005, 1055, 2568, 17677, 2015, 1010, 2004, 2009, 2097, 26597, 2079, 2076, 2023, 23100, 2143, 1007, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2488, 5247, 2028, 1005, 1055, 2051, 4582, 2041, 1037, 3332, 2012, 1037, 3392, 3652, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 102]\n[101, 2023, 2143, 2001, 2763, 4427, 2011, 2643, 4232, 1005, 1055, 16137, 10841, 4115, 1010, 10768, 25300, 2078, 1998, 1045, 9075, 2017, 2000, 2156, 2008, 2143, 2612, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2143, 2038, 2048, 2844, 3787, 1998, 2216, 2024, 1010, 1006, 1015, 1007, 1996, 12689, 3772, 1006, 1016, 1007, 1996, 8052, 1010, 6151, 6810, 2099, 7178, 2135, 2204, 1010, 6302, 1012, 4237, 2013, 2008, 1010, 2054, 9326, 2033, 2087, 2003, 1996, 10866, 5460, 1997, 9033, 21202, 7971, 1012, 14229, 6396, 2386, 2038, 2000, 2022, 2087, 15703, 3883, 1999, 1996, 2088, 1012, 2016, 4490, 2061, 5236, 1998, 2007, 2035, 1996, 16371, 25469, 1999, 2023, 2143, 1010, 1012, 1012, 1012, 2009, 1005, 1055, 14477, 4779, 26884, 1012, 13599, 2000, 2643, 4232, 1005, 1055, 2143, 1010, 7789, 3012, 2038, 2042, 2999, 2007, 28072, 1012, 2302, 2183, 2205, 2521, 2006, 2023, 3395, 1010, 1045, 2052, 2360, 2008, 4076, 2013, 1996, 4489, 1999, 15084, 2090, 1996, 2413, 1998, 1996, 4467, 2554, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1037, 3185, 1997, 2049, 2051, 1010, 1998, 2173, 1012, 1016, 1013, 2184, 1012, 102]\n[101, 2821, 1010, 2567, 1012, 1012, 1012, 2044, 4994, 2055, 2023, 9951, 2143, 2005, 8529, 13876, 12129, 2086, 2035, 1045, 2064, 2228, 1997, 2003, 2008, 2214, 14911, 3389, 2299, 1012, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1000, 2003, 2008, 2035, 2045, 2003, 1029, 1029, 1000, 1012, 1012, 1012, 1045, 2001, 2074, 2019, 2220, 9458, 2043, 2023, 20482, 3869, 2718, 1996, 1057, 1012, 1055, 1012, 1045, 2001, 2205, 2402, 2000, 2131, 1999, 1996, 4258, 1006, 2348, 1045, 2106, 6133, 2000, 13583, 2046, 1000, 9119, 8912, 1000, 1007, 1012, 2059, 1037, 11326, 2012, 1037, 2334, 2143, 2688, 10272, 17799, 1011, 2633, 1045, 2071, 2156, 2023, 2143, 1010, 3272, 2085, 1045, 2001, 2004, 2214, 2004, 2026, 3008, 2020, 2043, 2027, 8040, 7317, 13699, 5669, 2000, 2156, 2009, 999, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2069, 3114, 2023, 2143, 2001, 2025, 10033, 2000, 1996, 10812, 13457, 1997, 2051, 2001, 2138, 1997, 1996, 27885, 11020, 20693, 2553, 13977, 2011, 2049, 1057, 1012, 1055, 1012, 2713, 1012, 8817, 1997, 2111, 19311, 2098, 2000, 2023, 27136, 2121, 1010, 3241, 2027, 2020, 2183, 2000, 2156, 1037, 3348, 2143, 1012, 1012, 1012, 2612, 1010, 2027, 2288, 7167, 1997, 2485, 22264, 1997, 1043, 11802, 2135, 1010, 16360, 23004, 25430, 18352, 1010, 2006, 1011, 2395, 7636, 1999, 20857, 6023, 25943, 1010, 2004, 5498, 2063, 2576, 3653, 29048, 1012, 1012, 1012, 1998, 7408, 3468, 2040, 1011, 14977, 23599, 3348, 5019, 2007, 7842, 22772, 1010, 5122, 5889, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 3451, 12696, 1010, 4151, 24665, 12502, 1010, 3181, 20785, 1012, 1012, 3649, 2023, 2518, 2001, 1010, 14021, 5596, 2009, 1010, 6402, 2009, 1010, 2059, 4933, 1996, 11289, 1999, 1037, 2599, 3482, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 7069, 9765, 27065, 2229, 2145, 26988, 2000, 2424, 3643, 1999, 2049, 11771, 18404, 6208, 2576, 11867, 7974, 8613, 1012, 1012, 2021, 2065, 2009, 4694, 1005, 1056, 2005, 1996, 15657, 9446, 1010, 2009, 2052, 2031, 2042, 6439, 1010, 2059, 6404, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2612, 1010, 1996, 1000, 1045, 2572, 8744, 1010, 8744, 1000, 1054, 10536, 16921, 7583, 2516, 2001, 5567, 10866, 2135, 2005, 2086, 2004, 1037, 14841, 26065, 3508, 2005, 22555, 2080, 3152, 1006, 1045, 2572, 8025, 1010, 20920, 1011, 2005, 5637, 3152, 1010, 1045, 2572, 8025, 1010, 2304, 1011, 2005, 1038, 2721, 2595, 24759, 28100, 3370, 3152, 1010, 4385, 1012, 1012, 1007, 1998, 2296, 2702, 2086, 2030, 2061, 1996, 2518, 9466, 2013, 1996, 2757, 1010, 2000, 2022, 7021, 2011, 1037, 2047, 4245, 1997, 26476, 2015, 2040, 2215, 2000, 2156, 2008, 1000, 20355, 3348, 2143, 1000, 2008, 1000, 4329, 3550, 1996, 2143, 3068, 1000, 1012, 1012, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 6300, 9953, 1010, 4468, 2066, 1996, 11629, 1012, 1012, 2030, 2065, 2017, 2442, 2156, 2009, 1011, 9278, 1996, 2678, 1998, 3435, 2830, 2000, 1996, 1000, 6530, 1000, 3033, 1010, 2074, 2000, 2131, 2009, 2058, 2007, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 102]\n\n\n- 결론: imdb 에는 자료가 “텍스트” 형태로 저장되어있고, tokenized_imdb 는 자료가 “텍스트 \\(\\cup\\) 숫자” 형태로 저장되어 있음. 즉 tokenized_imdb 는 원래데이터 (raw data) 와 전처리된 데이터 (preprocessed data) 가 같이 있음."
  },
  {
    "objectID": "posts/02wk-1.html#b.-step2-인공지능-생성",
    "href": "posts/02wk-1.html#b.-step2-인공지능-생성",
    "title": "02wk-1: IMDB 영화평 감성분석",
    "section": "B. Step2 – 인공지능 생성",
    "text": "B. Step2 – 인공지능 생성\n- 인공지능을 생성하는 코드\n\nfrom transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert/distilbert-base-uncased\", num_labels=2\n)\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
  },
  {
    "objectID": "posts/02wk-1.html#c.-step3-인공지능-학습",
    "href": "posts/02wk-1.html#c.-step3-인공지능-학습",
    "title": "02wk-1: IMDB 영화평 감성분석",
    "section": "C. Step3 – 인공지능 학습",
    "text": "C. Step3 – 인공지능 학습\n- 트레이너를 만들때 필요한 재료 data_collator 생성\n\nfrom transformers import DataCollatorWithPadding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n- 트레이너를 만들때 필요한 재료 compute_metrics 생성\n\nimport evaluate\nimport numpy as np\naccuracy = evaluate.load(\"accuracy\")\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy.compute(predictions=predictions, references=labels)\n\n- 트레이너를 만들때 필요한 재료 training_args(훈련지침?) 생성\n\nfrom transformers import TrainingArguments, Trainer\ntraining_args = TrainingArguments(\n    output_dir=\"my_awesome_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2, # 전체문제세트를 2번 공부하라..\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n)\n\n- 트레이너 생성\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_imdb[\"train\"],\n    eval_dataset=tokenized_imdb[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n- 트레이너를 이용하여 인공지능을 학습시키는 코드\n\ntrainer.train()\n\n\n    \n      \n      \n      [3126/3126 12:33, Epoch 2/2]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n0.222000\n0.202918\n0.921160\n\n\n2\n0.147300\n0.226498\n0.932000\n\n\n\n\n\n\nTrainOutput(global_step=3126, training_loss=0.20518110291132619, metrics={'train_runtime': 753.5002, 'train_samples_per_second': 66.357, 'train_steps_per_second': 4.149, 'total_flos': 6556904415524352.0, 'train_loss': 0.20518110291132619, 'epoch': 2.0})"
  },
  {
    "objectID": "posts/02wk-1.html#d.-step4-예측",
    "href": "posts/02wk-1.html#d.-step4-예측",
    "title": "02wk-1: IMDB 영화평 감성분석",
    "section": "D. Step4 – 예측",
    "text": "D. Step4 – 예측\n\ntext0 = \"The movie was a major disappointment. The plot was weak, and the pacing felt disjointed. The characters lacked depth, making it hard to connect with them. Predictable twists and a cliché resolution left no sense of excitement. Visually, it was unimpressive, and the soundtrack didn’t fit the scenes, pulling me out of the experience. Overall, it failed to deliver anything memorable.\"\ntext1 = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\"\n\n\nfrom transformers import pipeline\nclassifier1 = pipeline(\"sentiment-analysis\", model=\"my_awesome_model/checkpoint-1563\")\nprint(classifier1(text0))\nprint(classifier1(text1))\n\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n\n\n[{'label': 'LABEL_0', 'score': 0.9910238981246948}]\n[{'label': 'LABEL_1', 'score': 0.993097722530365}]\n\n\n\nclassifier2 = pipeline(\"sentiment-analysis\", model=\"my_awesome_model/checkpoint-3126\")\nprint(classifier2(text0))\nprint(classifier2(text1))\n\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n\n\n[{'label': 'LABEL_0', 'score': 0.9966901540756226}]\n[{'label': 'LABEL_1', 'score': 0.9965715408325195}]"
  },
  {
    "objectID": "posts/06wk-1.html#a.-이미지-자료의-이해",
    "href": "posts/06wk-1.html#a.-이미지-자료의-이해",
    "title": "06wk-1, 07wk-1: UCF101 영상자료 분류",
    "section": "A. 이미지 자료의 이해",
    "text": "A. 이미지 자료의 이해\n- plt.imshow(...) 에서 ... 자리에 오는 numpy array의 shape이 (??,??) 와 같이 2차원 꼴이면 흑백이미지를 출력\n\nnp.array([[0, 150], [0,255]])\n\narray([[  0, 150],\n       [  0, 255]])\n\n\n\nplt.imshow(np.array([[0, 150], [0,255]]),cmap='gray')\nplt.colorbar()\n\n\n\n\n\n\n\n\n- plt.imshow(...) 에서 ... 자리에 오는 numpy array의 shape이 (??,??,3) 이러한 형태이면 칼라이미지를 출력\n\nR = np.array([[0,255],\n              [0,255]])\nG = np.array([[255,0],\n              [0,  0]])\nB = np.array([[0,  0],\n              [255,0]])\nplt.imshow(np.stack([R,G,B],axis=-1))\n\n\n\n\n\n\n\n\n- plt.imshow(...)에서 ... 자리에 오는 numpy array 의 dtype이 int인지 float인지에 따라서 시각화 결과가 다름\n\nint일 경우: 0을 최소값, 255를 최대값으로 생각하고 그림을 그려줌.\nfloat일 경우: 0.0을 최소값, 1.0을 최대값으로 생각하고 그림을 그려줌.\n\n\nR = np.array([[0,1],\n              [0,1]])\nG = np.array([[1,0],\n              [0,0]])\nB = np.array([[0,0],\n              [1,0]])\n#plt.imshow(np.stack([R,G,B],axis=-1))\nplt.imshow(np.stack([R,G,B],axis=-1).astype(float))"
  },
  {
    "objectID": "posts/06wk-1.html#b.-tsr.permute",
    "href": "posts/06wk-1.html#b.-tsr.permute",
    "title": "06wk-1, 07wk-1: UCF101 영상자료 분류",
    "section": "B. tsr.permute()",
    "text": "B. tsr.permute()\n- 실수로 R,G,B 를 이상한 방식으로 stack 했다고 가정하자.\n\nR = np.array([[0,255],[0,255]])\nG = np.array([[255,0],[0,0]])\nB = np.array([[0,0],[255,0]])\n# img = np.stack([R,G,B],axis=-1) # &lt;-- 원래는 이걸 하려고 했었음..\nimg = np.stack([R,G,B]) # &lt;-- 실수로 이렇게 만들었다고 하자...\nimg.shape\n\n(3, 2, 2)\n\n\n- 차원이 맞지 않아서 plt.imshow(img)이 동작하지 않음. 동작하기 위해서는 plt.imshow(...) 에서 ... 자리에 있는 numpy array가 HWC 와 같은 형식으로 되어야함. (그런데 지금은 CHW 임)\n\nplt.imshow(img)\n\nTypeError: Invalid shape (3, 2, 2) for image data\n\n\n\n\n\n\n\n\n\n- 에러를 피하기 위해서는 차원을 (3,2,2) 에서 (2,2,3) 으로 바꾸어야함.\n\nimg.reshape(2,2,3)\n\narray([[[  0, 255,   0],\n        [255, 255,   0]],\n\n       [[  0,   0,   0],\n        [  0, 255,   0]]])\n\n\n\nplt.imshow(img.reshape(2,2,3)) # ?? 이상한 그림이 나왔음\n\n\n\n\n\n\n\n\n\n우리가 원하는 그림은 아니네??\n왜 이런일이 생기는가??\n\n- 이미지자료의 차원을 바꾸고 싶다면 reshape을 쓰지 말고 np.transpose를 이용하라\n\nimg.reshape(2,2,3),  np.transpose(img,(1,2,0))\n\n(array([[[  0, 255,   0],\n         [255, 255,   0]],\n \n        [[  0,   0,   0],\n         [  0, 255,   0]]]),\n array([[[  0, 255,   0],\n         [255,   0,   0]],\n \n        [[  0,   0, 255],\n         [255,   0,   0]]]))\n\n\n\n위의 두개의 array는 차원이 같음. \\(\\to\\) 그림자체는 둘다 그려짐\n그렇지만 같은 array는 아님 \\(\\to\\) 당연히 그림도 달라짐\n\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(img.reshape(2,2,3))\nax[1].imshow(np.transpose(img, (1,2,0)))\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n왼쪽은 이상한 그림, 오른쪽은 우리가 원하는 그림\n\n- numpy array 가 아니고 pytorch tensor 일 경우에는 아래의 방법으로 변환\n\narr = np.random.rand(2,4,3)\ntsr = torch.tensor(arr)\n\n\nnp.transpose(arr,(2,0,1)) # (2,4,3) --&gt; (3,2,4)\n\narray([[[0.91648551, 0.14451989, 0.12429414, 0.08726179],\n        [0.68349001, 0.67635927, 0.80430196, 0.02365825]],\n\n       [[0.18466195, 0.4075408 , 0.17702914, 0.1660341 ],\n        [0.01750461, 0.07114555, 0.16904054, 0.91556281]],\n\n       [[0.45540234, 0.53803747, 0.56285487, 0.01249792],\n        [0.0097584 , 0.6514433 , 0.93219221, 0.95797311]]])\n\n\n\ntsr.permute(2,0,1) # (2,4,3) --&gt; (3,2,4)\n# np.transpose(tsr,(2,0,1)) # 같은코드\n\ntensor([[[0.9165, 0.1445, 0.1243, 0.0873],\n         [0.6835, 0.6764, 0.8043, 0.0237]],\n\n        [[0.1847, 0.4075, 0.1770, 0.1660],\n         [0.0175, 0.0711, 0.1690, 0.9156]],\n\n        [[0.4554, 0.5380, 0.5629, 0.0125],\n         [0.0098, 0.6514, 0.9322, 0.9580]]], dtype=torch.float64)"
  },
  {
    "objectID": "posts/06wk-1.html#c.-동영상-자료-이해",
    "href": "posts/06wk-1.html#c.-동영상-자료-이해",
    "title": "06wk-1, 07wk-1: UCF101 영상자료 분류",
    "section": "C. 동영상 자료 이해",
    "text": "C. 동영상 자료 이해\n- 동영상자료 = 여러사진들의 모음 = [사진1, 사진2, 사진3, 사진4, ….]\n\nnp.random.seed(43052)\nv = (np.random.rand(4,60,60,3)*255).astype(\"uint8\")\nv[0,:,:,0] = 255 # 첫번쨰 프레임의 R 채널을 모두 최대치로 설정\nv[1,:,:,1] = 255 # 두번쨰 프레임의 G 채널을 모두 최대치로 설정\nv[2,:,:,2] = 255 # 세번째 프레임의 B 채널을 모두 최대치로 설정\n\n\nfig, ax = plt.subplots(2,2)\n# ax[0][0].imshow(v[0,:,:,:])\n# ax[0][1].imshow(v[1,:,:,:])\n# ax[1][0].imshow(v[2,:,:,:])\n# ax[1][1].imshow(v[3,:,:,:])\nax[0][0].imshow(v[0])\nax[0][1].imshow(v[1])\nax[1][0].imshow(v[2])\nax[1][1].imshow(v[3])\n\n\n\n\n\n\n\n\n\nframes = [frame for frame in v]\n#frames = [사진1, 사진2, 사진3, 사진4]\n\n\nimageio.mimsave(\"sample.gif\",frames)\n\n\n#IPython.display.Image(\"sample.gif\")"
  },
  {
    "objectID": "posts/06wk-1.html#d.-np.clip",
    "href": "posts/06wk-1.html#d.-np.clip",
    "title": "06wk-1, 07wk-1: UCF101 영상자료 분류",
    "section": "D. np.clip()",
    "text": "D. np.clip()\n\narr = np.array([[300, -20, 150], [400, 100, 0], [255, 500, -100]])\narr\n\narray([[ 300,  -20,  150],\n       [ 400,  100,    0],\n       [ 255,  500, -100]])\n\n\n\nnp.clip(arr,0,255)\n\narray([[255,   0, 150],\n       [255, 100,   0],\n       [255, 255,   0]])"
  },
  {
    "objectID": "posts/06wk-1.html#a.-다운로드",
    "href": "posts/06wk-1.html#a.-다운로드",
    "title": "06wk-1, 07wk-1: UCF101 영상자료 분류",
    "section": "A. 다운로드",
    "text": "A. 다운로드\n- imports\n\nimport huggingface_hub\n\n- 처음에는 /root/.cache 에 huggingface라는 폴더가 존재하지 않음\n\nfile_path = huggingface_hub.hf_hub_download(\n    repo_id=\"sayakpaul/ucf101-subset\",\n    filename=\"UCF101_subset.tar.gz\",\n    repo_type=\"dataset\"\n)\n\n\n\n\n\nfile_path\n\n'/root/.cache/huggingface/hub/datasets--sayakpaul--ucf101-subset/snapshots/b9984b8d2a95e4a1879e1b071e9433858d0bc24a/UCF101_subset.tar.gz'\n\n\n- 위의명령어를 실행하면 /root/.cache 에 적당한 폴더가 만들어지고 UCF101_subset.tar.gz 와 같은 파일이 다운로드 되어있음\n- huggingface_hub.hf_hub_download 함수의 역할: (1) UCF101_subset.tar.gz 파일을 다운로드 (2) 다운로드한 압축파일의 경로를 str으로 리턴"
  },
  {
    "objectID": "posts/06wk-1.html#b.-압축풀기",
    "href": "posts/06wk-1.html#b.-압축풀기",
    "title": "06wk-1, 07wk-1: UCF101 영상자료 분류",
    "section": "B. 압축풀기",
    "text": "B. 압축풀기\n- imports\n\nimport tarfile\n\n- 아래의 코드는 현재폴더에 UCF101_subset.tar.gz 파일의 압축을 해제하는 코드이다.\n\n# file_path는 다운로드한 압축파일이 존재하는 경로와 파일명이 string으로 저장되어있음.\nwith tarfile.open(file_path) as t:\n     t.extractall(\".\") # 여기에서 \".\"은 현재폴더라는 의미\n\n- tarfile.TarFile의 인스턴스 t 가 가지는 주요기능\n\nt = tarfile.open(file_path)\nt.extractall(\"./asdf\")\n\n- tarfile.TarFile의 인스턴스 t가 가지는 덜 중요한 기능: 더 이상 압축을 풀고 싶지 않을때, t.close()를 이용하여 t.extractall()을 봉인할 수 있다.\n\nt = tarfile.open(file_path) # t 오브젝트 생성\nt.closed # 오브젝트 생성 직후에는 t.closed 는 False\n\nFalse\n\n\n\nt.extractall(\"./여기에\")\n\n\nt.extractall(\"./저기에\")\n\n\nt.close() # 더이상 무분별하게 압축을 풀고싶지 않다면 이걸 실행\n\n\nt.closed # 그렇다면 t.closed 가 True로 되고\n\nTrue\n\n\n\nt.extractall(\"./또풀어\") # 에러발생\n\nOSError: TarFile is closed\n\n\n- tarfile.TarFile의 인스턴스 t가 가지는 덜 중요한 기능2: t._check() 은 t.closed = True일때 실행하면 에러를 발생시키는 역할을 한다.\n\nprint(t.closed)\nt._check() # t.closed 가 open 일 경우에는 에러X\n\nFalse\n\n\n\nt.close()\nprint(t.closed)\nt._check() # t.closed 가 open 일 경우에는 에러X\n\nTrue\n\n\nOSError: TarFile is closed\n\n\n- with의 이해\n아래와 같은 코드를 작성하고 싶음\n1. `t`를 만듦. 즉 `TarFile`의 인스턴스를 만듦.\n2. `t`가 열린상태인지 체크 (=압축풀기가 가능한 상태인지 체크를 해줌, 닫힌상태라면 에러메시지 발생하고 진행중지)\n3. 압축해제\n4. `t`를 닫힌상태로 만듦.\n아래와 같이 쓸 수 있음\n\nt = tarfile.open(file_path)\nt._check()\nt.extractall(\".\")\nt.close()\n\n위의 코드는 아래의 코드와 같은 효과\n\nwith tarfile.open(file_path) as t:\n  ## 시작코드 -- t._ckech() 이 자동실행 (사실은 t.__enter__()이 실행)\n  t.extractall(\".\") ## 본코드\n  ## 마무리코드 -- t.close() 이 자동실행 (사실은 t.__exit__()이 실행)\n\n\nt.closed # 예쁘게 잘 닫혀있음..\n\nTrue\n\n\n\n\n\n\n\n\nNote\n\n\n\nwith ... as ???:에서 ??? 자리에 들어가는 오브젝트는 __enter__()와 __exit__()라는 숨겨진 메서드를 가진 오브젝트여야 한다.\nimport tarfile\n\n# tar 파일을 열고 자동으로 닫아주는 with 문 사용\nwith tarfile.open('example.tar.gz') as tar:  # tarfile 객체는 __enter__, __exit__ 메서드를 가짐\n    tar.extractall(path='output_folder')  # 본래 코드: tar 파일 안의 파일들을 추출\n# 여기서 `tar.__exit__()`가 호출되어 tar 파일이 자동으로 닫힘\n위의 예시에서 tarfile.open()이 반환하는 tar 오브젝트가 __enter__()와 __exit__() 를 가지고 있다. 위의 코드가 실행되는 원리는 대략적으로 아래와 같다.\n\nwith 문이 실행되면, 먼저 tar.__enter__()가 호출된다. 이 메서드는 tar 파일을 열고, 파일을 처리할 준비를 한다.\ntarfile.open('example.tar.gz')  # 이 때 tar.__enter__() 호출\n본래 코드 실행: 이후 with 문 안의 본래 코드, 즉 tar.extractall(path='output_folder')가 실행되어 tar 파일의 내용을 추출한다.\ntar.extractall(path='output_folder')  # 본래 코드 실행\nwith 문이 종료될 때, tar.__exit__() 메서드가 호출되어 tar 파일이 자동으로 닫힌다. 이로 인해 파일을 명시적으로 닫을 필요가 없어지며, 자원 관리가 안전하게 이루어진다.\n# with 문 종료 시 tar.__exit__() 호출\n\n이렇게 with 문은 __enter__()와 __exit__() 메서드를 가진 객체를 활용하여, 자원의 열림과 닫힘을 자동으로 처리하는 구조이다."
  },
  {
    "objectID": "posts/06wk-1.html#c.-동영상확인",
    "href": "posts/06wk-1.html#c.-동영상확인",
    "title": "06wk-1, 07wk-1: UCF101 영상자료 분류",
    "section": "C. 동영상확인",
    "text": "C. 동영상확인\n- 코덱문제로 인하여 v_Basketball_g01_c01.avi는 재생이 안됨\n- 아래의 명령어를 사용하여 v_Basketball_g01_c01.avi 파일을 output_video.mp4 로 변환\n!ffmpeg -i UCF101_subset/train/Basketball/v_Basketball_g01_c01.avi -vcodec libx264 -acodec aac output_video.mp4 -y\n- output_video.mp4 를 다운로드하고 재생"
  },
  {
    "objectID": "posts/06wk-1.html#d.-동영상-불러오기",
    "href": "posts/06wk-1.html#d.-동영상-불러오기",
    "title": "06wk-1, 07wk-1: UCF101 영상자료 분류",
    "section": "D. 동영상 불러오기",
    "text": "D. 동영상 불러오기\n- imports\n\nimport pytorchvideo.data\n# import os\n# import pathlib\n\n\n#. 동영상(avi) \\(\\to\\) 텐서\n\n_train_dataset = pytorchvideo.data.labeled_video_dataset(\n    data_path = 'UCF101_subset/train',\n    clip_sampler = pytorchvideo.data.make_clip_sampler(\"random\", 10),\n    decode_audio = False,\n#    transform = train_transform,\n)\n\n\nvideo_cthw = next(_train_dataset)['video']\nvideo_cthw.shape\n\ntorch.Size([3, 141, 240, 320])\n\n\n\n\n# 텐서 \\(\\to\\) 동영상\n- 차원변환\n\nvideo_thwc = video_cthw.permute(1,2,3,0)\nvideo_thwc.shape\n\ntorch.Size([141, 240, 320, 3])\n\n\n\nplt.imshow(video_thwc[-1].numpy().astype('uint8'))\n\n\n\n\n\n\n\n\n- gif 만들기\n\nframes = [frame.numpy().astype('uint8') for frame in video_thwc]\nimageio.mimsave(\"sample.gif\",frames)\n\n\nIPython.display.Image(\"sample.gif\")\n\n&lt;IPython.core.display.Image object&gt;\n\n\n- “텐서 \\(\\to\\) 영상” 이 되는 함수를 선언하자.\n\ndef _display_gif(video_cthw):\n    video_thwc = video_cthw.permute(1,2,3,0)\n    frames = [frame.numpy().astype('uint8') for frame in video_thwc]\n    imageio.mimsave(\"sample.gif\",frames)\n    return IPython.display.Image(\"sample.gif\")\n\n\n_display_gif(next(_train_dataset)['video'])\n\n&lt;IPython.core.display.Image object&gt;\n\n\n- pytorchvideo.data.make_clip_sampler(\"random\", 10) 의 의미: 영상을 보고 스스로 파악해보세요..\n\npytorchvideo.data.make_clip_sampler(\"uniform\", 10) 의 의미도 스스로 살펴보세요\n\n\n_train_dataset = pytorchvideo.data.labeled_video_dataset(\n    data_path = 'UCF101_subset/train',\n    clip_sampler = pytorchvideo.data.make_clip_sampler(\"random\", 1),\n    decode_audio = False,\n#    transform = train_transform,\n)\n_display_gif(next(_train_dataset)['video'])\n\n&lt;IPython.core.display.Image object&gt;"
  },
  {
    "objectID": "posts/06wk-1.html#e.-모델생성",
    "href": "posts/06wk-1.html#e.-모델생성",
    "title": "06wk-1, 07wk-1: UCF101 영상자료 분류",
    "section": "E. 모델생성",
    "text": "E. 모델생성\n- imports\n\nimport transformers\n\n- 모델생성\n\nlabel2id = {\n    'ApplyEyeMakeup': 0,\n    'ApplyLipstick': 1,\n    'Archery': 2,\n    'BabyCrawling': 3,\n    'BalanceBeam': 4,\n    'BandMarching': 5,\n    'BaseballPitch': 6,\n    'Basketball': 7,\n    'BasketballDunk': 8,\n    'BenchPress': 9\n}\nid2label = {\n    0: 'ApplyEyeMakeup',\n    1: 'ApplyLipstick',\n    2: 'Archery',\n    3: 'BabyCrawling',\n    4: 'BalanceBeam',\n    5: 'BandMarching',\n    6: 'BaseballPitch',\n    7: 'Basketball',\n    8: 'BasketballDunk',\n    9: 'BenchPress'\n}\nmodel = transformers.VideoMAEForVideoClassification.from_pretrained(\n    \"MCG-NJU/videomae-base\",\n    label2id=label2id,\n    id2label=id2label,\n    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n)\n\nSome weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
  },
  {
    "objectID": "posts/06wk-1.html#f.-변환-to-trvaltest-생성",
    "href": "posts/06wk-1.html#f.-변환-to-trvaltest-생성",
    "title": "06wk-1, 07wk-1: UCF101 영상자료 분류",
    "section": "F. 변환 \\(\\to\\) tr/val/test 생성",
    "text": "F. 변환 \\(\\to\\) tr/val/test 생성\n- imports\n\nimport pytorchvideo.transforms\nimport torchvision.transforms\n\n- 이미지전처리에 도움이 되는 image_processor 를 불러오는 코드\n\nimage_processor = transformers.VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n\n- 저장된값들\n\nprint(\n    f\"mean: {image_processor.image_mean}\\n\"\n    f\"std: {image_processor.image_std}\\n\"\n    f\"size: {image_processor.size}\\n\"\n    f\"resize_to: {(224,224)}\\n\"\n    f\"num_frames_to_sample: {model.config.num_frames}\\n\"\n    f\"clip_duration: {2.1333333333333333}\"\n)\n\nmean: [0.485, 0.456, 0.406]\nstd: [0.229, 0.224, 0.225]\nsize: {'shortest_edge': 224}\nresize_to: (224, 224)\nnum_frames_to_sample: 16\nclip_duration: 2.1333333333333333\n\n\n\n모델 MCG-NJU/videomae-base 의 성능을 최고로 이끌어 내기 위해서는 어떠한 최적화된 변환이 있음\n위에 제시된 값들은 그 최적화된 변환을 구현하기위해 필요한 값들임.\n\n- 변환된 이미지텐서를 올바르게 출력하기 위해 _display_gif 를 수정하여 display_gif 를 만듦.\n\ndef display_gif(video_cthw):\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    #---# \n    video_thwc = video_cthw.permute(1,2,3,0)\n    frames = [] \n    for frame in video_thwc:\n        img = frame = frame.numpy()\n        img = (img * std) + mean \n        img = (img * 255).astype('uint8')\n        img = img.clip(0,255)\n        frames.append(img)\n    imageio.mimsave(\"sample.gif\",frames, duration=0.25)\n    return IPython.display.Image(\"sample.gif\")    \n\n- 최적화된 변환을 적용하여 train_dataset을 불러옴\n\ntrain_dataset = pytorchvideo.data.labeled_video_dataset(\n    data_path = 'UCF101_subset/train',\n    clip_sampler = pytorchvideo.data.make_clip_sampler(\"random\", 2.1333333333333333),\n    decode_audio = False,\n    transform = pytorchvideo.transforms.ApplyTransformToKey(\n            key=\"video\",\n            transform=torchvision.transforms.Compose(\n                [\n                    pytorchvideo.transforms.UniformTemporalSubsample(16),\n                    torchvision.transforms.Lambda(lambda x: x / 255.0),\n                    pytorchvideo.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n                    pytorchvideo.transforms.RandomShortSideScale(min_size=256, max_size=320),\n                    torchvision.transforms.RandomCrop((224,224)),\n                    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n                ]\n            ),\n        )\n)\n\n- 이제 val_dataset, test_dataset 도 불러옴\n\nval_dataset = pytorchvideo.data.labeled_video_dataset(\n    data_path = 'UCF101_subset/val',\n    clip_sampler = pytorchvideo.data.make_clip_sampler(\"uniform\", 2.1333333333333333),\n    decode_audio = False,\n    transform = pytorchvideo.transforms.ApplyTransformToKey(\n            key=\"video\",\n            transform=torchvision.transforms.Compose(\n                [\n                    pytorchvideo.transforms.UniformTemporalSubsample(16),\n                    torchvision.transforms.Lambda(lambda x: x / 255.0),\n                    pytorchvideo.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n                    torchvision.transforms.Resize((224,224))\n                ]\n            ),\n        )\n)\ntest_dataset = pytorchvideo.data.labeled_video_dataset(\n    data_path = 'UCF101_subset/test',\n    clip_sampler = pytorchvideo.data.make_clip_sampler(\"uniform\", 2.1333333333333333),\n    decode_audio = False,\n    transform = pytorchvideo.transforms.ApplyTransformToKey(\n            key=\"video\",\n            transform=torchvision.transforms.Compose(\n                [\n                    pytorchvideo.transforms.UniformTemporalSubsample(16),\n                    torchvision.transforms.Lambda(lambda x: x / 255.0),\n                    pytorchvideo.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n                    torchvision.transforms.Resize((224,224))\n                ]\n            ),\n        )\n)"
  },
  {
    "objectID": "posts/06wk-1.html#g.-데이터콜렉터",
    "href": "posts/06wk-1.html#g.-데이터콜렉터",
    "title": "06wk-1, 07wk-1: UCF101 영상자료 분류",
    "section": "G. 데이터콜렉터",
    "text": "G. 데이터콜렉터\n- 일단은 아래가 실행되지 않음을 관찰하자.\n\nmodel(train_dataset) \n\nAttributeError: 'LabeledVideoDataset' object has no attribute 'shape'\n\n\n- 실행되는 형태1: model(tsr_ntchw)\n\nexamples = [next(train_dataset), next(train_dataset)]\n\n실행예시1\n\nmodel(examples[0]['video'].permute(1,0,2,3).reshape(1,16,3,224,224))\n\nImageClassifierOutput(loss=None, logits=tensor([[-0.1585,  0.5085,  0.1967, -0.7588,  0.3329, -0.5446,  0.5754, -0.2992,\n         -1.0191, -0.4156]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n실행예시2\n\nmodel(torch.stack([examples[0]['video'].permute(1,0,2,3), examples[1]['video'].permute(1,0,2,3)],axis=0))\n\nImageClassifierOutput(loss=None, logits=tensor([[-0.1585,  0.5085,  0.1967, -0.7588,  0.3329, -0.5446,  0.5754, -0.2992,\n         -1.0191, -0.4156],\n        [-0.0798,  0.1997,  0.2231, -0.6410,  0.6482, -0.3275,  0.1121, -0.1804,\n         -0.4605, -0.3052]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n실행예시3\n\nmodel(torch.stack([example['video'].permute(1,0,2,3) for example in examples],axis=0))\n\nImageClassifierOutput(loss=None, logits=tensor([[-0.1585,  0.5085,  0.1967, -0.7588,  0.3329, -0.5446,  0.5754, -0.2992,\n         -1.0191, -0.4156],\n        [-0.0798,  0.1997,  0.2231, -0.6410,  0.6482, -0.3275,  0.1121, -0.1804,\n         -0.4605, -0.3052]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n- 실행되는 형태1: model(**{\"pixel_values\":tsr_ntchw, \"labels\":tsrlb_n})\n\nexamples = [next(train_dataset), next(train_dataset)]\ntsr_ntchw = torch.stack([example['video'].permute(1,0,2,3) for example in examples])\ntsrlb_n = torch.tensor([example['label'] for example in examples])\n\n\nmodel(**{\"pixel_values\":tsr_ntchw, \"labels\":tsrlb_n})\n\nImageClassifierOutput(loss=tensor(2.3263, grad_fn=&lt;NllLossBackward0&gt;), logits=tensor([[-0.2561,  0.1628,  0.3724, -0.6019,  0.4608, -0.2620,  0.2757, -0.0861,\n         -0.4760, -0.1048],\n        [-0.1494,  0.0018,  0.2914, -0.1403,  0.1759, -0.3493,  0.5711,  0.1108,\n         -0.2272, -0.1822]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n- 그렇다면 데이터콜렉터는?\n\ndef collate_fn(examples): # examples 는 [Dict, Dict, ...]\n    tsr_ntchw = torch.stack([example['video'].permute(1,0,2,3) for example in examples])\n    tsrlb_n = torch.tensor([example['label'] for example in examples])\n    return dict(pixel_values=tsr_ntchw,labels=tsrlb_n)\n\n\n# model(**collate_fn(val_dataset)) # 좀 오래걸리니까 실행하지 마세요"
  },
  {
    "objectID": "posts/10wk-1.html#a.-.select",
    "href": "posts/10wk-1.html#a.-.select",
    "title": "10wk-1: Datasets 클래스",
    "section": "A. .select()",
    "text": "A. .select()\n# 예시1\n\n#d = datasets.Dataset.from_list([emotion['train'][0],emotion['train'][1],emotion['train'][2],emotion['train'][3]])\nd = emotion['train'].select(range(4))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n#"
  },
  {
    "objectID": "posts/10wk-1.html#b.-.shuffle",
    "href": "posts/10wk-1.html#b.-.shuffle",
    "title": "10wk-1: Datasets 클래스",
    "section": "B. .shuffle()",
    "text": "B. .shuffle()\n# 예시1\n\nd = emotion['train'].select(range(4))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n\nshow(d.shuffle())\n\nList Overview:\nTotal items: 4\n\n1. list[0]\n   - Type: dict\n   - Length: 2\n   - Values: {'text': 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property', 'label': 2}\n\n2. list[1]\n   - Type: dict\n   - Length: 2\n   - Values: {'text': 'im grabbing a minute to post i feel greedy wrong', 'label': 3}\n\n3. list[2]\n   - Type: dict\n   - Length: 2\n   - Values: {'text': 'i didnt feel humiliated', 'label': 0}\n\n4. list[3]\n   - Type: dict\n   - Length: 2\n   - Values: {'text': 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 'label': 0}\n\n\n#\n# 예시2\n\n# emotion['train']에서 처음 4개의 observation/example 을 뽑는 코드\nd = emotion['train'].select(range(4))\nd \n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n\n# emotion['train']에서 랜덤으로 4개의 observation/example 을 뽑는 코드\nd = emotion['train'].shuffle().select(range(4))\nd \n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n#"
  },
  {
    "objectID": "posts/10wk-1.html#c.-.select_columns",
    "href": "posts/10wk-1.html#c.-.select_columns",
    "title": "10wk-1: Datasets 클래스",
    "section": "C. .select_columns()",
    "text": "C. .select_columns()\n# 예시1\n\nd = emotion['train'].select(range(4))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n\nd.select_columns(['text'])\n\nDataset({\n    features: ['text'],\n    num_rows: 4\n})\n\n\n\nd.select_columns(['label'])\n\nDataset({\n    features: ['label'],\n    num_rows: 4\n})\n\n\n\nd.select_columns(['text','label'])\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n#"
  },
  {
    "objectID": "posts/10wk-1.html#d.-.set_format",
    "href": "posts/10wk-1.html#d.-.set_format",
    "title": "10wk-1: Datasets 클래스",
    "section": "D. .set_format()",
    "text": "D. .set_format()\n# 예시1\n\nd = emotion['train'].select(range(4))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n\nd.set_format(type=\"pandas\")\n\n\nd['text']\n\n0                              i didnt feel humiliated\n1    i can go from feeling so hopeless to so damned...\n2     im grabbing a minute to post i feel greedy wrong\n3    i am ever feeling nostalgic about the fireplac...\nName: text, dtype: object\n\n\n\nd['label']\n\n0    0\n1    0\n2    3\n3    2\nName: label, dtype: int64\n\n\n#\n# 예시2\n\nd = emotion['train'].select(range(4))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n\nd.set_format(type=\"pandas\",columns=['label'])\n\n\nd['text']\n\n['i didnt feel humiliated',\n 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n 'im grabbing a minute to post i feel greedy wrong',\n 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property']\n\n\n\nd['label']\n\n0    0\n1    0\n2    3\n3    2\nName: label, dtype: int64\n\n\n#\n# 예시3\n\nd = emotion['train'].select(range(4))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n\nd.set_format(type=\"pt\",columns=['label'])\n\n\nd['text']\n\n['i didnt feel humiliated',\n 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n 'im grabbing a minute to post i feel greedy wrong',\n 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property']\n\n\n\nd['label']\n\ntensor([0, 0, 3, 2])\n\n\n#"
  },
  {
    "objectID": "posts/10wk-1.html#e.-.reset_format",
    "href": "posts/10wk-1.html#e.-.reset_format",
    "title": "10wk-1: Datasets 클래스",
    "section": "E. .reset_format()",
    "text": "E. .reset_format()\n# 예시1\n\nd = emotion['train'].select(range(4))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n\nd.set_format(type=\"pt\",columns=['label'])\n\n\nd['text']\n\n['i didnt feel humiliated',\n 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n 'im grabbing a minute to post i feel greedy wrong',\n 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property']\n\n\n\nd['label']\n\ntensor([0, 0, 3, 2])\n\n\n\nd.reset_format()\n\n\nd['text']\n\n['i didnt feel humiliated',\n 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n 'im grabbing a minute to post i feel greedy wrong',\n 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property']\n\n\n\nd['label']\n\n[0, 0, 3, 2]\n\n\n#"
  },
  {
    "objectID": "posts/10wk-1.html#a.-d.map",
    "href": "posts/10wk-1.html#a.-d.map",
    "title": "10wk-1: Datasets 클래스",
    "section": "A. d.map()",
    "text": "A. d.map()\n# 예제1 – .map()에 대한 이해\n아래와 같은 Dataset이 있다고 하자.\n\nd = emotion['train'].select(range(4))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\nd.map()을 이용하여 아래와 같이 변환하라.\n\n\n\n\n\n\n\n\n데이터\n변환전\n변환후\n\n\n\n\nd[0]\ntext: strlabel: int\ntext: strlabel: intinput_ids: [int,…,int]attention_mask: [int,…,int]\n\n\nd[:1]\ntext: [str]label: [int]\ntext: [str]label: [int]input_ids: [[int,…,int]]attention_mask: [[int,…,int]]\n\n\n\n(풀이1) – d.map()을 사용하지 않은 풀이..\nd는 아래와 같은 구조로 이해할 수 있음\n\nd = [example_1, example_2, example_3, example_4]\nexample_i = {'text': xxx, 'label' = yyy}\n\n리스트화\n\nlst = d.to_list()\nlst\n\n[{'text': 'i didnt feel humiliated', 'label': 0},\n {'text': 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n  'label': 0},\n {'text': 'im grabbing a minute to post i feel greedy wrong', 'label': 3},\n {'text': 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property',\n  'label': 2}]\n\n\n리스트의 첫 요소에 변환적용\n\nl = lst[0]\nl\n\n{'text': 'i didnt feel humiliated', 'label': 0}\n\n\n\nr = tokenizer(l['text'])\nr\n\n{'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n\n\nl와 tokenizer(l['text'])을 합침\n\nl\n\n{'text': 'i didnt feel humiliated', 'label': 0}\n\n\n\nr\n\n{'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n\n\n\nl|r \n\n{'text': 'i didnt feel humiliated', 'label': 0, 'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n\n\n반복\n\nlst2 = [l | tokenizer(l['text']) for l in lst]\nlst2\n\n[{'text': 'i didnt feel humiliated', 'label': 0, 'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]},\n {'text': 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 'label': 0, 'input_ids': [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n {'text': 'im grabbing a minute to post i feel greedy wrong', 'label': 3, 'input_ids': [101, 10047, 9775, 1037, 3371, 2000, 2695, 1045, 2514, 20505, 3308, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n {'text': 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property', 'label': 2, 'input_ids': [101, 1045, 2572, 2412, 3110, 16839, 9080, 12863, 2055, 1996, 13788, 1045, 2097, 2113, 2008, 2009, 2003, 2145, 2006, 1996, 3200, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}]\n\n\nlst2를 d2로..\n\nd2 = datasets.Dataset.from_list(lst2)\nd2\n\nDataset({\n    features: ['text', 'label', 'input_ids', 'attention_mask'],\n    num_rows: 4\n})\n\n\n\nd2[0]\n\n{'text': 'i didnt feel humiliated',\n 'label': 0,\n 'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n\n\n\nd2[:1]\n\n{'text': ['i didnt feel humiliated'],\n 'label': [0],\n 'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}\n\n\n(풀이2)\n\ndef m_transform(example):\n    # example = l = {'text': 'i didnt feel humiliated', 'label': 0} \n    result = tokenizer(example['text'])\n    return result \n\n\nd2 = d.map(m_transform)\nd2[0]\n\n{'text': 'i didnt feel humiliated',\n 'label': 0,\n 'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n\n\n\nd2[:1]\n\n{'text': ['i didnt feel humiliated'],\n 'label': [0],\n 'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}\n\n\n#\n:::{.callout-note}\n.map()의 특징\n\n특징1: m_transform()은 입력으로 example = {'text':xxx, 'label':yyy} 꼴을 가정한다.\n특징2: .map()은 변환전 dict와 변환후 dict를 합친다."
  },
  {
    "objectID": "posts/10wk-1.html#b.-dd.map",
    "href": "posts/10wk-1.html#b.-dd.map",
    "title": "10wk-1: Datasets 클래스",
    "section": "B. dd.map()",
    "text": "B. dd.map()\n# 예제1 – dd에도 .map을 적용할 수 있음\n아래와 같은 DatasetDict가 있다고 하자.\n\ndd = datasets.DatasetDict({\n    'train':emotion['train'].select(range(4)),\n    'test':emotion['test'].select(range(4)),\n})\ndd\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 4\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 4\n    })\n})\n\n\ndd.map()을 이용하여 아래와 같이 변환하라.\n\n\n\n\n\n\n\n\n\ntr/test\n데이터\n변환전\n변환후\n\n\n\n\ntrain\nd[0]\ntext: strlabel: int\ntext: strlabel: intinput_ids: [int,…,int]attention_mask: [int,…,int]\n\n\ntrain\nd[:1]\ntext: [str]label: [int]\ntext: [str]label: [int]input_ids: [[int,…,int]]attention_mask: [[int,…,int]]\n\n\ntest\nd[0]\ntext: strlabel: int\ntext: strlabel: intinput_ids: [int,…,int]attention_mask: [int,…,int]\n\n\ntest\nd[:1]\ntext: [str]label: [int]\ntext: [str]label: [int]input_ids: [[int,…,int]]attention_mask: [[int,…,int]]\n\n\n\n(풀이)\n\ndef m_transform(example):\n    # example = l = {'text': 'i didnt feel humiliated', 'label': 0} \n    result = tokenizer(example['text'])\n    return result \n\n\ndd2 = dd.map(m_transform)\n\n\ndd2['train'][0]\n\n{'text': 'i didnt feel humiliated',\n 'label': 0,\n 'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n\n\n\ndd2['train'][:1]\n\n{'text': ['i didnt feel humiliated'],\n 'label': [0],\n 'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}\n\n\n\ndd2['test'][0]\n\n{'text': 'im feeling rather rotten so im not very ambitious right now',\n 'label': 0,\n 'input_ids': [101,\n  10047,\n  3110,\n  2738,\n  11083,\n  2061,\n  10047,\n  2025,\n  2200,\n  12479,\n  2157,\n  2085,\n  102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\ndd2['test'][:1]\n\n{'text': ['im feeling rather rotten so im not very ambitious right now'],\n 'label': [0],\n 'input_ids': [[101,\n   10047,\n   3110,\n   2738,\n   11083,\n   2061,\n   10047,\n   2025,\n   2200,\n   12479,\n   2157,\n   2085,\n   102]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n\n\n#"
  },
  {
    "objectID": "posts/10wk-1.html#c.-d.mapbatchtrue",
    "href": "posts/10wk-1.html#c.-d.mapbatchtrue",
    "title": "10wk-1: Datasets 클래스",
    "section": "C. d.map(batch=True)",
    "text": "C. d.map(batch=True)\n# 예제1 – d.map(batch=True)의 이해\n아래와 같은 Dataset이 있다고 하자.\n\nd = emotion['train'].select(range(8))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 8\n})\n\n\nd.map(batch=True)을 이용하여 아래와 같이 변환하라.\n\n\n\n\n\n\n\n\n\n데이터\n변환전\n변환후\n특이사항\n\n\n\n\nd[0]\ntext: strlabel: int\ntext: strlabel: intinput_ids: [int,…,int]attention_mask: [int,…,int]\n변환시 2개의 example씩 묶어서 패딩\n\n\nd[:1]\ntext: [str]label: [int]\ntext: [str]label: [int]input_ids: [[int,…,int]]attention_mask: [[int,…,int]]\n변환시 2개의 example씩 묶어서 패딩\n\n\n\n(풀이1) – 실패\n\ndef m_transform(example):\n    # example = {'text':xxx, 'label':yyy}\n    result = tokenizer(example['text'],padding=True)\n    return result \n\n\nd2 = d.map(m_transform)\nd2\n\nDataset({\n    features: ['text', 'label', 'input_ids', 'attention_mask'],\n    num_rows: 8\n})\n\n\n\nrprint(\"d2[:4]['input_ids']\")\nshow(d2[:4]['input_ids'])\n\nd2[:4]['input_ids']\n\n\n\nList Overview:\nTotal items: 4\n\n1. list[0]\n   - Type: list\n   - Length: 7\n   - Values: [101, 1045, 2134, 2102, 2514, 26608, 102]\n\n2. list[1]\n   - Type: list\n   - Length: 23\n   - Values: [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]\n\n3. list[2]\n   - Type: list\n   - Length: 12\n   - Values: [101, 10047, 9775, 1037, 3371, 2000, 2695, 1045, 2514, 20505, 3308, 102]\n\n4. list[3]\n   - Type: list\n   - Length: 22\n   - Values: [101, 1045, 2572, 2412, 3110, 16839, 9080, 12863, 2055, 1996, 13788, 1045, 2097, 2113, 2008, 2009, 2003, 2145, 2006, 1996, 3200, 102]\n\n\n(풀이2) – 성공\n\n# def m_transform(example):\n#     # example = {'text':xxx, 'label':yyy}\n#     result = tokenizer(example['text'], padding=True)\n#     return result \ndef m_transform_batch(example_batch):\n    # example_batch = {'text':[xxx,xxxx], 'label':[yyy,yyyy]}\n    result = tokenizer(example_batch['text'], padding=True)\n    return result \n\n\nd2 = d.map(m_transform_batch,batched=True,batch_size=2)\nd2\n\nDataset({\n    features: ['text', 'label', 'input_ids', 'attention_mask'],\n    num_rows: 8\n})\n\n\n\nrprint(\"d2[:4]['input_ids']\")\nshow(d2[:4]['input_ids'])\n\nd2[:4]['input_ids']\n\n\n\nList Overview:\nTotal items: 4\n\n1. list[0]\n   - Type: list\n   - Length: 23\n   - Values: [101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n2. list[1]\n   - Type: list\n   - Length: 23\n   - Values: [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]\n\n3. list[2]\n   - Type: list\n   - Length: 22\n   - Values: [101, 10047, 9775, 1037, 3371, 2000, 2695, 1045, 2514, 20505, 3308, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n4. list[3]\n   - Type: list\n   - Length: 22\n   - Values: [101, 1045, 2572, 2412, 3110, 16839, 9080, 12863, 2055, 1996, 13788, 1045, 2097, 2113, 2008, 2009, 2003, 2145, 2006, 1996, 3200, 102]\n\n\n#\n:::{.callout-note}\n.map(batch=True)의 특징\n\n특징1: m_transform_batch()은 입력으로 example_batch = {'text':[xxx,xxxx,...], 'label':[yyy,yyyy,...]} 꼴을 가정한다.\n특징2: example_batch는 batch_size만큼 데이터가 있다고 생각한다."
  },
  {
    "objectID": "posts/10wk-1.html#d.-d.map-칼럼선택",
    "href": "posts/10wk-1.html#d.-d.map-칼럼선택",
    "title": "10wk-1: Datasets 클래스",
    "section": "D. d.map() + 칼럼선택",
    "text": "D. d.map() + 칼럼선택\n# 예제1 – attention_mask 제외\n아래와 같은 Dataset이 있다고 하자.\n\nd = emotion['train'].select(range(4))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\nd.map()을 이용하여 아래와 같이 변환하라.\n\n\n\n\n\n\n\n\n데이터\n변환전\n변환후\n\n\n\n\nd[0]\ntext: strlabel: int\ntext: strlabel: intinput_ids: [int,…,int]\n\n\nd[:1]\ntext: [str]label: [int]\ntext: [str]label: [int]input_ids: [[int,…,int]]\n\n\n\n(풀이1)\n\ndef m_transform(example):\n    # example = {'text':xxx, 'label':yyy}\n    result = tokenizer(example['text'])\n    return result\n\n\nd2 = d.map(m_transform)\nd2 = d2.select_columns(['text', 'label', 'input_ids'])\nd2\n\nDataset({\n    features: ['text', 'label', 'input_ids'],\n    num_rows: 4\n})\n\n\n\nd2[0]\n\n{'text': 'i didnt feel humiliated',\n 'label': 0,\n 'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102]}\n\n\n\nd2[:1]\n\n{'text': ['i didnt feel humiliated'],\n 'label': [0],\n 'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102]]}\n\n\n(풀이2)\n\ndef m_transform(example):\n    # example = {'text':xxx, 'label':yyy}\n    result = tokenizer(example['text'])\n    del result['attention_mask']\n    return result\n\n\nd2 = d.map(m_transform)\nd2\n\nDataset({\n    features: ['text', 'label', 'input_ids'],\n    num_rows: 4\n})\n\n\n\nd2[0]\n\n{'text': 'i didnt feel humiliated',\n 'label': 0,\n 'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102]}\n\n\n\nd2[:1]\n\n{'text': ['i didnt feel humiliated'],\n 'label': [0],\n 'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102]]}\n\n\n#\n# 예제2 – text 제외\n아래와 같은 Dataset이 있다고 하자.\n\nd = emotion['train'].select(range(4))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\nd.map()을 이용하여 아래와 같이 변환하라.\n\n\n\n\n\n\n\n\n데이터\n변환전\n변환후\n\n\n\n\nd[0]\ntext: strlabel: int\nlabel: intinput_ids: [int,…,int]attention_mask: [int,…,int]\n\n\nd[:1]\ntext: [str]label: [int]\nlabel: [int]input_ids: [[int,…,int]]attention_mask: [[int,…,int]]\n\n\n\n(풀이1)\n\ndef m_transform(example):\n    # example = {'text': xxx, 'label':yyy}\n    result = tokenizer(example['text'])\n    del example['text']\n    return result\n\n\nd2 = d.map(m_transform)\nd2\n\nDataset({\n    features: ['label', 'input_ids', 'attention_mask'],\n    num_rows: 4\n})\n\n\n\nd2[0]\n\n{'label': 0,\n 'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n\n\n\nd2[:1]\n\n{'label': [0],\n 'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}\n\n\n(풀이2)\n\ndef m_transform(example):\n    # example = {'text': xxx, 'label':yyy}\n    result = tokenizer(example['text'])\n    return result\n\n\nd2 = d.map(m_transform)\nd2 = d2.select_columns(['label', 'input_ids', 'attention_mask'])\nd2 \n\nDataset({\n    features: ['label', 'input_ids', 'attention_mask'],\n    num_rows: 4\n})\n\n\n\nd2[0]\n\n{'label': 0,\n 'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n\n\n\nd2[:1]\n\n{'label': [0],\n 'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}\n\n\n#\n:::{.callout-note}\n.map()에서 컬럼을 제외하려면?\n\ndel을 이용한 풀이: 제외하고자 하는 column이 example에 있을 경우, result에 있을 경우 미묘하게 다름.\nselect를 이용한 풀이: 제외하고자 하는 column이 example에 있든지 result에 있든지 상관없음."
  },
  {
    "objectID": "posts/10wk-1.html#e.-d.map-타입변환-star",
    "href": "posts/10wk-1.html#e.-d.map-타입변환-star",
    "title": "10wk-1: Datasets 클래스",
    "section": "E. d.map() + 타입변환 (\\(\\star\\))",
    "text": "E. d.map() + 타입변환 (\\(\\star\\))\n# 예제1 – .map()을 이용한 타입변환은 불가능\n아래와 같은 Dataset이 있다고 하자.\n\nd = emotion['train'].select(range(4))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\nd.map()을 이용하여 아래와 같이 변환하라.\n\n\n\n\n\n\n\n\n데이터\n변환전\n변환후\n\n\n\n\nd[0]\ntext: strlabel: int\nlabel: intinput_ids: tensor([int,…,int])attention_mask: tensor([int,…,int])\n\n\nd[:1]\ntext: [str]label: [int]\nlabel:[int]input_ids: tensor([[int,…,int]])attention_mask: tensor([[int,…,int]])\n\n\nd[:2]\ntext: [str,str]label: [int,int]\nlabel:[int,int]input_ids: tensor([[int,…,int],[int,…,int]])attention_mask: tensor([[int,…,int],[int,…,int]])\n\n\n\n(풀이1) – 실패\n\ndef m_transform(example):\n    # example = {'text': xxx, 'label':yyy}\n    result = tokenizer(example['text'])\n    del example['text']\n    result['input_ids'] = torch.tensor(result['input_ids'])\n    result['attention_mask'] = torch.tensor(result['attention_mask'])\n    return result\n\n\nd2 = d.map(m_transform)\n\n\nd2[0]\n\n{'label': 0,\n 'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n\n\n\nd2[:1]\n\n{'label': [0],\n 'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}\n\n\n(풀이2) – 실패\n\ndef m_transform(example):\n    # example = {'text': xxx, 'label':yyy}\n    result = tokenizer(example['text'],return_tensors='pt')\n    del example['text']\n    return result\n\n\nd2 = d.map(m_transform)\nd2\n\nDataset({\n    features: ['label', 'input_ids', 'attention_mask'],\n    num_rows: 4\n})\n\n\n\nd2[0]\n\n{'label': 0,\n 'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}\n\n\n\nd2[:1]\n\n{'label': [0],\n 'input_ids': [[[101, 1045, 2134, 2102, 2514, 26608, 102]]],\n 'attention_mask': [[[1, 1, 1, 1, 1, 1, 1]]]}\n\n\n\n도데체 왜 자료형을 안바꿔주는거야??\n\n(풀이3) – 이것도 실패한다고?\n\nlst = d.to_list()\nlst\n\n[{'text': 'i didnt feel humiliated', 'label': 0},\n {'text': 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n  'label': 0},\n {'text': 'im grabbing a minute to post i feel greedy wrong', 'label': 3},\n {'text': 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property',\n  'label': 2}]\n\n\n\nlst2 = [] \nfor l in lst:\n    result = tokenizer(l['text'])\n    result['input_ids'] = torch.tensor(result['input_ids'])\n    result['attention_mask'] = torch.tensor(result['attention_mask'])\n    del l['text']\n    lst2.append(l|result)\nlst2\n\n[{'label': 0, 'input_ids': tensor([  101,  1045,  2134,  2102,  2514, 26608,   102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1])},\n {'label': 0, 'input_ids': tensor([  101,  1045,  2064,  2175,  2013,  3110,  2061, 20625,  2000,  2061,\n          9636, 17772,  2074,  2013,  2108,  2105,  2619,  2040, 14977,  1998,\n          2003,  8300,   102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])},\n {'label': 3, 'input_ids': tensor([  101, 10047,  9775,  1037,  3371,  2000,  2695,  1045,  2514, 20505,\n          3308,   102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])},\n {'label': 2, 'input_ids': tensor([  101,  1045,  2572,  2412,  3110, 16839,  9080, 12863,  2055,  1996,\n         13788,  1045,  2097,  2113,  2008,  2009,  2003,  2145,  2006,  1996,\n          3200,   102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}]\n\n\n\nd2 = datasets.Dataset.from_list(lst2)\nd2\n\nDataset({\n    features: ['label', 'input_ids', 'attention_mask'],\n    num_rows: 4\n})\n\n\n\nd2[0]\n\n{'label': 0,\n 'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n\n\n\nd2[:1]\n\n{'label': [0],\n 'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}\n\n\n(풀이4) – 성공??\n\ndef m_transform(example):\n    # example = {'text': xxx, 'label': yyy}\n    result = tokenizer(example['text'])\n    del example['text']\n    return result\n\n\nd2 = d.map(m_transform)\nd2.set_format(type=\"pt\",columns=['input_ids','attention_mask'],output_all_columns=True)\n\n\nd2[0]\n\n{'input_ids': tensor([  101,  1045,  2134,  2102,  2514, 26608,   102]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1]),\n 'label': 0}\n\n\n\nd2[:1]\n\n{'input_ids': tensor([[  101,  1045,  2134,  2102,  2514, 26608,   102]]),\n 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]]),\n 'label': [0]}\n\n\n\nd2[:2]['input_ids'] \n\n[tensor([  101,  1045,  2134,  2102,  2514, 26608,   102]),\n tensor([  101,  1045,  2064,  2175,  2013,  3110,  2061, 20625,  2000,  2061,\n          9636, 17772,  2074,  2013,  2108,  2105,  2619,  2040, 14977,  1998,\n          2003,  8300,   102])]\n\n\n#"
  },
  {
    "objectID": "posts/10wk-1.html#a.-d.with_transform",
    "href": "posts/10wk-1.html#a.-d.with_transform",
    "title": "10wk-1: Datasets 클래스",
    "section": "A. d.with_transform()",
    "text": "A. d.with_transform()\n# 예제1\n아래와 같은 Dataset이 있다고 하자.\n\nd = emotion['train'].select(range(4))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\nd.with_transform()을 이용하여 아래와 같이 변환하라.\n\n\n\n\n\n\n\n\n데이터\n변환전\n변환후\n\n\n\n\nd[0]\ntext: strlabel: int\ntext: strlabel: intinput_ids: [int,…,int]attention_mask: [int,…,int]\n\n\nd[:1]\ntext: [str]label: [int]\ntext: [str]label: [int]input_ids: [[int,…,int]]attention_mask: [[int,…,int]]\n\n\n\n(풀이1)\n\ndct = d.to_dict()\ndct\n\n{'text': ['i didnt feel humiliated',\n  'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n  'im grabbing a minute to post i feel greedy wrong',\n  'i am ever feeling nostalgic about the fireplace i will know that it is still on the property'],\n 'label': [0, 0, 3, 2]}\n\n\n\nresult = tokenizer(d['text'])\ndct2 = dct | result\nd2 = datasets.Dataset.from_dict(dct2)\n\n\nd2[0]\n\n{'text': 'i didnt feel humiliated',\n 'label': 0,\n 'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n\n\n\nd2[:1]\n\n{'text': ['i didnt feel humiliated'],\n 'label': [0],\n 'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}\n\n\n(풀이2)\n\ndef w_transform(examples): \n    #examples = {'text':[xxx,xxxx,...], 'label':[yyy,yyyy,...]}\n    result = tokenizer(examples['text'])\n    result = examples | result\n    return result\n\n\nd2 = d.with_transform(w_transform)\nd2\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n\nd2[0]\n\n{'text': 'i didnt feel humiliated',\n 'label': 0,\n 'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n\n\n\nd2[0:1]\n\n{'text': ['i didnt feel humiliated'], 'label': [0], 'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}\n\n\n#\n:::{.callout-note}\n.with_transform()와 .map()의 차이점\n\n.map()은 입력으로 example꼴을, .with_transform()은 입력으로 examples를 기대한다.\n.map()은 변환전과 변환후 데이터가 자동으로 합쳐진다. .with_transform()은 변환후 데이터만 살아남는다.\n.map()은 변환이 실제로 이루어진다. .with_transform()은 변환이 실제로 이루어지지 않다가 d[0],d[:1] 등이 실행하는 순간 이루어진다.\n\n# 예제2\n아래와 같은 Dataset이 있다고 하자.\n\nd = emotion['train'].select(range(4))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\nd.with_transform()을 이용하여 아래와 같이 변환하라.\n\n\n\n\n\n\n\n\n데이터\n변환전\n변환후\n\n\n\n\nd[0]\ntext: strlabel: int\ntext: strlabel: intinput_ids: [int,…,int]\n\n\nd[:1]\ntext: [str]label: [int]\ntext: [str]label: [int]input_ids: [[int,…,int]]\n\n\n\n(풀이)\n\ndef w_transform(examples):\n    # examples = {'text':[xxx,xxxx,....], 'label':[yyy,yyyy,...]}\n    result = tokenizer(examples['text'])\n    del result['attention_mask']\n    result['text'] = examples['text']\n    result['label'] = examples['label']\n    return result\n\n\nd2 = d.with_transform(w_transform)\nd2\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n\nd2[0]\n\n{'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102],\n 'text': 'i didnt feel humiliated',\n 'label': 0}\n\n\n\nd2[:1]\n\n{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102]], 'text': ['i didnt feel humiliated'], 'label': [0]}\n\n\n#\n# 예제3\n아래와 같은 Dataset이 있다고 하자.\n\nd = emotion['train'].select(range(4))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\nd.with_transform()을 이용하여 아래와 같이 변환하라.\n\n\n\n\n\n\n\n\n데이터\n변환전\n변환후\n\n\n\n\nd[0]\ntext: strlabel: int\nlabel: intinput_ids: [int,…,int]attention_mask: [int,…,int]\n\n\nd[:1]\ntext: [str]label: [int]\nlabel: [int]input_ids: [[int,…,int]]attention_mask: [[int,…,int]]\n\n\n\n(풀이)\n\ndef w_transform(examples):\n    # examples = {'text':[xxx,xxxx,....], 'label':[yyy,yyyy,...]}\n    result = tokenizer(examples['text'])\n    result['label'] = examples['label']\n    return result\n\n\nd2 = d.with_transform(w_transform)\nd2\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n\nd2[0]\n\n{'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1],\n 'label': 0}\n\n\n\nd2[:1]\n\n{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]], 'label': [0]}\n\n\n#\n# 예제4\n아래와 같은 Dataset이 있다고 하자.\n\nd = emotion['train'].select(range(4))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\nd.with_transform()을 이용하여 아래와 같이 변환하라.\n\n\n\n\n\n\n\n\n데이터\n변환전\n변환후\n\n\n\n\nd[0]\ntext: strlabel: int\nlabel: tensor(int)input_ids: tensor([int,…,int])attention_mask: tensor([int,…,int])\n\n\nd[:1]\ntext: [str]label: [int]\nlabel: tensor([int])input_ids: tensor([[int,…,int]])attention_mask: tensor([[int,…,int]])\n\n\n\n(풀이) – 실패\n\ndef w_transform(examples):\n    # examples = {'text':[xxx,xxxx,...],'label':[yyy,yyyy,...]}\n    result = tokenizer(examples['text'])\n    result['label'] = torch.tensor(examples['label'])\n    result['input_ids'] = torch.tensor(result['input_ids'])\n    result['attention_mask'] = torch.tensor(result['attention_mask'])\n    return result \n\n\nd2 = d.with_transform(w_transform)\nd2\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n\nd2[0]\n\n{'input_ids': tensor([  101,  1045,  2134,  2102,  2514, 26608,   102]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1]),\n 'label': tensor(0)}\n\n\n\nd2[:1]\n\n{'input_ids': tensor([[  101,  1045,  2134,  2102,  2514, 26608,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]]), 'label': tensor([0])}\n\n\n\nd2[:2]\n\nValueError: expected sequence of length 7 at dim 1 (got 23)\n\n\n에러나는 이유\n\nexamples = d[:2]\nexamples\n\n{'text': ['i didnt feel humiliated',\n  'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake'],\n 'label': [0, 0]}\n\n\n\nresult = tokenizer(examples['text'])\nresult\n\n{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n\n\n\ntorch.tensor(examples['label'])\n\ntensor([0, 0])\n\n\n\nresult['label'] = torch.tensor(examples['label'])\nresult\n\n{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'label': tensor([0, 0])}\n\n\n\ntorch.tensor(result['input_ids'])\n\nValueError: expected sequence of length 7 at dim 1 (got 23)\n\n\n\n패딩….\n\n(풀이2) – 성공\n\ndef w_transform(examples):\n    # examples = {'text':[xxx,xxxx,...],'label':[yyy,yyyy,...]}\n    result = tokenizer(examples['text'],padding=True)\n    result['label'] = torch.tensor(examples['label'])\n    result['input_ids'] = torch.tensor(result['input_ids'])\n    result['attention_mask'] = torch.tensor(result['attention_mask'])\n    return result \n\n\nd2 = d.with_transform(w_transform)\nd2\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n\nd2[0]\n\n{'input_ids': tensor([  101,  1045,  2134,  2102,  2514, 26608,   102]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1]),\n 'label': tensor(0)}\n\n\n\nd2[:1]\n\n{'input_ids': tensor([[  101,  1045,  2134,  2102,  2514, 26608,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]]), 'label': tensor([0])}\n\n\n\nd2[:2]\n\n{'input_ids': tensor([[  101,  1045,  2134,  2102,  2514, 26608,   102,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [  101,  1045,  2064,  2175,  2013,  3110,  2061, 20625,  2000,  2061,\n          9636, 17772,  2074,  2013,  2108,  2105,  2619,  2040, 14977,  1998,\n          2003,  8300,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'label': tensor([0, 0])}\n\n\n(풀이3) – 이것도 성공..\n\ndef w_transform(examples):\n    # examples = {'text':[xxx,xxxx,...],'label':[yyy,yyyy,...]}\n    result = tokenizer(examples['text'],padding=True,return_tensors=\"pt\")\n    result['label'] = torch.tensor(examples['label'])\n    return result \n\n\nd2 = d.with_transform(w_transform)\nd2\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n\nd2[0]\n\n{'input_ids': tensor([  101,  1045,  2134,  2102,  2514, 26608,   102]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1]),\n 'label': tensor(0)}\n\n\n\nd2[:1]\n\n{'input_ids': tensor([[  101,  1045,  2134,  2102,  2514, 26608,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]]), 'label': tensor([0])}\n\n\n\nd2[:2]\n\n{'input_ids': tensor([[  101,  1045,  2134,  2102,  2514, 26608,   102,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0],\n        [  101,  1045,  2064,  2175,  2013,  3110,  2061, 20625,  2000,  2061,\n          9636, 17772,  2074,  2013,  2108,  2105,  2619,  2040, 14977,  1998,\n          2003,  8300,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'label': tensor([0, 0])}\n\n\n#"
  },
  {
    "objectID": "posts/10wk-1.html#b.-dd.with_transform",
    "href": "posts/10wk-1.html#b.-dd.with_transform",
    "title": "10wk-1: Datasets 클래스",
    "section": "B. dd.with_transform()",
    "text": "B. dd.with_transform()\n# 예제1\n아래와 같은 DatasetDict가 있다고 하자.\n\ndd = datasets.DatasetDict({\n    'train':emotion['train'].select(range(4)),\n    'test':emotion['test'].select(range(4)),\n})\ndd\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 4\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 4\n    })\n})\n\n\ndd.map()을 이용하여 아래와 같이 변환하라.\n\n\n\n\n\n\n\n\n\ntr/test\n데이터\n변환전\n변환후\n\n\n\n\ntrain\nd[0]\ntext: strlabel: int\ntext: strlabel: intinput_ids: [int,…,int]attention_mask: [int,…,int]\n\n\ntrain\nd[:1]\ntext: [str]label: [int]\ntext: [str]label: [int]input_ids: [[int,…,int]]attention_mask: [[int,…,int]]\n\n\ntest\nd[0]\ntext: strlabel: int\ntext: strlabel: intinput_ids: [int,…,int]attention_mask: [int,…,int]\n\n\ntest\nd[:1]\ntext: [str]label: [int]\ntext: [str]label: [int]input_ids: [[int,…,int]]attention_mask: [[int,…,int]]\n\n\n\n(풀이)\n\ndef w_transform(examples):\n    # examples = {'text':[xxx,xxxx,...], 'label':[yyy,yyyy,....]}\n    result = tokenizer(examples['text'])\n    result = examples | result \n    return result \n\n\ndd2 = dd.with_transform(w_transform)\ndd2\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 4\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 4\n    })\n})\n\n\n\ndd2['train'][0]\n\n{'text': 'i didnt feel humiliated',\n 'label': 0,\n 'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n\n\n\ndd2['train'][:1]\n\n{'text': ['i didnt feel humiliated'], 'label': [0], 'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}\n\n\n\ndd2['test'][0]\n\n{'text': 'im feeling rather rotten so im not very ambitious right now',\n 'label': 0,\n 'input_ids': [101,\n  10047,\n  3110,\n  2738,\n  11083,\n  2061,\n  10047,\n  2025,\n  2200,\n  12479,\n  2157,\n  2085,\n  102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\ndd2['test'][:1]\n\n{'text': ['im feeling rather rotten so im not very ambitious right now'], 'label': [0], 'input_ids': [[101, 10047, 3110, 2738, 11083, 2061, 10047, 2025, 2200, 12479, 2157, 2085, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n\n\n#"
  },
  {
    "objectID": "posts/10wk-1.html#c.-d.reset_format",
    "href": "posts/10wk-1.html#c.-d.reset_format",
    "title": "10wk-1: Datasets 클래스",
    "section": "C. d.reset_format()",
    "text": "C. d.reset_format()\n# 예시1\n\nd = emotion['train'].select(range(4))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n\ndef w_transform(examples):\n    # examples = {'text':[xxx,xxxx,...], 'label':[yyy,yyyy,...]}\n    result = tokenizer(examples['text'])\n    return result \n\n\nd2 = d.with_transform(w_transform)\nd2\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n\nd2[0]\n\n{'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n\n\n\nd2[:1]\n\n{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}\n\n\n\nd2.reset_format()\n\n\nd2[0]\n\n{'text': 'i didnt feel humiliated', 'label': 0}\n\n\n\nd2[0:1]\n\n{'text': ['i didnt feel humiliated'], 'label': [0]}\n\n\n#\n# 예시2\n\nd = emotion['train'].select(range(4))\nd\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n\ndef w_transform(examples):\n    # examples = {'text':[xxx,xxxx,...], 'label':[yyy,yyyy,...]}\n    result = tokenizer(examples['text'],padding=True)\n    result['label'] = examples['label']\n    return result \n\n\nd2 = d.with_transform(w_transform)\nd2\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n\n\n\nd2[0]\n\n{'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1],\n 'label': 0}\n\n\n\nd2[:1]\n\n{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]], 'label': [0]}\n\n\n\nd2[:2]\n\n{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'label': [0, 0]}\n\n\n\nd2.set_format(type=\"pt\")\n\n\nd2[0]\n\n{'text': 'i didnt feel humiliated', 'label': tensor(0)}\n\n\n\nd2[:1]\n\n{'text': ['i didnt feel humiliated'], 'label': tensor([0])}\n\n\n\nd2[:2]\n\n{'text': ['i didnt feel humiliated',\n  'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake'],\n 'label': tensor([0, 0])}\n\n\n#\n\n\n\n\n\n\nNote\n\n\n\n.with_transform 은 .set_format 궁합이 안맞음"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/08wk-2.html#a.-공통점",
    "href": "posts/08wk-2.html#a.-공통점",
    "title": "08wk-2: numpy와 torch의 차이",
    "section": "A. 공통점",
    "text": "A. 공통점\n- 기본마인드: numpy에서 가능한건 torch에서도 가능하다고 생각하면 된다.\n- 벡터선언 후 브로드캐스팅\n\nnp.array([1,2,3]) + 1\n\narray([2, 3, 4])\n\n\n\ntorch.tensor([1,2,3]) + 1\n\ntensor([2, 3, 4])\n\n\n- 형태변환\ntorch.tensor의 형태변환\n\ntsr1 = torch.tensor([1,2,3])\nprint(\n    f\"텐서값 = {tsr1}\\n\"\n    f\"shape = {tsr1.shape}\"\n)\n\n텐서값 = tensor([1, 2, 3])\nshape = torch.Size([3])\n\n\n\ntsr2 = tsr1.reshape(3,1)\nprint(\n    f\"텐서값 = {tsr2}\\n\"\n    f\"shape = {tsr2.shape}\"\n)\n\n텐서값 = tensor([[1],\n        [2],\n        [3]])\nshape = torch.Size([3, 1])\n\n\n\ntsr3 = tsr1.reshape(1,3)\nprint(\n    f\"텐서값 = {tsr3}\\n\"\n    f\"shape = {tsr3.shape}\"\n)\nensor([1, 2, 3])\n\n텐서값 = tensor([[1, 2, 3]])\nshape = torch.Size([1, 3])\n\n\nnp.array의 형태변환\n\nnp.array([1,2,3]).reshape(1,3)\n\narray([[1, 2, 3]])\n\n\n\nnp.array([1,2,3]).reshape(3,1)\n\narray([[1],\n       [2],\n       [3]])\n\n\n- 기타함수들도 비슷하게 운용됨\ntorch.tensor에서의 stack\n\ntsr1 = torch.tensor([1,2,3])\ntsr2 = torch.tensor([2,3,4])\n\n\ntorch.stack([tsr1,tsr2],axis=0)\n\ntensor([[1, 2, 3],\n        [2, 3, 4]])\n\n\n\ntorch.stack([tsr1,tsr2],axis=1)\n\ntensor([[1, 2],\n        [2, 3],\n        [3, 4]])\n\n\nnp.array에서의 stack\n\narr1 = np.array([1,2,3])\narr2 = np.array([2,3,4])\n\n\nnp.stack([arr1,arr2],axis=0)\n\narray([[1, 2, 3],\n       [2, 3, 4]])\n\n\n\nnp.stack([arr1,arr2],axis=1)\n\narray([[1, 2],\n       [2, 3],\n       [3, 4]])"
  },
  {
    "objectID": "posts/08wk-2.html#b.-차이점1-기본자료형",
    "href": "posts/08wk-2.html#b.-차이점1-기본자료형",
    "title": "08wk-2: numpy와 torch의 차이",
    "section": "B. 차이점1 – 기본자료형",
    "text": "B. 차이점1 – 기본자료형\n- 1/3와 같은 숫자를 저장할때 넘파이의 경우 기본 data type은 float64임.\n\na = np.array(1/3)\na, a.dtype\n\n(array(0.33333333), dtype('float64'))\n\n\n\nprint(\n    f\"값: {a}\\n\"\n    f\"dtype: {a.dtype}\"\n)\n\n값: 0.3333333333333333\ndtype: float64\n\n\n- 그런데 텐서는 기본 data type이 float32임. (그래서 정확하게 저장되지 않음)\n\na = torch.tensor(1/3)\na, a.dtype\n\n(tensor(0.3333), torch.float32)\n\n\n\nprint(\n    f\"값: {a}\\n\"\n    f\"dtype: {a.dtype}\"\n)\n\n값: 0.3333333432674408\ndtype: torch.float32\n\n\n- 억지로 저장할 수는 있음 (그런데 이럴 경우 dtype 꼬리표가 붙음)\n\na = torch.tensor(1/3,dtype=torch.float64)\na, a.dtype\n\n(tensor(0.3333, dtype=torch.float64), torch.float64)\n\n\n\nprint(\n    f\"값: {a}\\n\"\n    f\"dtype: {a.dtype}\"\n)\n\n값: 0.3333333333333333\ndtype: torch.float64\n\n\n- 자료형 변환시 이러한 기분나쁜 꼬리표가 붙을 수 있음\n# 예시1\n\na = np.array(1/3)\na\n\narray(0.33333333)\n\n\n\ntorch.tensor(a)\n\ntensor(0.3333, dtype=torch.float64)\n\n\n#\n# 예시2\n\na = torch.tensor(1/3)\na\n\ntensor(0.3333)\n\n\n\nnp.array(a)\n\narray(0.33333334, dtype=float32)\n\n\n#\n- torch의 경우 float64로 저장되면 모델이 안돌아갈 수 있으니 (메모리를 많이 차지해서) 반드시 dtype을 float32로 바꾸는 습관을 가지면 좋음\n\na = np.array(1/3)\ntsr = torch.tensor(a)\ntsr\n\ntensor(0.3333, dtype=torch.float64)\n\n\n\ntsr.float() # 바꾸는방법1\n\ntensor(0.3333)\n\n\n\ntsr.to(torch.float32) # 바꾸는방법2\n\ntensor(0.3333)\n\n\n- numpy는 할줄알죠?\n\narr = np.array(1/3,dtype=np.float32)\narr\n\narray(0.33333334, dtype=float32)\n\n\n\narr.astype(np.float64)\n\narray(0.33333334)\n\n\n- torch에서 data type을 바꿔주는 메소드는 상당히 유용합니다. (아까 말했듯이 형식을 잘 못 맞추면 코드가 안돌아가요)\n\na = torch.tensor([1,0,1])\na, a.float()\n\n(tensor([1, 0, 1]), tensor([1., 0., 1.]))\n\n\n\na = torch.tensor([1,0,1])\na, a.bool()\n\n(tensor([1, 0, 1]), tensor([ True, False,  True]))\n\n\n\na = torch.tensor([1.0,0,1])\na, a.long()\n\n(tensor([1., 0., 1.]), tensor([1, 0, 1]))\n\n\n- torch에서 data type 이 중요한이유?\n\n의미상 맞는 코드인데, dtype이 안맞으면 실행이 안될 수 있다."
  },
  {
    "objectID": "posts/08wk-2.html#c.-차이점2-메소드차이",
    "href": "posts/08wk-2.html#c.-차이점2-메소드차이",
    "title": "08wk-2: numpy와 torch의 차이",
    "section": "C. 차이점2 – 메소드차이",
    "text": "C. 차이점2 – 메소드차이\n- torch 와 numpy의 메소드(=자료에 내장된 특수함수)들이 완전히 같지는 않음.\n\ntsr = torch.tensor([1,2,3])\ntsr\n\ntensor([1, 2, 3])\n\n\n\ntsr.numpy() # np.array로 바꿔주는 메소드\n\narray([1, 2, 3])\n\n\n\ntsr.float() # 자료형을 float으로 바꿔주는 메소드\n\ntensor([1., 2., 3.])\n\n\n\narr = np.array([1,2,3])\narr\n\narray([1, 2, 3])\n\n\n\narr.numpy()\n\nAttributeError: 'numpy.ndarray' object has no attribute 'numpy'\n\n\n\narr.float()\n\nAttributeError: 'numpy.ndarray' object has no attribute 'float'"
  },
  {
    "objectID": "posts/08wk-2.html#d.-차이점3-미분연산지원-star",
    "href": "posts/08wk-2.html#d.-차이점3-미분연산지원-star",
    "title": "08wk-2: numpy와 torch의 차이",
    "section": "D. 차이점3 – 미분연산지원 (\\(\\star\\))",
    "text": "D. 차이점3 – 미분연산지원 (\\(\\star\\))\n# 예제1 – 파이썬을 기본문법을 이용한 미분\n\ndef f(x):\n    return x**2\n\n\na = 2\nh = 0.001 \n(f(a+h)-f(a))/h \n\n4.000999999999699\n\n\n#\n# 예제2 – torch를 이용한 미분\n\ndef f(x):\n    return x**2\na = torch.tensor(2.0, requires_grad=True)\nb = f(a)\nb.backward()\na.grad\n\ntensor(4.)\n\n\n#\n# 예제3\n\na = torch.tensor(3.0, requires_grad=True)\na # 토치텐서... 그런데 이제 미분꼬리표를 곁들인..\n\ntensor(3., requires_grad=True)\n\n\n\nprint(a.grad) # a에서의 기울기\n\nNone\n\n\n\nb = f(a) # b=a**2 \nb # 여기에도 꼬리표같은게 붙어있음..\n\ntensor(9., grad_fn=&lt;PowBackward0&gt;)\n\n\n\nc = 2*b # c = 2*a**2\nc # 여기에도 꼬리표같은게 붙어있음.. \n\ntensor(18., grad_fn=&lt;MulBackward0&gt;)\n\n\n\nc.backward() # c=2b=2a**2 를 미분하세요... 뭐로??? 미분꼬리표의 근원인 a로!\n\n\na,a.grad # a값과 a에서의 기울기 \n\n(tensor(3., requires_grad=True), tensor(12.))\n\n\n\n\\(b=f(a)=a^2\\)\n\\(c=2b=2f(a)=2a^2\\)\n\\(c\\)를 \\(a\\)로 미분하면 \\(4a\\)가 된다..\n\n#\n# 예제4\n\na = torch.tensor(3.0, requires_grad=True)\na\n\ntensor(3., requires_grad=True)\n\n\n\nprint(a.grad)\n\nNone\n\n\n\nb = f(a)\nb\n\ntensor(9., grad_fn=&lt;PowBackward0&gt;)\n\n\n\nc = 2*b\n\n\nb.backward() # b를미분하세요.. 뭐로?? 미분꼬리표의 근원인 a로..\n\n\na, a.grad\n\n(tensor(3., requires_grad=True), tensor(6.))\n\n\n\n\\(a=3\\)\n\\(b=f(a)=a^2\\)\n\\(b\\)를 \\(a\\)로 미분하면 \\(2a\\) 가 된다..\n\n#\n# 예제5\n\na = torch.tensor(3.0,requires_grad=True)\na # 토치텐서.. 미분꼬리표가 곁들여진.. \n\ntensor(3., requires_grad=True)\n\n\n\nb = f(a) # 미분꼬리표가 b까지 따라옴\nb\n\ntensor(9., grad_fn=&lt;PowBackward0&gt;)\n\n\n\nc = 2*b # 미분꼬리표가 c까지 따라옴\nc\n\ntensor(18., grad_fn=&lt;MulBackward0&gt;)\n\n\n\nc = c.detach() # c에서 미분꼬리표를 제거 \nc\n\ntensor(18.)\n\n\n\nc.backward() # c=2b=2f(a) 를 미분하세요.. 뭐로?? 꼬리표의 근원으로..? 근데 꼬리표가 없어?? --&gt; 미분불가능\n\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n\n\n\nb.backward()\n\n\na,a.grad\n\n(tensor(3., requires_grad=True), tensor(6.))\n\n\n#"
  },
  {
    "objectID": "posts/08wk-2.html#e.-차이점4-cuda-연산지원-star",
    "href": "posts/08wk-2.html#e.-차이점4-cuda-연산지원-star",
    "title": "08wk-2: numpy와 torch의 차이",
    "section": "E. 차이점4 – cuda 연산지원 (\\(\\star\\))",
    "text": "E. 차이점4 – cuda 연산지원 (\\(\\star\\))\n- torch는 cuda을 지원해서 좋다.. (그런데 쓰는 입장에선 불편하다)\n- torch.tensor를 선언하고 그 값들을 cuda로 혹은 cpu로 옮겨보자..\n\na = torch.tensor([1.0, 2.0, 3.0])\nb = torch.tensor([2.0, 3.0, 4.0])\na,b\n\n(tensor([1., 2., 3.]), tensor([2., 3., 4.]))\n\n\n\na.to(\"cuda\") \n\ntensor([1., 2., 3.], device='cuda:0')\n\n\n\na # a.to(\"cuda\") 를 실행해도 a값 자체가 변하는건 아님.. \n\ntensor([1., 2., 3.])\n\n\n\na_cuda = a.to(\"cuda:0\")\nb_cuda = b.to(\"cuda:0\")\na_cuda,b_cuda\n\n(tensor([1., 2., 3.], device='cuda:0'), tensor([2., 3., 4.], device='cuda:0'))\n\n\n\na,b,a_cuda,b_cuda\n\n(tensor([1., 2., 3.]),\n tensor([2., 3., 4.]),\n tensor([1., 2., 3.], device='cuda:0'),\n tensor([2., 3., 4.], device='cuda:0'))\n\n\n\na_cuda.to(\"cpu\"),b_cuda.to(\"cpu\") # *.to(\"cpu\") 도 *의 값자체를 변화시키지 않음. \n\n(tensor([1., 2., 3.]), tensor([2., 3., 4.]))\n\n\n- 구분을 위해서 변수이름을 *_cpu, *_cuda와 같이정의하자.\n\na_cpu = torch.tensor([1.0, 2.0, 3.0])\nb_cpu = torch.tensor([2.0, 3.0, 4.0])\na_cuda = torch.tensor([1.0, 2.0, 3.0]).to(\"cuda\")\nb_cuda = torch.tensor([2.0, 3.0, 4.0]).to(\"cuda\")\n\n\na_cpu,b_cpu,a_cuda,b_cuda\n\n(tensor([1., 2., 3.]),\n tensor([2., 3., 4.]),\n tensor([1., 2., 3.], device='cuda:0'),\n tensor([2., 3., 4.], device='cuda:0'))\n\n\n- cuda는 cuda끼리, cpu는 cpu끼리 연산가능\n\na_cpu + b_cpu\n\ntensor([3., 5., 7.])\n\n\n\ncpu끼리 연산하면 연산가능하고, 결과도 cpu에…\n\n\na_cpu + b_cuda\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n\n\n\na_cuda + b_cpu\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n\n\n\na_cuda + b_cuda\n\ntensor([3., 5., 7.], device='cuda:0')\n\n\n\ncuda끼리 연산하면 연산가능하고, 그 결과도 cuda에 존재함."
  },
  {
    "objectID": "posts/04wk-1.html#a.-트레이너의-제1역할-cpu에서-gpu로..",
    "href": "posts/04wk-1.html#a.-트레이너의-제1역할-cpu에서-gpu로..",
    "title": "04wk-1: 감성분석 파고들기 (2)",
    "section": "A. 트레이너의 제1역할 – CPU에서 GPU로..",
    "text": "A. 트레이너의 제1역할 – CPU에서 GPU로..\n\n# 트레이너 생성전\n- 인공지능의 파라메터 상태확인 1\n\n인공지능.classifier.weight\n\nParameter containing:\ntensor([[-0.0234,  0.0279,  0.0242,  ...,  0.0091, -0.0063, -0.0133],\n        [ 0.0087,  0.0007, -0.0099,  ...,  0.0183, -0.0007,  0.0295]],\n       requires_grad=True)\n\n\n\n중요한내용1: 숫자들 = 초기숫자들\n중요한내용2: 숫자들이 CPU에 존재한다는 의미\n\n- 인공지능을 이용한 예측 1\n\n확률계산하기(인공지능(**데이터콜렉터([토크나이저(\"This movie was a huge disappointment.\")])))\n\ntensor([[0.4642, 0.5358]], grad_fn=&lt;DivBackward0&gt;)\n\n\n\n확률계산하기(인공지능(**데이터콜렉터([토크나이저(\"This was a masterpiece.\")])))\n\ntensor([[0.4664, 0.5336]], grad_fn=&lt;DivBackward0&gt;)\n\n\n\n\n# 트레이너 생성후\n- 트레이너생성\n\n## Step3 \n트레이너세부지침 = 트레이너세부지침생성기(\n    output_dir=\"my_awesome_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2, # 전체문제세트를 2번 공부하라..\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n)\n트레이너 = 트레이너생성기(\n    model=인공지능,\n    args=트레이너세부지침,\n    train_dataset=전처리된훈련자료,\n    eval_dataset=전처리된검증자료,\n    tokenizer=토크나이저,\n    data_collator=데이터콜렉터,\n    compute_metrics=평가하기,\n)\n\n- 인공지능의 파라메터 상태확인 2\n\n인공지능.classifier.weight\n\nParameter containing:\ntensor([[-0.0234,  0.0279,  0.0242,  ...,  0.0091, -0.0063, -0.0133],\n        [ 0.0087,  0.0007, -0.0099,  ...,  0.0183, -0.0007,  0.0295]],\n       device='cuda:0', requires_grad=True)\n\n\n\n중요한내용1: 숫자들 = 초기숫자들\n중요한내용2: device=“cuda:0” // 숫자들이 GPU에 있다는 의미\n\n- 인공지능을 이용한 예측 2\n\n확률계산하기(인공지능(**데이터콜렉터([토크나이저(\"This movie was a huge disappointment.\")]).to(\"cuda:0\")))\n\ntensor([[0.4642, 0.5358]], device='cuda:0', grad_fn=&lt;DivBackward0&gt;)\n\n\n\n확률계산하기(인공지능(**데이터콜렉터([토크나이저(\"This was a masterpiece.\")]).to(\"cuda:0\")))\n\ntensor([[0.4664, 0.5336]], device='cuda:0', grad_fn=&lt;DivBackward0&gt;)\n\n\n\n트레이너의 제1역할: 트레이너는 생성과 동시에 하는역할이 있는데, 바로 인공지능의 파라메터를 GPU에 올리는 것이다."
  },
  {
    "objectID": "posts/04wk-1.html#b.-트레이너의-제2역할-예측하기",
    "href": "posts/04wk-1.html#b.-트레이너의-제2역할-예측하기",
    "title": "04wk-1: 감성분석 파고들기 (2)",
    "section": "B. 트레이너의 제2역할 – 예측하기",
    "text": "B. 트레이너의 제2역할 – 예측하기\n\n트레이너의 제2역할: 트레이너.predict() 사용가능. 트레이너.predict()의 입력형태는 input_ids, attention_mask, label 이 존재하는 Dataset\n\n# 예제1 트레이너를 이용한 예측\n\nsample_dict = {\n    'text': [\"This movie was a huge disappointment.\"],\n    'label': [0],\n    'input_ids': [[101, 2023, 3185, 2001, 1037, 4121, 10520, 1012, 102]],\n    'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]\n}\nsample_dataset = datasets.Dataset.from_dict(sample_dict)\nsample_dataset\n\nDataset({\n    features: ['text', 'label', 'input_ids', 'attention_mask'],\n    num_rows: 1\n})\n\n\n\n트레이너.predict(sample_dataset)\n\n\n\n\nPredictionOutput(predictions=array([[-0.11731032,  0.02610314]], dtype=float32), label_ids=array([0]), metrics={'test_loss': 0.7674226760864258, 'test_model_preparation_time': 0.0009, 'test_accuracy': 0.0, 'test_runtime': 1.1868, 'test_samples_per_second': 0.843, 'test_steps_per_second': 0.843})\n\n\n\nlogits = np.array([[-0.11731032,  0.02610314]])\nnp.exp(logits)/ np.exp(logits).sum(axis=1)\n\narray([[0.46420796, 0.53579204]])\n\n\n#\n# 예제2 – 트레이너를 이용하여 train_data, test_data 의 prediction 값을 구하라.\n\n트레이너.predict(전처리된데이터['train'])\n\n\n\n\nPredictionOutput(predictions=array([[-0.08470809,  0.0023939 ],\n       [-0.08299972,  0.02850237],\n       [-0.06004279,  0.01801764],\n       ...,\n       [-0.02317078, -0.01451463],\n       [-0.00802051, -0.02698467],\n       [-0.03900156, -0.02573229]], dtype=float32), label_ids=array([0, 0, 0, ..., 1, 1, 1]), metrics={'test_loss': 0.6949957609176636, 'test_model_preparation_time': 0.0009, 'test_accuracy': 0.4916, 'test_runtime': 89.9353, 'test_samples_per_second': 277.978, 'test_steps_per_second': 17.379})\n\n\n\n트레이너.predict(전처리된데이터['test'])\n\n\n\n\nPredictionOutput(predictions=array([[-0.03815563,  0.00212397],\n       [-0.08166712, -0.00432102],\n       [-0.10371644,  0.03148611],\n       ...,\n       [-0.08171435, -0.00681646],\n       [-0.09139054,  0.01050513],\n       [-0.06704493,  0.0221498 ]], dtype=float32), label_ids=array([0, 0, 0, ..., 1, 1, 1]), metrics={'test_loss': 0.6949934363365173, 'test_model_preparation_time': 0.0009, 'test_accuracy': 0.4912, 'test_runtime': 89.8578, 'test_samples_per_second': 278.217, 'test_steps_per_second': 17.394})\n\n\n#"
  },
  {
    "objectID": "posts/04wk-1.html#c.-트레이너의-제3역할-학습-및-결과저장",
    "href": "posts/04wk-1.html#c.-트레이너의-제3역할-학습-및-결과저장",
    "title": "04wk-1: 감성분석 파고들기 (2)",
    "section": "C. 트레이너의 제3역할 – 학습 및 결과저장",
    "text": "C. 트레이너의 제3역할 – 학습 및 결과저장\n\n# 학습\n\n트레이너.train()\n\n\n    \n      \n      \n      [3126/3126 11:57, Epoch 2/2]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nModel Preparation Time\nAccuracy\n\n\n\n\n1\n0.221900\n0.199376\n0.001900\n0.923000\n\n\n2\n0.145700\n0.233206\n0.001900\n0.931000\n\n\n\n\n\n\nTrainOutput(global_step=3126, training_loss=0.20330691871472223, metrics={'train_runtime': 718.1146, 'train_samples_per_second': 69.627, 'train_steps_per_second': 4.353, 'total_flos': 6556904415524352.0, 'train_loss': 0.20330691871472223, 'epoch': 2.0})\n\n\n\n25000 / 16 \n\n1562.5\n\n\n\n1563 * 2 \n\n3126\n\n\n\n\n# 학습후\n- 인공지능이 똑똑해졌을까?\n- 인공지능의 파라메터 상태확인 3\n\n인공지능.classifier.weight\n\nParameter containing:\ntensor([[-0.0230,  0.0279,  0.0239,  ...,  0.0085, -0.0062, -0.0143],\n        [ 0.0084,  0.0007, -0.0097,  ...,  0.0189, -0.0008,  0.0304]],\n       device='cuda:0', requires_grad=True)\n\n\n인공지능의 파라메터 상태확인 2와 비교삿\nParameter containing:\ntensor([[-0.0234,  0.0279,  0.0242,  ...,  0.0091, -0.0063, -0.0133],\n        [ 0.0087,  0.0007, -0.0099,  ...,  0.0183, -0.0007,  0.0295]],\n       device='cuda:0', requires_grad=True)\n숫자들이 바뀐걸 확인 \\(\\to\\) 뭔가 다른 계산결과를 준다는 의미겠지? \\(\\to\\) 진짜 그런지 보자..\n- 인공지능을 이용한 예측 3\n\n확률계산하기(인공지능(**데이터콜렉터([토크나이저(\"This movie was a huge disappointment.\")]).to(\"cuda:0\")))\n\ntensor([[0.9885, 0.0115]], device='cuda:0', grad_fn=&lt;DivBackward0&gt;)\n\n\n\n확률계산하기(인공지능(**데이터콜렉터([토크나이저(\"This was a masterpiece.\")]).to(\"cuda:0\")))\n\ntensor([[0.0219, 0.9781]], device='cuda:0', grad_fn=&lt;DivBackward0&gt;)\n\n\n- 다시 트레이너를 이용하여 train_data, test_data 의 prediction 값을 구해보자.\n\n트레이너.predict(전처리된데이터['train'])\n\n\n\n\nPredictionOutput(predictions=array([[ 1.5927906 , -1.3617778 ],\n       [ 2.36449   , -2.1244614 ],\n       [ 2.1150742 , -1.9663404 ],\n       ...,\n       [-2.690494  ,  2.2624812 ],\n       [ 0.32332823, -0.05149931],\n       [-1.96404   ,  1.7741865 ]], dtype=float32), label_ids=array([0, 0, 0, ..., 1, 1, 1]), metrics={'test_loss': 0.1358911097049713, 'test_model_preparation_time': 0.0008, 'test_accuracy': 0.95244, 'test_runtime': 89.3077, 'test_samples_per_second': 279.931, 'test_steps_per_second': 17.501})\n\n\n\n트레이너.predict(전처리된데이터['test'])\n\nPredictionOutput(predictions=array([[ 2.5778208 , -2.250055  ],\n       [ 1.396875  , -1.2021751 ],\n       [ 2.1249173 , -1.9882215 ],\n       ...,\n       [-0.3729212 ,  0.34651938],\n       [ 0.19383232,  0.01419903],\n       [-1.2160491 ,  1.007748  ]], dtype=float32), label_ids=array([0, 0, 0, ..., 1, 1, 1]), metrics={'test_loss': 0.19937647879123688, 'test_model_preparation_time': 0.0008, 'test_accuracy': 0.923, 'test_runtime': 90.2744, 'test_samples_per_second': 276.933, 'test_steps_per_second': 17.314})\n\n\n- 우리가 가져야할 생각: 신기하다 X // 노가다 많이 했구나.. O"
  },
  {
    "objectID": "posts/01wk-1-강의소개.html",
    "href": "posts/01wk-1-강의소개.html",
    "title": "01wk-1: 강의소개",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. 이 수업을 들어야 하는 이유\n\npass\n\n\n\n3. 이 수업을 듣지 말아야 하는 이유\n1. F학점을 줄 수 있음.\n\n모든 퀴즈를 보더라도 성적미달시 F학점 부여\n졸업이 임박한 경우 수강을 권장하지 않음\n\n2. 플립러닝\n\n수업은 비대면수업으로 진행하며, 수업 시간에는 퀴즈를 봄.\n대면 수업에 익숙하고 비대면 수업에 익숙하지 않은 학생들의 경우 수강을 권장하지 않음.\n매주 진행되는 퀴즈가 부담스러운 학생은 수강을 권장하지 않음.\n\n3. 기계학습, 기계학습활용\n\n기계학습활용: 이론적인 설명을 최소화하고, 실습 및 활용에 중점을 둔 수업\n기계학습: 이론과 실습을 같이 배우는 수업\n기계학습활용과 기계학습은 선수과목관계가 아니며, 기계학습활용을 듣지 않고도 기계학습을 이해하는데 문제가 없음.\n\n4. 잘하는 사람들이 많다.\n\n통계학과 고학년, 타학과 고수들\n\n5. 학점이 짜다.\n\n짜게 느껴진다가 더 정확한 표현같아요\n\n6. 파이썬문법이 선행되어야 함.\n\n리스트를 만드는 방법, numpy array가 무엇인지, colab 사용방법 등을 설명하지 않음.\n\n7. cost-effective 하지 않다.\n\n여러가지 이유로..\n이 교과목을 위해서 너무 많은 노력을 해야한다면 드랍하는 것이 좋다고 생각함.\n\n\n\n4. 학점산정방식\n- 강의계획서: 중간40, 기말40, 출석10, 과제10\n- 실제운영: 퀴즈90, 과제10\n\n매주 퀴즈를 보므로 출석은 퀴즈에 포함\n중간/기말 대신 매주 퀴즈로 평가하므로 중간/기말 점수도 퀴즈에 포함된다고 볼 수 있음."
  },
  {
    "objectID": "posts/01wk-2.html#a.-imdb",
    "href": "posts/01wk-2.html#a.-imdb",
    "title": "01wk-2: IMDB 자료 살펴보기, 지도학습의 개념",
    "section": "A. imdb",
    "text": "A. imdb\n- 이제 imdb가 뭔지 살펴보자..\n\n아마 데이터가 있겠죠?\n그런데 이걸 어떻게 보죠??\n\n- 뜯어보자..\n\nimdb # 데이터인듯\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})\n\n\n\n딕셔너리 아니야?\n\n- 딕셔너리는 아님\n\ntype(imdb)\n\ndatasets.dataset_dict.DatasetDict\n\n\n- 그런데 거의 딕셔너리 비슷한 느낌으로 일단 사용되는것 같음.\n\nimdb.keys()\n\ndict_keys(['train', 'test', 'unsupervised'])\n\n\n\nimdb['train']\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 25000\n})\n\n\n\nimdb['test']\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 25000\n})\n\n\n\nimdb['unsupervised']\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 50000\n})\n\n\n- imdb는 딕셔너리 같은 것이고, imdb['train']와 같은 명령어로 세부항목에 접근가능함. 즉 아래의 구조임.\n\nimdb['train'] \\(\\subset\\) imdb\nimdb['test'] \\(\\subset\\) imdb\nimdb['unsupervised'] \\(\\subset\\) imdb"
  },
  {
    "objectID": "posts/01wk-2.html#b.-imdbtrain",
    "href": "posts/01wk-2.html#b.-imdbtrain",
    "title": "01wk-2: IMDB 자료 살펴보기, 지도학습의 개념",
    "section": "B. imdb['train']",
    "text": "B. imdb['train']\n- imdb['train']을 살펴보자..\n\nimdb['train']\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 25000\n})\n\n\n- 이건 딕셔너리 처럼 안되는 것 같네..\n\nimdb['train'].keys()\n\nAttributeError: 'Dataset' object has no attribute 'keys'\n\n\n\nimdb['train']['features']\n\nKeyError: \"Column features not in the dataset. Current columns in the dataset: ['text', 'label']\"\n\n\n- 그럼 imdb['train']는 어떻게 쓰라는 것이냐?? \\(\\to\\) 쓸만한 기능이 있을지 dir로 체크 \\(\\to\\) __getitem__이 있음.. \\(\\to\\) imdb['train'][0] 를 써볼 용기가 생김..\n\nset(dir(imdb['train'])) & {'__getitem__'}\n\n{'__getitem__'}\n\n\n\nimdb['train'][0]\n\n{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.&lt;br /&gt;&lt;br /&gt;The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.&lt;br /&gt;&lt;br /&gt;What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.&lt;br /&gt;&lt;br /&gt;I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n 'label': 0}\n\n\n\n실행이 되었음.\n기쁨.\n\n- imdb['train'][1], imdb['train'][-1], imdb['train'][:2] 등을 실행해보자..\n\nimdb['train'][:5]\n\n{'text': ['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.&lt;br /&gt;&lt;br /&gt;The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.&lt;br /&gt;&lt;br /&gt;What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.&lt;br /&gt;&lt;br /&gt;I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n  '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.',\n  \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.&lt;br /&gt;&lt;br /&gt;One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).&lt;br /&gt;&lt;br /&gt;One might better spend one's time staring out a window at a tree growing.&lt;br /&gt;&lt;br /&gt;\",\n  \"This film was probably inspired by Godard's Masculin, féminin and I urge you to see that film instead.&lt;br /&gt;&lt;br /&gt;The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it's unattractive. Comparing to Godard's film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.&lt;br /&gt;&lt;br /&gt;A movie of its time, and place. 2/10.\",\n  'Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..&lt;br /&gt;&lt;br /&gt;\"Is that all there is??\" ...I was just an early teen when this smoked fish hit the U.S. I was too young to get in the theater (although I did manage to sneak into \"Goodbye Columbus\"). Then a screening at a local film museum beckoned - Finally I could see this film, except now I was as old as my parents were when they schlepped to see it!!&lt;br /&gt;&lt;br /&gt;The ONLY reason this film was not condemned to the anonymous sands of time was because of the obscenity case sparked by its U.S. release. MILLIONS of people flocked to this stinker, thinking they were going to see a sex film...Instead, they got lots of closeups of gnarly, repulsive Swedes, on-street interviews in bland shopping malls, asinie political pretension...and feeble who-cares simulated sex scenes with saggy, pale actors.&lt;br /&gt;&lt;br /&gt;Cultural icon, holy grail, historic artifact..whatever this thing was, shred it, burn it, then stuff the ashes in a lead box!&lt;br /&gt;&lt;br /&gt;Elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings..But if it weren\\'t for the censorship scandal, it would have been ignored, then forgotten.&lt;br /&gt;&lt;br /&gt;Instead, the \"I Am Blank, Blank\" rhythymed title was repeated endlessly for years as a titilation for porno films (I am Curious, Lavender - for gay films, I Am Curious, Black - for blaxploitation films, etc..) and every ten years or so the thing rises from the dead, to be viewed by a new generation of suckers who want to see that \"naughty sex film\" that \"revolutionized the film industry\"...&lt;br /&gt;&lt;br /&gt;Yeesh, avoid like the plague..Or if you MUST see it - rent the video and fast forward to the \"dirty\" parts, just to get it over with.&lt;br /&gt;&lt;br /&gt;'],\n 'label': [0, 0, 0, 0, 0]}\n\n\n\nimdb['train'][-1]\n\n{'text': 'The story centers around Barry McKenzie who must go to England if he wishes to claim his inheritance. Being about the grossest Aussie shearer ever to set foot outside this great Nation of ours there is something of a culture clash and much fun and games ensue. The songs of Barry McKenzie(Barry Crocker) are highlights.',\n 'label': 1}\n\n\n- imdb['train'][0] 을 구체적으로 살펴보자.\n\nprint(imdb['train'][0]['text'])\nprint(imdb['train'][0]['label'])\n\nI rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.&lt;br /&gt;&lt;br /&gt;The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.&lt;br /&gt;&lt;br /&gt;What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.&lt;br /&gt;&lt;br /&gt;I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n0\n\n\n- imdb['train'][1] 을 구체적으로 살펴보자.\n\nprint(imdb['train'][1]['text'])\nprint(imdb['train'][1]['label'])\n\n\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we're treated to the site of Vincent Gallo's throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won't see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women's bodies.\n0\n\n\n- imdb['train'][100] 을 구체적으로 살펴보자.\n\nprint(imdb['train'][100]['text'])\nprint(imdb['train'][100]['label'])\n\nTerrible movie. Nuff Said.&lt;br /&gt;&lt;br /&gt;These Lines are Just Filler. The movie was bad. Why I have to expand on that I don't know. This is already a waste of my time. I just wanted to warn others. Avoid this movie. The acting sucks and the writing is just moronic. Bad in every way. The only nice thing about the movie are Deniz Akkaya's breasts. Even that was ruined though by a terrible and unneeded rape scene. The movie is a poorly contrived and totally unbelievable piece of garbage.&lt;br /&gt;&lt;br /&gt;OK now I am just going to rag on IMDb for this stupid rule of 10 lines of text minimum. First I waste my time watching this offal. Then feeling compelled to warn others I create an account with IMDb only to discover that I have to write a friggen essay on the film just to express how bad I think it is. Totally unnecessary.\n0\n\n\n\n영어: Terrible movie. Nuff Said.These Lines are Just Filler. The movie was bad. Why I have to expand on that I don’t know. This is already a waste of my time. I just wanted to warn others. Avoid this movie. The acting sucks and the writing is just moronic. Bad in every way. The only nice thing about the movie are Deniz Akkaya’s breasts. Even that was ruined though by a terrible and unneeded rape scene. The movie is a poorly contrived and totally unbelievable piece of garbage.OK now I am just going to rag on IMDb for this stupid rule of 10 lines of text minimum. First I waste my time watching this offal. Then feeling compelled to warn others I create an account with IMDb only to discover that I have to write a friggen essay on the film just to express how bad I think it is. Totally unnecessary.\n\n\n한글: 끔찍한 영화. 더 할 말 없음. 이 줄들은 그냥 채우기일 뿐이다. 영화가 나빴다. 내가 왜 이걸 더 길게 써야 하는지 모르겠다. 이건 이미 내 시간 낭비다. 나는 그저 다른 사람들에게 경고하고 싶었을 뿐이다. 이 영화를 피하라. 연기도 형편없고, 대본도 완전히 멍청하다. 모든 면에서 나쁜 영화다. 영화에서 유일하게 좋은 것은 데니즈 아카야의 가슴뿐이었다. 하지만 그것조차도 끔찍하고 불필요한 강간 장면 때문에 망쳤다. 이 영화는 조잡하게 만들어졌고, 전혀 믿을 수 없는 쓰레기다. 이제 IMDb에 대한 불만을 좀 말하겠다. 10줄 이상 써야 한다는 이 어리석은 규칙 때문에 말이다. 먼저 이 쓰레기를 보면서 내 시간을 낭비했다. 그리고 다른 사람들에게 경고하려고 IMDb 계정을 만들었더니, 영화에 대해 내가 얼마나 나쁘게 생각하는지를 표현하려면 이 따위 에세이를 써야 한다는 걸 알게 되었다. 완전히 불필요하다.\n\n\n영화평인듯..\n겁나 뭐라고함.. 부정적임..\n이 text에 대한 label은 0\n\n- imdb['train'][-1] 을 구체적으로 살펴보자.\n\nprint(imdb['train'][-1]['text'])\nprint(imdb['train'][-1]['label'])\n\nThe story centers around Barry McKenzie who must go to England if he wishes to claim his inheritance. Being about the grossest Aussie shearer ever to set foot outside this great Nation of ours there is something of a culture clash and much fun and games ensue. The songs of Barry McKenzie(Barry Crocker) are highlights.\n1\n\n\n\n영어: The story centers around Barry McKenzie who must go to England if he wishes to claim his inheritance. Being about the grossest Aussie shearer ever to set foot outside this great Nation of ours there is something of a culture clash and much fun and games ensue. The songs of Barry McKenzie(Barry Crocker) are highlights.\n\n\n한글: 이 이야기는 유산을 상속받기 위해 영국으로 가야 하는 배리 맥켄지에 초점을 맞추고 있습니다. 배리 맥켄지는 이 위대한 호주를 떠난 사람 중 가장 거친 호주 농부로, 문화적 충돌이 일어나고 다양한 재미와 사건들이 이어집니다. 배리 맥켄지(배리 크로커)가 부르는 노래들이 이 영화의 하이라이트입니다.\n\n- 요약: imdb['train'] 에는 여러개의 영화평이 있고, 각각 긍정평가와 부정평가를 담고 있음.\n\n몇개의 영화평이 있냐? 25000\n부정평가는 0, 긍정평가는 1로 라벨링\n\n\nlen(imdb['train'])\n\n25000\n\n\n\nimdb['train'][24999]\n\n{'text': 'The story centers around Barry McKenzie who must go to England if he wishes to claim his inheritance. Being about the grossest Aussie shearer ever to set foot outside this great Nation of ours there is something of a culture clash and much fun and games ensue. The songs of Barry McKenzie(Barry Crocker) are highlights.',\n 'label': 1}\n\n\n\nimdb['train'][-1]\n\n{'text': 'The story centers around Barry McKenzie who must go to England if he wishes to claim his inheritance. Being about the grossest Aussie shearer ever to set foot outside this great Nation of ours there is something of a culture clash and much fun and games ensue. The songs of Barry McKenzie(Barry Crocker) are highlights.',\n 'label': 1}"
  },
  {
    "objectID": "posts/01wk-2.html#c.-imdbtest",
    "href": "posts/01wk-2.html#c.-imdbtest",
    "title": "01wk-2: IMDB 자료 살펴보기, 지도학습의 개념",
    "section": "C. imdb['test']",
    "text": "C. imdb['test']\n- imdb['train'] 과 비슷함..\n\nimdb['test'][0]\n\n{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n 'label': 0}\n\n\n\nlen(imdb['test'])\n\n25000"
  },
  {
    "objectID": "posts/01wk-2.html#d.-imdbunsupervised",
    "href": "posts/01wk-2.html#d.-imdbunsupervised",
    "title": "01wk-2: IMDB 자료 살펴보기, 지도학습의 개념",
    "section": "D. imdb['unsupervised']",
    "text": "D. imdb['unsupervised']\n- imdb['unsupervised'] 를 살펴보자.\n\nimdb['unsupervised'][3]\n\n{'text': \"Being that the only foreign films I usually like star a Japanese person in a rubber suit who crushes little tiny buildings and tanks, I had high hopes for this movie. I thought that this was a movie that wouldn't put me to sleep. WRONG! Starts off with a bang, okay, now she's in training, alright, she's an assassin, I'm still with you, oh, now she's having this moral dilemma and she can't decide if she loves her boyfriend or her controller, zzzzz.... Oh well, back to Gamera!\",\n 'label': -1}\n\n\n\nimdb['train'], imdb['test'] 와 비슷해 보이지만 살짝 다름.\nlabel 값이 특이하게 -1\n\n- 혹시 imdb['unsupervised'][??] 의 모든 라벨값이 모두 -1인가?\n\nset([l['label'] for l in imdb['unsupervised']])\n\n{-1}\n\n\n\n일단 라벨은 -1 밖에 없음.\n\n\nset([l['label'] for l in imdb['train']])\n\n{0, 1}\n\n\n\nset([l['label'] for l in imdb['test']])\n\n{0, 1}\n\n\n\nimdb['train'],\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 25000\n})\n\n\n\nimdb['test']\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 25000\n})\n\n\n- 느낌상 imdb['unsupervised']는 imdb['train'] 와 imdb['test'] 에서 text를 합치고, label은 모두 -1 로 바꾼 자료가 아닐까? 하는 의심이 들었음. —&gt; 아니었음..\n\ntxt0 = imdb['unsupervised'][0]['text']\ntxt0\n\n'This is just a precious little diamond. The play, the script are excellent. I cant compare this movie with anything else, maybe except the movie \"Leon\" wonderfully played by Jean Reno and Natalie Portman. But... What can I say about this one? This is the best movie Anne Parillaud has ever played in (See please \"Frankie Starlight\", she\\'s speaking English there) to see what I mean. The story of young punk girl Nikita, taken into the depraved world of the secret government forces has been exceptionally over used by Americans. Never mind the \"Point of no return\" and especially the \"La femme Nikita\" TV series. They cannot compare the original believe me! Trash these videos. Buy this one, do not rent it, BUY it. BTW beware of the subtitles of the LA company which \"translate\" the US release. What a disgrace! If you cant understand French, get a dubbed version. But you\\'ll regret later :)'\n\n\n\n[l for l in imdb['train'] if l['text'] == txt0]\n\n[]\n\n\n\n[l for l in imdb['test'] if l['text'] == txt0]\n\n[]\n\n\n\n[l for l in imdb['unsupervised'] if l['text'] == txt0]\n\n[{'text': 'This is just a precious little diamond. The play, the script are excellent. I cant compare this movie with anything else, maybe except the movie \"Leon\" wonderfully played by Jean Reno and Natalie Portman. But... What can I say about this one? This is the best movie Anne Parillaud has ever played in (See please \"Frankie Starlight\", she\\'s speaking English there) to see what I mean. The story of young punk girl Nikita, taken into the depraved world of the secret government forces has been exceptionally over used by Americans. Never mind the \"Point of no return\" and especially the \"La femme Nikita\" TV series. They cannot compare the original believe me! Trash these videos. Buy this one, do not rent it, BUY it. BTW beware of the subtitles of the LA company which \"translate\" the US release. What a disgrace! If you cant understand French, get a dubbed version. But you\\'ll regret later :)',\n  'label': -1}]"
  },
  {
    "objectID": "posts/01wk-2.html#f.-정리",
    "href": "posts/01wk-2.html#f.-정리",
    "title": "01wk-2: IMDB 자료 살펴보기, 지도학습의 개념",
    "section": "F. 정리",
    "text": "F. 정리\n- 요약:\n\nimdb는 imdb['train'], imdb['test'], imdb['unsupervised'] 로 나누어져 있음.\nimdb['train'], imdb['test'] 에는 각각 (text,label)와 같은 형식으로 정보가 저장되어 있음. 여기에서 label은 0 혹은 1의 값을 가지는데 0은 부정, 1은 긍정을 의미함.\nimdb['unsupervised'] 에도 조사해보니 각각 (text,label)와 같은 형식으로 정보가 저장되어 있었지만, 여기에서 label값은 모두 -1의 값만 있었음. 따라서 사살상 imdb['unsupervised']는 text에 대한 정보만 있다고 생각해도 무방. 그 text가 영화에 대한 긍정평가인지 부정평가인지 분류가 되어있지 않은 상태.\n\n- 외우세요: train, test, unsupervised 와 같은 단어는 매우 중요한 단어니까 일단 눈여겨보세요"
  },
  {
    "objectID": "posts/01wk-2.html#a.-기계학습딥러닝-과업",
    "href": "posts/01wk-2.html#a.-기계학습딥러닝-과업",
    "title": "01wk-2: IMDB 자료 살펴보기, 지도학습의 개념",
    "section": "A. 기계학습/딥러닝 과업",
    "text": "A. 기계학습/딥러닝 과업\n- 왜 데이터가 imdb['train'], imdb['test'], imdb['unsupervised'] 와 같이 나누어져 있는가?\n\n개념: 기계학습/딥러닝은 과업은 크게 지도학습과 비지도학습이 있음.\n데이터에서 imdb['train'], imdb['test'] 는 지도학습 모델을 배우기 위한 예제데이터이고, imdb['unsupervised']는 비지도학습모델을 배우기 위한 예제데이터임.\n아무튼 우리가 하는 “감성분석”은 지도학습이고, 따라서 우리는 imdb['train'], imdb['test'] 에만 관심을 가질 예정임."
  },
  {
    "objectID": "posts/01wk-2.html#b.-지도학습",
    "href": "posts/01wk-2.html#b.-지도학습",
    "title": "01wk-2: IMDB 자료 살펴보기, 지도학습의 개념",
    "section": "B. 지도학습",
    "text": "B. 지도학습\n- 지도학습이란? (이 예제에 한정하여 설명)\n\n자료가 “(텍스트, 라벨)” 의 형태로 정리가 되어 있을때, “텍스트”를 입력으로 주면 “라벨”을 출력해주는 함수 f를 찾는 일\n코드로 예를들어 설명하면 적당히 f라는 함수가 존재하여 아래와 같은 동작이 가능하도록 해야함.\n\nf(\"영화가 너무 재미없어요\")\n&gt; \"부정평가입니다\"\n\nf(\"영화, 괜찮은데요??\")\n&gt; \"긍정평가입니다\"\n\nf(\"배우들 연기 진짜 잘함. 영상미도 있음. 그런데 스토리 때문에 망했네.\")\n&gt; \"부정평가입니다\"\n- 이러한 함수 f를 우리가 잘 정의한다면 좋겠음. (가능한가??)\n- 대충 아래와 같은 과정을 거친다고 생각하면 편리함.\n\n정보(숫자,텍스트,이미지,…) \\(\\to\\) 숫자 \\(\\to\\) 계산 \\(\\to\\) 계산된숫자 \\(\\to\\) 정보\n\n- 예를들면 아래와 같은 방식이 가능\n\n긍정단어 = {'좋아', '재미있었음', '잘생김', '예뻐', '연기훌륭함'}\n부정단어 = {'지루해', '재미없었음', '비추천'}\n\n\ntxt0 = \"남주가 너무 잘생김 여주도 예뻐 영화가 중간에 조금 지루해 그래도 아무튼 재미있었음\"\n\n\n긍정단어수 = sum([l in 긍정단어 for l in txt0.split(' ')])\n부정단어수 = sum([l in 부정단어 for l in txt0.split(' ')])\n긍정평가일확률 = 긍정단어수/(긍정단어수+부정단어수)\n부정평가일확률 = 부정단어수/(긍정단어수+부정단어수)\n긍정평가일확률,부정평가일확률\n\n(0.75, 0.25)\n\n\n\n긍정평가군!\n\n\ndef f(text):\n    긍정단어수 = sum([l in 긍정단어 for l in text.split(' ')])\n    부정단어수 = sum([l in 부정단어 for l in text.split(' ')])\n    긍정평가일확률 = 긍정단어수/(긍정단어수+부정단어수)\n    부정평가일확률 = 부정단어수/(긍정단어수+부정단어수)\n    if 긍정평가일확률 &gt; 0.5:\n        return 긍정평가일확률,\"긍정평가\"\n    else:\n        return 부정평가일확률,\"부정평가\"\n\n\nf(\"남주가 너무 잘생김 여주도 예뻐 영화가 중간에 조금 지루해 그래도 아무튼 재미있었음\")\n\n(0.75, '긍정평가')\n\n\n- 당연히 현재는 많은 문제점이 있음.\n\nf(\"남주가 너무 잘생김. 여주도 예뻐. 영화가 중간에 조금 지루해 그래도 아무튼 재미있었음.\")\n\n(1.0, '부정평가')\n\n\n\nf(\"캐스팅 좋아 여주인공 특히 예뻐 배우들 연기훌륭함 그렇지만 스토리때문에 나는 개인적으로 비추천\")\n\n(0.75, '긍정평가')\n\n\n- 다행인점: 좋은 f를 만들기 위해서 우리가 고민할 필요 없음. (똑똑한 사람들이 다 만들어 놓음. 그리고 만들고 있음.)\n\n옛날방식: f 를 한땀한땀 설계. 초고수가 밤새 코딩해서 진짜 잘 맞추는 f를 한번에 제시.\n최근방식: f 를 대충 설계. 거의 멍청이 수준의 f임. 그런데 데이터를 줄수록 f가 점점 똑똑해짐. 나중에는 다 맞춤. –&gt; 인공지능???"
  },
  {
    "objectID": "posts/01wk-2.html#c.-traintest-자료의-의미",
    "href": "posts/01wk-2.html#c.-traintest-자료의-의미",
    "title": "01wk-2: IMDB 자료 살펴보기, 지도학습의 개념",
    "section": "C. train/test 자료의 의미",
    "text": "C. train/test 자료의 의미\n- 초고수가 f를 직접 설계하던 시대에는 별로 문제가 없었음. 그런데 최근 컴퓨터가 데이터를 보고 f를 스스로 수정하기 시작하면서 이상한 방식으로 f가 수정되는 경우가 보고되고 있음. f가 똑똑해 보이는데, 사실 멍청한 상태..\n\n어떻게 이런일이 가능하지??\n\n- 최규빈 교수의 착각\n\n상상: 나는 학생들에게 파이썬프로그래밍을 잘 강의하고 싶었다. 나는 학생들에게 다양한 문제를 풀어줬으며, 문제를 풀면서 학생들이 스스로 개념을 깨우치길 원했다. 나는 다양한 예시를 통해서 이해하는 것이 좋다고 생각했기 때문이다. 예시는 많을수록 좋으니까 한 학기동안 총 1000개의 문제를 풀어줬다. 기말고사는 풀어준 문제중에서 약 20문항을 샘플링하여 출제했다. 놀랍게도 학생들이 모두 만점을 받았으며 나는 아주 만족스러웠다. 한 학기 동안 고생한 보람이 있어보였다. 눈물이 흘렀다.\n질문: 최규빈 교수는 잘 평가한 걸까요? 학생들이 진짜 파이썬프로그래밍을 잘 이해했을까요?? (학과교수님들께 자랑해도 될까요?)\n이렇게 질문하고 싶지 않아요?: 1000개의 문제에서 샘플링하여 출제하지 않고, 새로운 문항을 개발하여 학생들에게 제시했다면??\n요령이 있는 교수라면 이렇게 했을거에요: 50000개의 문제세트가 있다고 하자. 수업시간에는 학생들에게 예시로 25000개 정도의 문항을 풀이하며 설명. 기말고사는 수업시간에 풀이하지 않은 25000개의 문항을 출제함.\n만약에 학생들이 수업시간에 풀어준 25000개의 문제를 올바르게 이해했다면, 수업시간에 풀이하지 않은 문항 25000개 역시 잘 풀었을 것임.\n\n- 이 상황을 살짝 말만 바꿔볼게요.\n\n상상: 나는 인공지능에게 “영화평가 텍스트를 주면 그것이 긍정평가인지 부정평가인지 판단하는 능력”을 잘 학습시키고 싶었다. 나는 인공지능에게 다양한 데이터를 제공했으며, 데이터를 보고 인공지능이 스스로 원리를 깨우치길 원했다. 데이터는 많을수록 좋으니까 약 50000개의 “(텍스트,라벨)” 쌍을 제공했다. 그리고 50000개의 “(텍스트.라벨)” 쌍에서 약 20문항을 샘플링하여 테스트했다. 놀랍게도 인공지능은 20문항을 모두 맞추었다. 나는 아주 만족스러웠다. 눈물이 흘렀다.\n질문: 저는 인공지능을 잘 학습시켰을까요?\n이렇게 하고 싶지 않아요?: 50000개의 데이터중, 25000개의 “(텍스트,라벨)”만 인공지능에게 제공하여 학습시킴. 그리고 나머지 25000개는 평가용으로 테스트해봄.\n만약에 인공지능이 진짜 영화평가 텍스트를 바탕으로 그것이 긍정평가인지 부정평가인지 판단하는 능력을 길렀다면?? 내가 인공지능에게 제공하지 않은 25000개의 데이터에 대해서도 함수 f 가 올바르게 동작해야함.\n\n- train data / test data\n\ntrain data 는 인공지능에게 학습용으로 제공하는 데이터\ntest data 는 인공지능이 진짜 잘 학습했는지 평가하기 위해 학습시 제공하지 않는 자료\n\n\nimdb\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})"
  },
  {
    "objectID": "posts/01wk-2.html#d.-trainvaltest-자료의-의미",
    "href": "posts/01wk-2.html#d.-trainvaltest-자료의-의미",
    "title": "01wk-2: IMDB 자료 살펴보기, 지도학습의 개념",
    "section": "D. train/val/test 자료의 의미",
    "text": "D. train/val/test 자료의 의미\n- 학생들 입장에서 생각해본다면??\n\n소망: 내가 외우려고 한게 아니고, 하도 공부를 많이 하다보니 문제가 외워졌음. 나도 그러기 싫음. 나도 내가 올바르게 공부했는지 체크하고 싶어.\n아이디어: 교수님이 풀어준 25000개의 문항중, 15000개만 전략적으로 공부함. 그리고 나머지 10000개는 나 스스로 올바르게 공부했는지 체크하는 용도로 삼음.\n진행사항:\n\n1일차: {15000문항: 50%맞음, 10000문항: 45%맞음}\n\n2일차: {15000문항: 90%맞음, 10000문항: 85%맞음}\n3일차: {15000문항: 100%맞음, 10000문항: 70%맞음} &lt;— 이런 경험 있어요??\n\n판단: 이게 한 3일차쯤 공부하다보니까 문제를 내가 너무 외운것 같네? 오히려 2일차일때의 느낌이 더 좋음. 그냥 2일차의 느낌으로 시험보러 가자!!\n\n- 이럴경우 아래와 같이 상황이 정리된다.\n\n원래: 교수가 수업시간에 풀어준 25000문제 = 학생이 공부할 25000문제 = train // 교수가 기말시험으로 제출할 25000문제 = test\n바뀐상황: 학생이 공부할 15000문제 = train / 학생이 자가진단용으로 으로 뺀 10000문제 = test // 교수가 기말시험으로 제출할 25000문제 = 찐test\n\n- 학생이 자가진단용으로 빼둔 10000개의 문항을 보통 validation set 이라고 부른다. 따라서 아래와 같이 정리 가능하다.\n\ntrain = 15000문제 = 학생이 스스로 공부\nvalidation = 10000문제 = 학생이 공부할때 사용하지 않음. 자가진단용.\ntest = 25000문제 = 교수가 출제하는 시험\n\n- train / validation / test 에 대한 용어는 엄밀하지 않게 사용되는 경우가 많아 그때그때 상황에 맞게 알아서 해석해야 한다.\n\n억지상황1: 교수가 시험보지 않음. 그런데 내가 스스로 공부하면서 잘 공부하고 있는지 체크하고 싶어서 1000개의 문제를 구하고 그중 매일 800개만 학습하고 200개는 검증용으로 사용함. 이 경우 200개의 문항을 validation 이라고 부르기도 하고 test 라고 부르기도 함. (엄밀하게는 validation이 맞다고 생각하지만, 외부데이터가 없는 상황이므로 validation과 test의 경계가 흐릿해짐)\n억지상황2: 나 혼자 1000개의 문항을 800/200으로 나누어 매일 공부하고 있었음. 이때 나는 200개의 문항을 test라고 부르기도 하고, validation이라고 부르기도 했음. 그런데 갑자기 교수가 나보고 외부 코딩대회에 나가라고 함. 이 경우 200개의 문항은 validation 이 되고 외부코딩대회에서 출제된 문항이 test가 됨.\n\n- 느낌: 아래가 가장 정확한 설명임\n\ntrain: 학습에 사용하는 자료\nvalidation: 학습에 사용하지 않는 자료. 왜 안써? 더 좋은 훈련을 위한 목적.\ntest: 학습에 사용하지 않는 자료. 왜 안써? 올바르게 학습됨을 평가하기 위한 목적.\n\n\n헷갈리는 이유는 더 좋은 훈련을 위한 목적과 올바르게 학습됨을 평가하기 위한 목적이 무 자르듯이 구분되지 않기 때문.\n\n- 이상한 분류법\n\n데이터를 보통 2개의 셋으로 나누면 train/test 로 3개로 나누면 train/test/validation 으로 많이 표현.\n딱 맞는 정의는 아님. 의미상 구분해야함."
  },
  {
    "objectID": "posts/05wk-1.html#a.-데이터불러오기",
    "href": "posts/05wk-1.html#a.-데이터불러오기",
    "title": "05wk-1: Food-101 이미지자료 분류",
    "section": "A. 데이터불러오기",
    "text": "A. 데이터불러오기\n- 원래는 자료가 많음\n\nfood_full = datasets.load_dataset(\"food101\")\nfood_full\n# 자료가 약 10만개, 자료형은 DatasetDict 임 \n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 75750\n    })\n    validation: Dataset({\n        features: ['image', 'label'],\n        num_rows: 25250\n    })\n})\n\n\n- train에서 5000장만 가져옴\n\nfood5000 = datasets.load_dataset(\"food101\", split=\"train[:5000]\")\nfood5000\n# 자료는 5000개, 자료형은 Dataset \n\nDataset({\n    features: ['image', 'label'],\n    num_rows: 5000\n})\n\n\n- food5000에서 8:2로 데이터를 분리\n\nfood = food5000.train_test_split(test_size=0.2)\nfood\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 4000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 1000\n    })\n})"
  },
  {
    "objectID": "posts/05wk-1.html#b.-데이터-살펴보기",
    "href": "posts/05wk-1.html#b.-데이터-살펴보기",
    "title": "05wk-1: Food-101 이미지자료 분류",
    "section": "B. 데이터 살펴보기",
    "text": "B. 데이터 살펴보기\n- 이미지를 보는 방법\n\nimg = food['train'][0]['image']\nimg\n\n\n\n\n\n\n\n\n\ntype(img) # img의 자료형\n\nPIL.Image.Image\n\n\n- 이미지에 해당하는 라벨을 같이 확인하는 방법\n\n# food['train'][0]['image'] --- # 0번이미지\nfood['train'][0]['label'] # 0번이미지에 해당하는 라벨\n\n53\n\n\n20이 의미하는바가 무엇이지?\n\nlabels = food['train'].features['label'].names # 라벨들의 정보들\nlabels[:5]\n\n['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare']\n\n\n\nlabels[20] # 20이라는 숫자가 의미하는 음식이름은 'chicken_wings' 임\n\n'chicken_wings'\n\n\n정리하면\n\nimg_num = 11\nprint(labels[food['train'][img_num]['label']])\nfood['train'][img_num]['image']\n\nbruschetta\n\n\n\n\n\n\n\n\n\n- 이미지를 확인하면서 느낀점: 크기가 다름 \\(\\to\\) 각 이미지의 크기를 조사하고 싶다면?\n\n# 방법1 \nfood['train'][3]['image'].__str__().split(' ')[-3].split(\"=\")[-1].split(\"x\")\nsizes = [l['image'].__str__().split(' ')[-3].split(\"=\")[-1].split(\"x\") for l in food['train']]\nsizes[:4]\n\n[['512', '512'], ['512', '512'], ['512', '512'], ['512', '512']]\n\n\n\n# 방법2\nsizes = [l['image'].size for l in food['train']]\nsizes[:4]\n\n[(512, 512), (512, 512), (512, 512), (512, 512)]\n\n\n\n이미지의 크기가 서로 다르네? (텍스트의 길이가 서로 다르듯이?)\n인공지능입장에서는 이렇게 이미지의 크기가 통일되어 있지 않다면 싫어하겠는걸?\n이미지를 resize 하여 크기를 통일시켜주는 코드가 어딘가 반드시 필요하겠군.."
  },
  {
    "objectID": "posts/05wk-1.html#c.-torchvision.transforms",
    "href": "posts/05wk-1.html#c.-torchvision.transforms",
    "title": "05wk-1: Food-101 이미지자료 분류",
    "section": "C. torchvision.transforms",
    "text": "C. torchvision.transforms\n- 이미지자료 하나 받아두기\n\nimg = datasets.load_dataset(\"food101\",split=\"train[:1]\")[0]['image']\nimg\n\n\n\n\n\n\n\n\n\n# torchvision.transforms.RandomResizedCrop\n# 예시1 – 사이즈를 224,112로 조정\n\n자르고크기조정하기 = torchvision.transforms.RandomResizedCrop((224,112))\n\n\n자르고크기조정하기(img)\n\n\n\n\n\n\n\n\n#\n# 예시2 – 사이즈를 224,224로 조정\n\n# 방법1\n자르고크기조정하기 = torchvision.transforms.RandomResizedCrop((224,224))\n자르고크기조정하기(img)\n\n\n\n\n\n\n\n\n\n# 방법2\n자르고크기조정하기 = torchvision.transforms.RandomResizedCrop(224)\n자르고크기조정하기(img)\n\n\n\n\n\n\n\n\n#\n\n\n# torchvision.transforms.ToTensor()\n# 예시1\n\n텐서화하기 = torchvision.transforms.ToTensor()\n\n\n텐서화하기(img)\n\ntensor([[[0.1216, 0.1137, 0.1098,  ..., 0.0039, 0.0039, 0.0000],\n         [0.1255, 0.1216, 0.1176,  ..., 0.0039, 0.0039, 0.0000],\n         [0.1294, 0.1255, 0.1255,  ..., 0.0039, 0.0000, 0.0000],\n         ...,\n         [0.2588, 0.2745, 0.2863,  ..., 0.3765, 0.3882, 0.3922],\n         [0.2353, 0.2471, 0.2667,  ..., 0.3373, 0.3373, 0.3373],\n         [0.2235, 0.2275, 0.2471,  ..., 0.3333, 0.3176, 0.3059]],\n\n        [[0.1373, 0.1294, 0.1255,  ..., 0.1020, 0.1020, 0.0980],\n         [0.1412, 0.1373, 0.1333,  ..., 0.1020, 0.1020, 0.0980],\n         [0.1451, 0.1412, 0.1412,  ..., 0.1020, 0.0980, 0.0980],\n         ...,\n         [0.2471, 0.2627, 0.2745,  ..., 0.3647, 0.3765, 0.3882],\n         [0.2235, 0.2353, 0.2549,  ..., 0.3255, 0.3333, 0.3333],\n         [0.2118, 0.2157, 0.2353,  ..., 0.3216, 0.3137, 0.3020]],\n\n        [[0.1412, 0.1333, 0.1294,  ..., 0.0902, 0.0902, 0.0863],\n         [0.1451, 0.1412, 0.1451,  ..., 0.0902, 0.0902, 0.0863],\n         [0.1490, 0.1451, 0.1529,  ..., 0.0902, 0.0863, 0.0863],\n         ...,\n         [0.1725, 0.1882, 0.2000,  ..., 0.2431, 0.2549, 0.2667],\n         [0.1490, 0.1608, 0.1804,  ..., 0.2039, 0.2118, 0.2118],\n         [0.1373, 0.1412, 0.1608,  ..., 0.2000, 0.1922, 0.1804]]])\n\n\n\n텐서화하기(img).shape\n\ntorch.Size([3, 512, 384])\n\n\n#\n# 예시2 – 자르고크기조정하기 와 텐서화하기를 동시에 사용하는 경우\n\n텐서화하기(자르고크기조정하기(img)).shape\n\ntorch.Size([3, 224, 224])\n\n\n\n자르고크기조정하기(텐서화하기(img)).shape\n\ntorch.Size([3, 224, 224])\n\n\n#\n\n\n# torchvision.transforms.Normalize\n# 예시1\n\n표준화하기 = torchvision.transforms.Normalize(mean=[10,20,30],std=[0.5,1.0,1.5])\n\n\n표준화하기는 각 채널별로, mean을 뺸 뒤 std를 나눈 계산값을 리턴한다.\n\n\n표준화하기(텐서화하기(img)) # 숫자들이 계산됨\n\ntensor([[[-19.7569, -19.7725, -19.7804,  ..., -19.9922, -19.9922, -20.0000],\n         [-19.7490, -19.7569, -19.7647,  ..., -19.9922, -19.9922, -20.0000],\n         [-19.7412, -19.7490, -19.7490,  ..., -19.9922, -20.0000, -20.0000],\n         ...,\n         [-19.4824, -19.4510, -19.4275,  ..., -19.2471, -19.2235, -19.2157],\n         [-19.5294, -19.5059, -19.4667,  ..., -19.3255, -19.3255, -19.3255],\n         [-19.5529, -19.5451, -19.5059,  ..., -19.3333, -19.3647, -19.3882]],\n\n        [[-19.8627, -19.8706, -19.8745,  ..., -19.8980, -19.8980, -19.9020],\n         [-19.8588, -19.8627, -19.8667,  ..., -19.8980, -19.8980, -19.9020],\n         [-19.8549, -19.8588, -19.8588,  ..., -19.8980, -19.9020, -19.9020],\n         ...,\n         [-19.7529, -19.7373, -19.7255,  ..., -19.6353, -19.6235, -19.6118],\n         [-19.7765, -19.7647, -19.7451,  ..., -19.6745, -19.6667, -19.6667],\n         [-19.7882, -19.7843, -19.7647,  ..., -19.6784, -19.6863, -19.6980]],\n\n        [[-19.9059, -19.9111, -19.9137,  ..., -19.9399, -19.9399, -19.9425],\n         [-19.9033, -19.9059, -19.9033,  ..., -19.9399, -19.9399, -19.9425],\n         [-19.9007, -19.9033, -19.8980,  ..., -19.9399, -19.9425, -19.9425],\n         ...,\n         [-19.8850, -19.8745, -19.8667,  ..., -19.8379, -19.8301, -19.8222],\n         [-19.9007, -19.8928, -19.8797,  ..., -19.8641, -19.8588, -19.8588],\n         [-19.9085, -19.9059, -19.8928,  ..., -19.8667, -19.8719, -19.8797]]])\n\n\n숫자들이 어떻게 계산되었는가?\n\nprint(\"첫번째 채널(R)\")\n(텐서화하기(img)[0] - 10)/0.5, 표준화하기(텐서화하기(img))[0]\n\n첫번째 채널(R)\n\n\n(tensor([[-19.7569, -19.7725, -19.7804,  ..., -19.9922, -19.9922, -20.0000],\n         [-19.7490, -19.7569, -19.7647,  ..., -19.9922, -19.9922, -20.0000],\n         [-19.7412, -19.7490, -19.7490,  ..., -19.9922, -20.0000, -20.0000],\n         ...,\n         [-19.4824, -19.4510, -19.4275,  ..., -19.2471, -19.2235, -19.2157],\n         [-19.5294, -19.5059, -19.4667,  ..., -19.3255, -19.3255, -19.3255],\n         [-19.5529, -19.5451, -19.5059,  ..., -19.3333, -19.3647, -19.3882]]),\n tensor([[-19.7569, -19.7725, -19.7804,  ..., -19.9922, -19.9922, -20.0000],\n         [-19.7490, -19.7569, -19.7647,  ..., -19.9922, -19.9922, -20.0000],\n         [-19.7412, -19.7490, -19.7490,  ..., -19.9922, -20.0000, -20.0000],\n         ...,\n         [-19.4824, -19.4510, -19.4275,  ..., -19.2471, -19.2235, -19.2157],\n         [-19.5294, -19.5059, -19.4667,  ..., -19.3255, -19.3255, -19.3255],\n         [-19.5529, -19.5451, -19.5059,  ..., -19.3333, -19.3647, -19.3882]]))\n\n\n\nprint(\"두번째 채널(G)\")\n(텐서화하기(img)[1] - 20)/1.0, 표준화하기(텐서화하기(img))[1]\n\n두번째 채널(G)\n\n\n(tensor([[-19.8627, -19.8706, -19.8745,  ..., -19.8980, -19.8980, -19.9020],\n         [-19.8588, -19.8627, -19.8667,  ..., -19.8980, -19.8980, -19.9020],\n         [-19.8549, -19.8588, -19.8588,  ..., -19.8980, -19.9020, -19.9020],\n         ...,\n         [-19.7529, -19.7373, -19.7255,  ..., -19.6353, -19.6235, -19.6118],\n         [-19.7765, -19.7647, -19.7451,  ..., -19.6745, -19.6667, -19.6667],\n         [-19.7882, -19.7843, -19.7647,  ..., -19.6784, -19.6863, -19.6980]]),\n tensor([[-19.8627, -19.8706, -19.8745,  ..., -19.8980, -19.8980, -19.9020],\n         [-19.8588, -19.8627, -19.8667,  ..., -19.8980, -19.8980, -19.9020],\n         [-19.8549, -19.8588, -19.8588,  ..., -19.8980, -19.9020, -19.9020],\n         ...,\n         [-19.7529, -19.7373, -19.7255,  ..., -19.6353, -19.6235, -19.6118],\n         [-19.7765, -19.7647, -19.7451,  ..., -19.6745, -19.6667, -19.6667],\n         [-19.7882, -19.7843, -19.7647,  ..., -19.6784, -19.6863, -19.6980]]))\n\n\n\nprint(\"세번째 채널(B)\")\n(텐서화하기(img)[2] - 30)/1.5, 표준화하기(텐서화하기(img))[2]\n\n세번째 채널(B)\n\n\n(tensor([[-19.9059, -19.9111, -19.9137,  ..., -19.9399, -19.9399, -19.9425],\n         [-19.9033, -19.9059, -19.9033,  ..., -19.9399, -19.9399, -19.9425],\n         [-19.9007, -19.9033, -19.8980,  ..., -19.9399, -19.9425, -19.9425],\n         ...,\n         [-19.8850, -19.8745, -19.8667,  ..., -19.8379, -19.8301, -19.8222],\n         [-19.9007, -19.8928, -19.8797,  ..., -19.8641, -19.8588, -19.8588],\n         [-19.9085, -19.9059, -19.8928,  ..., -19.8667, -19.8719, -19.8797]]),\n tensor([[-19.9059, -19.9111, -19.9137,  ..., -19.9399, -19.9399, -19.9425],\n         [-19.9033, -19.9059, -19.9033,  ..., -19.9399, -19.9399, -19.9425],\n         [-19.9007, -19.9033, -19.8980,  ..., -19.9399, -19.9425, -19.9425],\n         ...,\n         [-19.8850, -19.8745, -19.8667,  ..., -19.8379, -19.8301, -19.8222],\n         [-19.9007, -19.8928, -19.8797,  ..., -19.8641, -19.8588, -19.8588],\n         [-19.9085, -19.9059, -19.8928,  ..., -19.8667, -19.8719, -19.8797]]))\n\n\n#\n\n\n# torchvision.transforms.Compose\n# 예시1 – 여러함수를 묶어 하나의 함수를 만드는 방법\n\n이미지처리하기 = torchvision.transforms.Compose([자르고크기조정하기, 텐서화하기, 표준화하기])\n\n\n이미지처리하기(img)\n\ntensor([[[-19.7647, -19.7333, -19.6392,  ..., -19.9922, -19.9922, -19.9922],\n         [-19.7647, -19.7412, -19.6706,  ..., -19.9922, -20.0000, -19.9922],\n         [-19.7490, -19.7255, -19.6784,  ..., -19.9843, -20.0000, -19.9922],\n         ...,\n         [-19.3412, -19.3176, -19.3412,  ..., -19.0980, -19.0824, -19.1373],\n         [-19.3882, -19.3569, -19.2784,  ..., -19.1059, -19.1765, -19.1765],\n         [-19.3098, -19.3098, -19.2784,  ..., -19.0980, -19.1059, -19.1216]],\n\n        [[-19.8706, -19.8549, -19.8078,  ..., -19.9059, -19.9098, -19.9137],\n         [-19.8706, -19.8588, -19.8235,  ..., -19.9059, -19.9137, -19.9176],\n         [-19.8627, -19.8510, -19.8275,  ..., -19.9059, -19.9176, -19.9176],\n         ...,\n         [-19.6784, -19.6667, -19.6784,  ..., -19.5922, -19.5882, -19.6118],\n         [-19.7059, -19.6863, -19.6510,  ..., -19.5922, -19.6275, -19.6235],\n         [-19.6706, -19.6706, -19.6549,  ..., -19.5882, -19.5843, -19.5922]],\n\n        [[-19.8954, -19.8850, -19.8510,  ..., -19.9425, -19.9451, -19.9477],\n         [-19.8954, -19.8850, -19.8614,  ..., -19.9425, -19.9477, -19.9477],\n         [-19.8902, -19.8771, -19.8614,  ..., -19.9425, -19.9503, -19.9477],\n         ...,\n         [-19.8510, -19.8431, -19.8510,  ..., -19.7856, -19.7830, -19.7987],\n         [-19.8693, -19.8562, -19.8327,  ..., -19.7856, -19.8118, -19.8092],\n         [-19.8458, -19.8458, -19.8353,  ..., -19.7830, -19.7830, -19.7882]]])\n\n\n#"
  },
  {
    "objectID": "posts/05wk-1.html#d.-이미지전처리",
    "href": "posts/05wk-1.html#d.-이미지전처리",
    "title": "05wk-1: Food-101 이미지자료 분류",
    "section": "D. 이미지전처리",
    "text": "D. 이미지전처리\n- 아래의 샘플코드를 살펴보자.\nimage_processor = transformers.AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\nnormalize = torchvision.transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\nsize = (\n    image_processor.size[\"shortest_edge\"]\n    if \"shortest_edge\" in image_processor.size\n    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n)\n_transforms = torchvision.transforms.Compose([\n    torchvision.transforms.RandomResizedCrop(size), \n    torchvision.transforms.ToTensor(), \n    normalize\n])\ndef transforms(examples):\n    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n    del examples[\"image\"]\n    return examples\nfood = food.with_transform(transforms)\n# 예비학습 1\n\na = +2\n(\n    (a,\"음수\") if a&lt;0 else (a,\"양수\")\n)\n\n(2, '양수')\n\n\n\ndct = {'shortest_edge':16, 'height': 224, 'width': 224}\ndct = {'height': 224, 'width': 224}\n(\n    dct['shortest_edge'] \n    if 'shortest_edge' in dct \n    else (dct['height'],dct['width'])\n)\n\n(224, 224)\n\n\n#\n# 예비학습 2\n\nimg.convert(\"L\")\n\n\n\n\n\n\n\n\n#\n- image_processor 살펴보기\n\nimage_processor = transformers.AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\nimage_processor\n\nFast image processor class &lt;class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'&gt; is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n\n\nViTImageProcessor {\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"image_processor_type\": \"ViTImageProcessor\",\n  \"image_std\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"resample\": 2,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"height\": 224,\n    \"width\": 224\n  }\n}\n\n\n\nimage_processor.image_mean\n\n[0.5, 0.5, 0.5]\n\n\n\nimage_processor.image_std\n\n[0.5, 0.5, 0.5]\n\n\n\nimage_processor.size['height'],image_processor.size['width']\n\n(224, 224)\n\n\n- 코드는 아래와 같이 단순화하여 이해할 수 있음.\n\n_transforms = torchvision.transforms.Compose([\n    torchvision.transforms.RandomResizedCrop((224,224)), \n    torchvision.transforms.ToTensor(), \n    torchvision.transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n])\ndef transforms(examples):\n    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n    del examples[\"image\"]\n    return examples\nfood_transformed = food.with_transform(transforms)\n\n- 주의점: food.with_transform(transforms)는 지연처리되는 코드임\nfood_transformed['train'][0:5] 이것이 나오는 원리?\n\nexamples = food['train'][0:5]\ntransforms(examples)\n\n{'label': [53, 6, 77, 10, 79],\n 'pixel_values': [tensor([[[ 0.0824,  0.0902,  0.1686,  ..., -0.3255, -0.3020, -0.3176],\n           [ 0.1686,  0.1216,  0.1843,  ..., -0.2863, -0.2784, -0.2863],\n           [ 0.2471,  0.1529,  0.2000,  ..., -0.3020, -0.2941, -0.2941],\n           ...,\n           [ 0.5765,  0.6941,  0.6392,  ..., -0.3098, -0.3255, -0.3176],\n           [ 0.5608,  0.7020,  0.6549,  ..., -0.3333, -0.3255, -0.3020],\n           [ 0.5529,  0.6706,  0.6235,  ..., -0.3333, -0.3098, -0.2941]],\n  \n          [[-0.0588, -0.0667, -0.0039,  ...,  0.2627,  0.2863,  0.2706],\n           [ 0.0275, -0.0353,  0.0118,  ...,  0.2392,  0.2549,  0.2471],\n           [ 0.1137, -0.0039,  0.0353,  ...,  0.1529,  0.1608,  0.1686],\n           ...,\n           [ 0.1922,  0.0196, -0.0745,  ..., -0.4353, -0.4588, -0.4510],\n           [ 0.2078,  0.0667, -0.0353,  ..., -0.4510, -0.4588, -0.4353],\n           [ 0.2235,  0.0745, -0.0431,  ..., -0.4510, -0.4431, -0.4275]],\n  \n          [[-0.0980, -0.1686, -0.0980,  ...,  0.4431,  0.4588,  0.4431],\n           [-0.0039, -0.1137, -0.0667,  ...,  0.4510,  0.4588,  0.4510],\n           [ 0.0824, -0.0588, -0.0196,  ...,  0.3882,  0.3882,  0.3882],\n           ...,\n           [ 0.0667, -0.1216, -0.2235,  ..., -0.5843, -0.6000, -0.5922],\n           [ 0.0824, -0.0667, -0.1765,  ..., -0.6000, -0.6000, -0.5686],\n           [ 0.1059, -0.0431, -0.1686,  ..., -0.6000, -0.5843, -0.5529]]]),\n  tensor([[[-0.2157, -0.2000, -0.2157,  ...,  0.0431,  0.0510,  0.0824],\n           [-0.1922, -0.2157, -0.2392,  ...,  0.0667,  0.0510,  0.0667],\n           [-0.2078, -0.2314, -0.2549,  ...,  0.1529,  0.1451,  0.1608],\n           ...,\n           [ 0.1373, -0.0196, -0.0118,  ..., -0.8980, -0.8902, -0.9137],\n           [ 0.0510, -0.0353, -0.1216,  ..., -0.8824, -0.8824, -0.8980],\n           [ 0.0353, -0.1373, -0.2392,  ..., -0.9059, -0.8980, -0.9059]],\n  \n          [[-0.3020, -0.2863, -0.3020,  ...,  0.0431,  0.0510,  0.0824],\n           [-0.2706, -0.2941, -0.3255,  ...,  0.0667,  0.0510,  0.0667],\n           [-0.2784, -0.3098, -0.3412,  ...,  0.1529,  0.1451,  0.1608],\n           ...,\n           [ 0.0118, -0.1529, -0.1608,  ..., -0.9137, -0.9059, -0.9294],\n           [-0.0745, -0.1843, -0.2706,  ..., -0.9216, -0.9216, -0.9294],\n           [-0.0902, -0.2706, -0.3882,  ..., -0.9529, -0.9451, -0.9373]],\n  \n          [[-0.3333, -0.3333, -0.3490,  ...,  0.0431,  0.0510,  0.0824],\n           [-0.3098, -0.3333, -0.3725,  ...,  0.0667,  0.0510,  0.0667],\n           [-0.3176, -0.3490, -0.3725,  ...,  0.1529,  0.1451,  0.1765],\n           ...,\n           [ 0.0510, -0.1294, -0.1373,  ..., -0.9059, -0.8980, -0.9216],\n           [-0.0588, -0.1686, -0.2549,  ..., -0.9137, -0.9059, -0.9216],\n           [-0.0824, -0.2706, -0.3882,  ..., -0.9373, -0.9294, -0.9294]]]),\n  tensor([[[-0.1765, -0.1216, -0.0824,  ...,  0.2863,  0.2627,  0.2392],\n           [-0.1843, -0.1451, -0.1216,  ...,  0.2941,  0.2706,  0.2549],\n           [-0.1765, -0.1608, -0.1608,  ...,  0.3020,  0.2863,  0.2706],\n           ...,\n           [-0.5451, -0.5843, -0.6078,  ...,  0.1608,  0.1529,  0.1686],\n           [-0.5922, -0.6235, -0.6392,  ...,  0.1529,  0.1529,  0.1608],\n           [-0.6000, -0.6157, -0.6314,  ...,  0.1529,  0.1451,  0.1529]],\n  \n          [[-0.7647, -0.7020, -0.6627,  ..., -0.3647, -0.3882, -0.4118],\n           [-0.7725, -0.7255, -0.7020,  ..., -0.3412, -0.3647, -0.3882],\n           [-0.7725, -0.7490, -0.7490,  ..., -0.3333, -0.3490, -0.3647],\n           ...,\n           [-0.8588, -0.8980, -0.9137,  ..., -0.3098, -0.3020, -0.2863],\n           [-0.9059, -0.9373, -0.9529,  ..., -0.3098, -0.3020, -0.2941],\n           [-0.9137, -0.9294, -0.9451,  ..., -0.3176, -0.3098, -0.3020]],\n  \n          [[-0.9294, -0.8902, -0.8745,  ..., -0.7725, -0.7961, -0.8196],\n           [-0.9137, -0.8980, -0.8980,  ..., -0.7569, -0.7804, -0.7961],\n           [-0.8902, -0.8980, -0.9216,  ..., -0.7490, -0.7647, -0.7804],\n           ...,\n           [-0.9608, -0.9843, -0.9843,  ..., -0.8824, -0.8745, -0.8588],\n           [-0.9843, -1.0000, -1.0000,  ..., -0.8902, -0.8745, -0.8667],\n           [-0.9765, -0.9922, -1.0000,  ..., -0.8902, -0.8824, -0.8745]]]),\n  tensor([[[ 0.4824,  0.4824,  0.4588,  ...,  0.4588,  0.5373,  0.6000],\n           [ 0.4745,  0.4824,  0.4667,  ...,  0.4431,  0.5216,  0.6078],\n           [ 0.4588,  0.4902,  0.4745,  ...,  0.4196,  0.4980,  0.6078],\n           ...,\n           [ 0.6549,  0.6549,  0.6549,  ...,  0.7176,  0.7255,  0.7333],\n           [ 0.6471,  0.6471,  0.6471,  ...,  0.7020,  0.7020,  0.7098],\n           [ 0.6471,  0.6471,  0.6471,  ...,  0.6941,  0.6863,  0.6941]],\n  \n          [[ 0.2157,  0.2235,  0.1922,  ...,  0.1373,  0.2235,  0.2863],\n           [ 0.2078,  0.2235,  0.1922,  ...,  0.1216,  0.2078,  0.2941],\n           [ 0.1843,  0.2078,  0.2000,  ...,  0.0902,  0.1765,  0.2941],\n           ...,\n           [ 0.5451,  0.5451,  0.5451,  ...,  0.5451,  0.5529,  0.5608],\n           [ 0.5373,  0.5373,  0.5373,  ...,  0.5373,  0.5373,  0.5451],\n           [ 0.5373,  0.5373,  0.5373,  ...,  0.5373,  0.5294,  0.5373]],\n  \n          [[-0.0824, -0.0902, -0.1294,  ..., -0.0510,  0.0275,  0.0824],\n           [-0.0980, -0.0980, -0.1294,  ..., -0.0667,  0.0118,  0.0902],\n           [-0.1294, -0.1216, -0.1294,  ..., -0.0902, -0.0196,  0.0902],\n           ...,\n           [ 0.3412,  0.3412,  0.3412,  ...,  0.2549,  0.2627,  0.2706],\n           [ 0.3333,  0.3333,  0.3333,  ...,  0.2471,  0.2471,  0.2549],\n           [ 0.3333,  0.3333,  0.3333,  ...,  0.2471,  0.2392,  0.2471]]]),\n  tensor([[[-0.0039,  0.0039, -0.0431,  ..., -0.2471, -0.2235, -0.1686],\n           [ 0.0353,  0.0353, -0.0275,  ..., -0.2863, -0.2784, -0.2235],\n           [ 0.0588,  0.0510,  0.0902,  ..., -0.3412, -0.3882, -0.3647],\n           ...,\n           [ 0.2941,  0.2863,  0.2863,  ...,  0.7569,  0.7725,  0.7333],\n           [ 0.3098,  0.3176,  0.3255,  ...,  0.7490,  0.7725,  0.7490],\n           [ 0.3255,  0.3176,  0.3098,  ...,  0.7333,  0.7412,  0.7412]],\n  \n          [[-0.6235, -0.6078, -0.6392,  ..., -0.5451, -0.5216, -0.4667],\n           [-0.5843, -0.5765, -0.6235,  ..., -0.5843, -0.5765, -0.5137],\n           [-0.5529, -0.5529, -0.4980,  ..., -0.6314, -0.6784, -0.6471],\n           ...,\n           [ 0.0824,  0.0745,  0.0667,  ...,  0.8431,  0.8510,  0.8118],\n           [ 0.1137,  0.1216,  0.1137,  ...,  0.8275,  0.8510,  0.8275],\n           [ 0.1451,  0.1294,  0.1216,  ...,  0.8196,  0.8196,  0.8196]],\n  \n          [[-0.5843, -0.5686, -0.6078,  ..., -0.5686, -0.5451, -0.4902],\n           [-0.5451, -0.5451, -0.5922,  ..., -0.6078, -0.6000, -0.5451],\n           [-0.5137, -0.5216, -0.4667,  ..., -0.6784, -0.7176, -0.6863],\n           ...,\n           [-0.4588, -0.4667, -0.4745,  ...,  0.7412,  0.7647,  0.7255],\n           [-0.4118, -0.4039, -0.4118,  ...,  0.7333,  0.7647,  0.7412],\n           [-0.3725, -0.3961, -0.4039,  ...,  0.7255,  0.7333,  0.7333]]])]}"
  },
  {
    "objectID": "posts/05wk-1.html#e.-모델생성",
    "href": "posts/05wk-1.html#e.-모델생성",
    "title": "05wk-1: Food-101 이미지자료 분류",
    "section": "E. 모델생성",
    "text": "E. 모델생성\n- 아래의 코드를 관찰하자.\n\nlabels = food[\"train\"].features[\"label\"].names\nlabel2id, id2label = dict(), dict()\nfor i, label in enumerate(labels):\n    label2id[label] = str(i)\n    id2label[str(i)] = label\n\n- 아래와같이 해도 별로 상관없음\n\nlabels = food[\"train\"].features[\"label\"].names\nlabel2id, id2label = dict(), dict()\nfor i, label in enumerate(labels):\n    label2id[label] = i\n    id2label[i] = label\n\n\nmodel = transformers.AutoModelForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224-in21k\",\n    num_labels=len(labels),\n    id2label=id2label,\n    label2id=label2id,\n)\n\nSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n- model을 이용하여 숫자계산하기\n\nmodel(이미지처리하기(img).reshape(1,3,224,224))\n\nImageClassifierOutput(loss=None, logits=tensor([[ 0.1802,  0.0154, -0.0286,  0.1381,  0.1594,  0.1030, -0.0519, -0.0977,\n          0.0769,  0.0004, -0.1125,  0.0636, -0.0168,  0.0445, -0.1157,  0.0365,\n         -0.1490, -0.0703, -0.0555,  0.1079,  0.0064, -0.0355, -0.0398,  0.0550,\n         -0.0309,  0.0438,  0.0830,  0.0493,  0.1102, -0.1054,  0.2540, -0.0235,\n         -0.0057, -0.1219, -0.0503,  0.0739, -0.2367, -0.2940, -0.0238,  0.0651,\n         -0.0553,  0.2288, -0.0849, -0.1456,  0.1050,  0.0350, -0.0409,  0.0726,\n          0.0939,  0.0163, -0.0317, -0.0972,  0.1399,  0.0602,  0.0774,  0.1712,\n         -0.1985,  0.2247,  0.0337,  0.1200, -0.2970,  0.2009, -0.0334,  0.0095,\n          0.1017, -0.0090, -0.1260,  0.0018, -0.0021, -0.0268, -0.0512,  0.0898,\n         -0.0089, -0.0047, -0.1235,  0.0556,  0.0534,  0.0927, -0.1971, -0.2613,\n          0.0457,  0.0878,  0.0171,  0.1649, -0.0150,  0.1617, -0.0626,  0.3391,\n          0.0153, -0.0823, -0.1149, -0.1372,  0.0361, -0.2213, -0.0517, -0.2159,\n         -0.0386,  0.0272, -0.1183, -0.1187, -0.0259]],\n       grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n\nmodel(표준화하기(자르고크기조정하기(텐서화하기(img))).reshape(1,3,224,224))\n\nImageClassifierOutput(loss=None, logits=tensor([[ 0.1816,  0.0150, -0.0283,  0.1393,  0.1577,  0.1032, -0.0525, -0.0986,\n          0.0773,  0.0016, -0.1113,  0.0635, -0.0166,  0.0453, -0.1155,  0.0367,\n         -0.1502, -0.0697, -0.0565,  0.1081,  0.0058, -0.0369, -0.0390,  0.0563,\n         -0.0331,  0.0419,  0.0839,  0.0476,  0.1102, -0.1055,  0.2542, -0.0207,\n         -0.0055, -0.1204, -0.0512,  0.0738, -0.2369, -0.2947, -0.0263,  0.0648,\n         -0.0547,  0.2307, -0.0848, -0.1443,  0.1064,  0.0352, -0.0402,  0.0714,\n          0.0937,  0.0166, -0.0309, -0.0991,  0.1390,  0.0605,  0.0772,  0.1714,\n         -0.1967,  0.2253,  0.0351,  0.1178, -0.2961,  0.1996, -0.0329,  0.0103,\n          0.1010, -0.0100, -0.1249,  0.0024, -0.0022, -0.0272, -0.0524,  0.0893,\n         -0.0066, -0.0054, -0.1212,  0.0565,  0.0547,  0.0920, -0.1956, -0.2616,\n          0.0440,  0.0879,  0.0146,  0.1640, -0.0156,  0.1637, -0.0607,  0.3420,\n          0.0156, -0.0815, -0.1170, -0.1376,  0.0354, -0.2222, -0.0502, -0.2163,\n         -0.0399,  0.0273, -0.1197, -0.1212, -0.0265]],\n       grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)"
  },
  {
    "objectID": "posts/05wk-1.html#f.-데이터콜렉터",
    "href": "posts/05wk-1.html#f.-데이터콜렉터",
    "title": "05wk-1: Food-101 이미지자료 분류",
    "section": "F. 데이터콜렉터",
    "text": "F. 데이터콜렉터\n- 데이터 콜렉터 생성\n\ndata_collator = transformers.DefaultDataCollator()\n\n- 데이터콜렉터에게 이번에 크게 기대하는건 없음.. 그냥 미니배치 정도만 묶어주면 됨\n\n# food_transformed['train'][:2]['pixel_values']\n# 이 자료는 [(3,224,224)-tensor, (3,224,224)-tensor] 와 같은 구조임\n# 그런데 이 자료를 그대로 model에 넣는다면 돌아가지 않음. \n# 우리는 이 자료를 (2,3,224,224)-tensor 로 바꾸어 모델에 입력해주어야함\n\n\nimport torch\nmodel(torch.stack(food_transformed['train'][:2]['pixel_values'],axis=0))\n\nImageClassifierOutput(loss=None, logits=tensor([[ 0.0233, -0.0542, -0.0910,  0.1429, -0.1218, -0.0605, -0.0512, -0.0149,\n         -0.0553,  0.2365, -0.1246,  0.0300, -0.1719,  0.1026, -0.0445,  0.0596,\n         -0.1646,  0.0445,  0.0357,  0.1003,  0.0470, -0.0338, -0.0120, -0.0312,\n         -0.0051, -0.1225,  0.0043, -0.0344,  0.1157, -0.0850,  0.1205, -0.0747,\n         -0.0633,  0.0550, -0.0845, -0.0268,  0.0171, -0.1168, -0.0548, -0.1143,\n         -0.1137, -0.0625, -0.0302, -0.0451, -0.0190,  0.0454, -0.2513,  0.1725,\n          0.0349, -0.0302, -0.0219, -0.0209,  0.0818,  0.1077,  0.1461,  0.0042,\n          0.1550, -0.0360, -0.0108, -0.0347, -0.0197, -0.0982, -0.0238,  0.2078,\n          0.1164, -0.0749,  0.0697,  0.0003, -0.1077,  0.0191,  0.0363,  0.0125,\n         -0.0580, -0.1021,  0.1538,  0.0011, -0.0077, -0.0038, -0.1236,  0.0141,\n         -0.1613,  0.0853,  0.0560,  0.0409, -0.0311, -0.1613, -0.0110, -0.0388,\n          0.1005,  0.0400,  0.1122, -0.0187,  0.1437,  0.0864,  0.0124, -0.0462,\n          0.0298,  0.0651, -0.0109, -0.0456,  0.1503],\n        [-0.0585,  0.0181,  0.1524, -0.1290, -0.1309, -0.1400,  0.0890, -0.0852,\n         -0.0480, -0.0018, -0.0519, -0.1151, -0.0754,  0.1088,  0.0568,  0.0252,\n         -0.0450, -0.0030,  0.0841, -0.0182,  0.0671, -0.0790, -0.0958,  0.1042,\n          0.0610, -0.0674,  0.0663, -0.1341,  0.0101, -0.0504,  0.1105,  0.0552,\n         -0.0276, -0.0005, -0.1353,  0.1348,  0.0501,  0.0253, -0.0275,  0.0402,\n          0.1875,  0.0109,  0.1450, -0.0828, -0.1221, -0.0034, -0.0355,  0.0660,\n         -0.0897, -0.0655, -0.0265,  0.0661,  0.1576,  0.1308, -0.0796, -0.0669,\n         -0.0257, -0.0299,  0.0725,  0.0683,  0.1621,  0.0411,  0.0669,  0.0807,\n         -0.0022,  0.0084, -0.0118, -0.0027,  0.1247, -0.0171,  0.0109, -0.0052,\n          0.0470, -0.0390, -0.0725,  0.0029, -0.1582,  0.0436, -0.0686, -0.1171,\n          0.0717,  0.0436,  0.0800,  0.0232,  0.0424, -0.1711, -0.0972,  0.0443,\n          0.1868, -0.0531, -0.0100, -0.0384, -0.0978,  0.0116,  0.0257, -0.0588,\n         -0.1156,  0.1142,  0.1482, -0.0009, -0.0891]],\n       grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n\ntorch.stack(food_transformed['train'][:2]['pixel_values'],axis=0) # 이코드 어렵죠\n\ntensor([[[[ 0.8588,  0.8745,  0.8588,  ..., -0.0902, -0.1529, -0.1765],\n          [ 0.8431,  0.8510,  0.8588,  ..., -0.1216, -0.1529, -0.1765],\n          [ 0.8431,  0.8353,  0.8353,  ..., -0.0902, -0.1451, -0.2000],\n          ...,\n          [ 0.9294,  0.9294,  0.9216,  ...,  0.6157,  0.6235,  0.6078],\n          [ 0.9216,  0.9294,  0.9216,  ...,  0.6235,  0.6314,  0.6392],\n          [ 0.9216,  0.9216,  0.9294,  ...,  0.6314,  0.6314,  0.6471]],\n\n         [[ 0.7569,  0.7490,  0.7333,  ...,  0.4039,  0.3412,  0.3020],\n          [ 0.7647,  0.7569,  0.7490,  ...,  0.3569,  0.3255,  0.2941],\n          [ 0.7569,  0.7490,  0.7333,  ...,  0.3569,  0.3020,  0.2784],\n          ...,\n          [ 0.9216,  0.9216,  0.9137,  ...,  0.4980,  0.5059,  0.4902],\n          [ 0.9137,  0.9216,  0.9137,  ...,  0.5059,  0.5137,  0.5216],\n          [ 0.9137,  0.9137,  0.9216,  ...,  0.5137,  0.5137,  0.5294]],\n\n         [[ 0.7882,  0.7882,  0.7882,  ...,  0.5686,  0.4902,  0.4510],\n          [ 0.7882,  0.7725,  0.7647,  ...,  0.5059,  0.4824,  0.4588],\n          [ 0.7961,  0.7569,  0.7333,  ...,  0.5059,  0.4588,  0.4431],\n          ...,\n          [ 0.9059,  0.9059,  0.8980,  ...,  0.4588,  0.4588,  0.4431],\n          [ 0.8980,  0.9059,  0.8980,  ...,  0.4667,  0.4667,  0.4667],\n          [ 0.8980,  0.8980,  0.9059,  ...,  0.4745,  0.4745,  0.4745]]],\n\n\n        [[[-0.1686, -0.2000, -0.2235,  ...,  0.1059,  0.1294,  0.1451],\n          [-0.2000, -0.1843, -0.2235,  ...,  0.0745,  0.0667,  0.0980],\n          [-0.1608, -0.1843, -0.2314,  ...,  0.0667,  0.0510,  0.0588],\n          ...,\n          [ 0.6078,  0.6471,  0.6000,  ...,  0.0431,  0.0196,  0.1059],\n          [ 0.5843,  0.6000,  0.5294,  ...,  0.0196, -0.0196,  0.0275],\n          [ 0.5608,  0.5608,  0.4275,  ...,  0.0039, -0.0275, -0.0431]],\n\n         [[-0.2078, -0.2549, -0.2863,  ...,  0.0902,  0.1137,  0.1294],\n          [-0.2314, -0.2235, -0.2706,  ...,  0.0510,  0.0431,  0.0745],\n          [-0.1843, -0.2235, -0.2706,  ...,  0.0275,  0.0118,  0.0196],\n          ...,\n          [ 0.6471,  0.6784,  0.6314,  ...,  0.0039, -0.0196,  0.0667],\n          [ 0.6157,  0.6314,  0.5608,  ..., -0.0118, -0.0510, -0.0118],\n          [ 0.5922,  0.5922,  0.4588,  ..., -0.0275, -0.0588, -0.0745]],\n\n         [[-0.2314, -0.2784, -0.3098,  ...,  0.1137,  0.1373,  0.1529],\n          [-0.2627, -0.2549, -0.3020,  ...,  0.0824,  0.0667,  0.0980],\n          [-0.2235, -0.2549, -0.3020,  ...,  0.0588,  0.0431,  0.0510],\n          ...,\n          [ 0.6941,  0.7412,  0.6941,  ..., -0.0196, -0.0431,  0.0353],\n          [ 0.6784,  0.7020,  0.6314,  ..., -0.0353, -0.0745, -0.0353],\n          [ 0.6627,  0.6627,  0.5294,  ..., -0.0510, -0.0824, -0.0980]]]])\n\n\n\ndata_collator([food_transformed['train'][0],food_transformed['train'][1]])['pixel_values'].shape # 이렇게 씀\n\ntorch.Size([2, 3, 224, 224])\n\n\n\n\n\n\n\n\n강의노트 오타수정\n\n\n\n이전 강의노트에서 오타가 있었습니다.\n## 수정전 \ndata_collator([food['train'][0],food['train'][1]])['pixel_values'].shape # 이렇게 씀\n\n## 수정후 \ndata_collator([food_transformed['train'][0],food_transformed['train'][1]])['pixel_values'].shape # 이렇게 씀"
  },
  {
    "objectID": "posts/05wk-1.html#g.-추론",
    "href": "posts/05wk-1.html#g.-추론",
    "title": "05wk-1: Food-101 이미지자료 분류",
    "section": "G. 추론",
    "text": "G. 추론\n- 코드정리1에 의하여 이미 학습되어있는 Trainer\n\n# Step4 \nclassifier = transformers.pipeline(\"image-classification\", model=\"my_awesome_food_model/checkpoint-186\")\n\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n\n\n- image url 에서 PIL 오브젝트 만들기: GPT 답변\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n# 이미지 URL\nurl = \"https://example.com/image.jpg\"\n\n# URL에서 이미지 불러오기\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content))\n\n# 이미지 확인\nimage.show()\n\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n# 이미지 URL\nurl = \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Shoyu_ramen%2C_at_Kasukabe_Station_%282014.05.05%29_1.jpg/500px-Shoyu_ramen%2C_at_Kasukabe_Station_%282014.05.05%29_1.jpg\"\n\n# URL에서 이미지 불러오기\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content))\nimage\n\n\n\n\n\n\n\n\n\nclassifier(image)\n\n[{'label': 'ramen', 'score': 0.9681490659713745},\n {'label': 'prime_rib', 'score': 0.6358652710914612},\n {'label': 'pork_chop', 'score': 0.6315569877624512},\n {'label': 'beignets', 'score': 0.6056348085403442},\n {'label': 'hamburger', 'score': 0.603554904460907}]"
  },
  {
    "objectID": "posts/03wk-1.html#a.-1단계",
    "href": "posts/03wk-1.html#a.-1단계",
    "title": "03wk-1: 감성분석 파고들기 (1)",
    "section": "A. 1단계",
    "text": "A. 1단계\n\n인공지능에 대한 이해\n\n- 인공지능 불러오기\n\ntorch.manual_seed(43052)\n인공지능 = model = transformers.AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert/distilbert-base-uncased\", num_labels=2\n)\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n- 인공지능의 정체? 엄청나게 많은 숫자들이 포함된 어떠한 물체 (엄청나게 많은 파라메터들이 포함된 네트워크)\n\n인공지능\n\nDistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)\n\n\n\n인공지능.classifier.weight\n\nParameter containing:\ntensor([[-0.0234,  0.0279,  0.0242,  ...,  0.0091, -0.0063, -0.0133],\n        [ 0.0087,  0.0007, -0.0099,  ...,  0.0183, -0.0007,  0.0295]],\n       requires_grad=True)\n\n\n\n인공지능.pre_classifier.weight\n\nParameter containing:\ntensor([[-0.0122,  0.0030,  0.0301,  ...,  0.0003,  0.0327, -0.0123],\n        [-0.0041,  0.0008,  0.0169,  ..., -0.0163, -0.0117, -0.0135],\n        [ 0.0020, -0.0215, -0.0021,  ...,  0.0016,  0.0102, -0.0483],\n        ...,\n        [-0.0026, -0.0327,  0.0099,  ...,  0.0341, -0.0184, -0.0109],\n        [ 0.0088, -0.0345, -0.0011,  ...,  0.0018,  0.0172, -0.0122],\n        [-0.0148,  0.0147, -0.0184,  ..., -0.0166,  0.0241,  0.0201]],\n       requires_grad=True)\n\n\n- 인공지능? “입력정보 -&gt; 정리된숫자 -&gt; 계산 -&gt; 계산된숫자 -&gt; 출력정보” 의 과정에서 “계산”을 담당.\n\n인공지능이 가지고 있는 숫자들은 “계산”에 사용된다.\n\n- 입력정보에 영화에 대한 부정적 평가에 해당하는 텍스트를 넣는다면?\n\n입력정보_원시텍스트 = \"This movie was a huge disappointment.\"\n정리된숫자_토큰화된자료 = 토크나이저(입력정보_원시텍스트,return_tensors='pt')\n정리된숫자_토큰화된자료\n\n{'input_ids': tensor([[  101,  2023,  3185,  2001,  1037,  4121, 10520,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n\n\n\n인공지능(**정리된숫자_토큰화된자료)\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.1173,  0.0261]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n\n계산된숫자_로짓 = 인공지능(**정리된숫자_토큰화된자료).logits.detach().numpy()\n계산된숫자_로짓\n\narray([[-0.11731046,  0.02610319]], dtype=float32)\n\n\n\n출력정보_확률 = np.exp(계산된숫자_로짓)/np.exp(계산된숫자_로짓).sum() # 0일확률(=부정평가일확률), 1일확률(=긍정평가일확률)\n출력정보_확률\n\narray([[0.46420792, 0.53579205]], dtype=float32)\n\n\n\n출력정보_확률.argmax() # 부정적 영화평가에 대한 인공지능의 예측 \n\n1\n\n\n\n계산된숫자_로짓.argmax() # 부정적 영화평가에 대한 인공지능의 예측 &lt;-- 이렇게 구해도 됩니다.. 왜??\n\n1\n\n\n- 입력정보에 영화에 대한 긍정적 평가에 해당하는 텍스트를 넣는다면?\n\n입력정보_원시텍스트 = \"This was a masterpiece.\"\n정리된숫자_토큰화된자료 = 토크나이저(입력정보_원시텍스트,return_tensors='pt')\n정리된숫자_토큰화된자료\n\n{'input_ids': tensor([[  101,  2023,  2001,  1037, 17743,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n\n\n\n인공지능(**정리된숫자_토큰화된자료)\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.1080,  0.0264]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n\n계산된숫자_로짓 = 인공지능(**정리된숫자_토큰화된자료).logits.detach().numpy()\n계산된숫자_로짓\n\narray([[-0.10802954,  0.02638793]], dtype=float32)\n\n\n\n출력정보_확률 = np.exp(계산된숫자_로짓)/np.exp(계산된숫자_로짓).sum() # 0일확률(=부정평가일확률), 1일확률(=긍정평가일확률)\n출력정보_확률\n\narray([[0.46644613, 0.53355384]], dtype=float32)\n\n\n\n출력정보_확률.argmax() # 긍정적 영화평가에 대한 인공지능의 예측 \n\n1\n\n\n\n계산된숫자_로짓.argmax() # 긍정적 영화평가에 대한 인공지능의 예측\n\n1\n\n\n- 아무숫자나 뱉어내는 듯 \\(\\to\\) 멍청한 인공지능 (옹호: 멍청한건 당연함. 아직 학습전이니까)\n- 인공지능에 대한 이해1: 인공지능은 “정리된숫자”를 입력으로 하고 일련의 계산을 거쳐서 “계산된숫자”를 출력해주는 함수라 생각할 수 있음.\n- 인공지능에 대한 이해2: 인공지능은 (1) 많은숫자들과 (2) 고유의 계산방식을 가지고 있음.\n\n인공지능이 내부에 자체적으로 저장하고 있는 숫자들 “파라메터”라고 부름.\n인공지능은 나름의 법칙에 따라 “데이터”와 “파라메터”의 숫자들을 연산함. 즉 인공지능은 자체적으로 데이터와 파라메터를 어떻게 계산할지 알고있는데, 이러한 고유의 계산방식을 “아키텍처”라고 말함.\n\n- 인공지능에 대한 이해3: 두개의 인공지능이 서로 다른 고유의 계산방식을 가지고 있다면 두 인공지능은 “다른 모델” 임.\n- 인공지능에 대한 이해3’: 동일한 생성방식으로 만들어진 인공지능들은 모두 같은 모델임. 예를들면 아래의 인공지능1,2는 같은 모델임\n인공지능1 = transformers.AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert/distilbert-base-uncased\", num_labels=2\n)\n인공지능2 = transformers.AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert/distilbert-base-uncased\", num_labels=2\n)\n- 인공지능에 대한 이해4: 두 인공지능이 같은모델이라고 해도, 항상 같은 결과를 주는건 아님. 파라메터에 따라 다른 결과를 줄 수도 있음. (예를들면 위의 인공지능1,2는 같은 모델이지만 다른 파라메터를 가지므로 다른 결과를 줌)"
  },
  {
    "objectID": "posts/03wk-1.html#b.-2단계",
    "href": "posts/03wk-1.html#b.-2단계",
    "title": "03wk-1: 감성분석 파고들기 (1)",
    "section": "B. 2단계",
    "text": "B. 2단계\n\n미니배치의 이해\n\n- 예비학습\n\narr = np.array([[1,2],[2,3],[3,4]])\narr\n\narray([[1, 2],\n       [2, 3],\n       [3, 4]])\n\n\n\narr / arr.sum(axis=1).reshape(-1,1)\n\narray([[0.33333333, 0.66666667],\n       [0.4       , 0.6       ],\n       [0.42857143, 0.57142857]])\n\n\n- 예비개념1: 인공지능은 사실 영화평을 하나씩 하나씩 처리하지 않는다. 덩어리로 처리한다.\n- 예비개념2: 그렇다고 해서 인공지능이 25000개를 모두 덩어리로 처리하는건 아니다 \\(\\to\\) 16개씩 혹은 32개씩 묶어서 작은덩어리를 만든 후 처리한다.\n\n16, 32와 같은 숫자를 batch_size 라고 한다.\n16개, 32개로 모인 작은덩어리를 미니배치라고 한다.\n\n- 16개의 입력정보를 한번에 처리\n\n입력정보들_원시텍스트 = 데이터['train'][:16]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n출력정보들_확률 = np.exp(계산된숫자들_로짓) / np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\n출력정보들_확률\n\narray([[0.47823825, 0.5217617 ],\n       [0.47215328, 0.5278467 ],\n       [0.4804948 , 0.5195052 ],\n       [0.46609667, 0.5339033 ],\n       [0.48814487, 0.5118551 ],\n       [0.48004225, 0.5199578 ],\n       [0.49288097, 0.50711906],\n       [0.49470255, 0.5052974 ],\n       [0.4941453 , 0.50585467],\n       [0.49016201, 0.50983804],\n       [0.495789  , 0.504211  ],\n       [0.48260576, 0.51739424],\n       [0.49386355, 0.5061364 ],\n       [0.49013382, 0.5098662 ],\n       [0.48240176, 0.5175982 ],\n       [0.49301967, 0.5069803 ]], dtype=float32)\n\n\n\n계산된숫자들_로짓.argmax(axis=1) # 인공지능의 예측\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n\n- 기억할 것: 정리된숫자들_토큰화된자료 는 모두 길이가 512임. (그렇게 되도록 패딩함)\n\n정리된숫자들_토큰화된자료['input_ids'].shape, 정리된숫자들_토큰화된자료['attention_mask'].shape\n\n(torch.Size([16, 512]), torch.Size([16, 512]))\n\n\n- 실제 단어수\n\n정리된숫자들_토큰화된자료['attention_mask'].sum(axis=1)\n\ntensor([363, 304, 133, 185, 495, 154, 143, 388, 512, 297, 365, 171, 192, 173,\n        470, 263])\n\n\n- 패딩된단어수\n\n512-정리된숫자들_토큰화된자료['attention_mask'].sum(axis=1)\n\ntensor([149, 208, 379, 327,  17, 358, 369, 124,   0, 215, 147, 341, 320, 339,\n         42, 249])\n\n\n- 이러한 변환이 필요한 이유? 인공지능은 항상 (n,m) 차원으로 정리된 숫자들만 입력으로 받을 수 있음.\n\n왜? 사실 인공지능은 행렬계산을 하도록 설계되어있음.\n그래서 할수없이 padding을 하거나 truncation을 하는 것임. (실제로는 행렬이 아니지만 억지로 행렬을 만들기 위해서)"
  },
  {
    "objectID": "posts/03wk-1.html#c.-3단계",
    "href": "posts/03wk-1.html#c.-3단계",
    "title": "03wk-1: 감성분석 파고들기 (1)",
    "section": "C. 3단계",
    "text": "C. 3단계\n\n동적패딩을 이해하자.\n\n- 만약에 batch_size=4로 설정하여 처리한다면?\n\n입력정보들_원시텍스트 = 데이터['train'][:4]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n출력정보들_확률 = np.exp(계산된숫자들_로짓) / np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\n출력정보들_확률\n\narray([[0.47823825, 0.5217618 ],\n       [0.47215328, 0.5278467 ],\n       [0.4804948 , 0.5195052 ],\n       [0.46609667, 0.5339033 ]], dtype=float32)\n\n\n\n계산된숫자들_로짓.argmax(axis=1) # 인공지능의 예측 \n\narray([1, 1, 1, 1])\n\n\n- 정리된숫자들_토큰화된자료['input_ids'] 의 차원은 어떠할까? (4,512)\n\n정리된숫자들_토큰화된자료['input_ids'].shape, 정리된숫자들_토큰화된자료['attention_mask'].shape\n\n(torch.Size([4, 363]), torch.Size([4, 363]))\n\n\n\n끝의 차원이 512가 아니라 363이다.. 왜??\n\n- 덩어리의 상태에 따라서 유동적으로 패딩 \\(\\to\\) 이렇게 해도 잘 돌아감\n\n입력정보들_원시텍스트 = 데이터['train'][:4]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n인공지능(**정리된숫자들_토큰화된자료)\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.0847,  0.0024],\n        [-0.0830,  0.0285],\n        [-0.0600,  0.0180],\n        [-0.0919,  0.0440]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n\n입력정보들_원시텍스트 = 데이터['train'][4:8]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n인공지능(**정리된숫자들_토큰화된자료)\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.0422,  0.0053],\n        [-0.0646,  0.0152],\n        [-0.0172,  0.0112],\n        [-0.0283, -0.0072]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n\n입력정보들_원시텍스트 = 데이터['train'][8:12]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n인공지능(**정리된숫자들_토큰화된자료)\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.0246, -0.0012],\n        [-0.0594, -0.0200],\n        [-0.0240, -0.0071],\n        [-0.0836, -0.0140]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n\n입력정보들_원시텍스트 = 데이터['train'][12:16]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n인공지능(**정리된숫자들_토큰화된자료)\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.0440, -0.0195],\n        [-0.0469, -0.0074],\n        [-0.0542,  0.0162],\n        [-0.0316, -0.0037]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n- 싹다 maxlen=512로 가정하고 패딩해서 돌린결과와 비교 \\(\\to\\) 같음 \\(\\to\\) 동적패딩이 효율적\n\n입력정보들_원시텍스트 = 데이터['train'][:16]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n인공지능(**정리된숫자들_토큰화된자료)\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-0.0847,  0.0024],\n        [-0.0830,  0.0285],\n        [-0.0600,  0.0180],\n        [-0.0919,  0.0440],\n        [-0.0422,  0.0053],\n        [-0.0646,  0.0152],\n        [-0.0172,  0.0112],\n        [-0.0283, -0.0072],\n        [-0.0246, -0.0012],\n        [-0.0594, -0.0200],\n        [-0.0240, -0.0071],\n        [-0.0836, -0.0140],\n        [-0.0440, -0.0195],\n        [-0.0469, -0.0074],\n        [-0.0542,  0.0162],\n        [-0.0316, -0.0037]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)"
  },
  {
    "objectID": "posts/03wk-1.html#d.-4단계",
    "href": "posts/03wk-1.html#d.-4단계",
    "title": "03wk-1: 감성분석 파고들기 (1)",
    "section": "D. 4단계",
    "text": "D. 4단계\n\n손실(=loss)의 개념을 이해하자\n\n- 정리된숫자들_토큰화된자료에서 labels를 추가 전달하면, 인공지능(**정리된숫자들_토큰화된자료)의 결과로 loss가 추가계산됨.\n\n입력정보들_원시텍스트 = 데이터['train'][:4]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n#데이터['train'][:4]['label'] # 정답확인\n정리된숫자들_토큰화된자료['labels'] = torch.tensor([0,0,0,0]) # 정답입력\n인공지능(**정리된숫자들_토큰화된자료)\n\nSequenceClassifierOutput(loss=tensor(0.7461, grad_fn=&lt;NllLossBackward0&gt;), logits=tensor([[-0.0847,  0.0024],\n        [-0.0830,  0.0285],\n        [-0.0600,  0.0180],\n        [-0.0919,  0.0440]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n- 정리를 해보자.\n\n입력정보들_원시텍스트 = 데이터['train'][:4]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\nprint(f'실제정답: {데이터['train'][:4]['label']}') # 정답확인\n정리된숫자들_토큰화된자료['labels'] = torch.tensor([0,0,0,0]) # 정답입력\n계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n출력정보들_확률 = np.exp(계산된숫자들_로짓)/np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\nprint(f'인공지능의예측: {계산된숫자들_로짓.argmax(axis=1)}')\nprint(f'인공지능의확신정도: {출력정보들_확률.max(axis=1)}')\nprint(f'손실(loss): {인공지능(**정리된숫자들_토큰화된자료).loss.item():.6f}')\n\n실제정답: [0, 0, 0, 0]\n인공지능의예측: [1 1 1 1]\n인공지능의확신정도: [0.5217618 0.5278467 0.5195052 0.5339033]\n손실(loss): 0.746100\n\n\n- loss는 작을수록 주어진 데이터에 대한 정답을 잘 맞추고 있다고 볼 수 있음. (그렇지만 학습이 잘 되었다는걸 보장하지는 못함)\n텍스트0-텍스트3\n\nprint(\"텍스트0 -- 텍스트3\")\n입력정보들_원시텍스트 = 데이터['train'][:4]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\nprint(f'실제정답: {데이터['train'][:4]['label']}') # 정답확인\n정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][:4]['label']) # 정답입력\n계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n출력정보들_확률 = np.exp(계산된숫자들_로짓)/np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\nprint(f'인공지능의예측: {계산된숫자들_로짓.argmax(axis=1)}')\nprint(f'인공지능의확신정도: {출력정보들_확률.max(axis=1)}')\nprint(f'손실(loss): {인공지능(**정리된숫자들_토큰화된자료).loss.item():.6f}')\n\n텍스트0 -- 텍스트3\n실제정답: [0, 0, 0, 0]\n인공지능의예측: [1 1 1 1]\n인공지능의확신정도: [0.5217618 0.5278467 0.5195052 0.5339033]\n손실(loss): 0.746100\n\n\n텍스트12498-텍스트12501\n\nprint(\"텍스트12498 -- 텍스트12501\")\n입력정보들_원시텍스트 = 데이터['train'][12498:12502]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\nprint(f'실제정답: {데이터['train'][12498:12502]['label']}') # 정답확인\n정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][12498:12502]['label']) # 정답입력\n계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n출력정보들_확률 = np.exp(계산된숫자들_로짓)/np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\nprint(f'인공지능의예측: {계산된숫자들_로짓.argmax(axis=1)}')\nprint(f'인공지능의확신정도: {출력정보들_확률.max(axis=1)}')\nprint(f'손실(loss): {인공지능(**정리된숫자들_토큰화된자료).loss.item():.6f}')\n\n텍스트12498 -- 텍스트12501\n실제정답: [0, 0, 1, 1]\n인공지능의예측: [1 1 1 1]\n인공지능의확신정도: [0.52731967 0.5243837  0.51578873 0.5351162 ]\n손실(loss): 0.694952\n\n\n텍스트12502-텍스트12506\n\nprint(\"텍스트12502 -- 텍스트12506\")\n입력정보들_원시텍스트 = 데이터['train'][12502:12506]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\nprint(f'실제정답: {데이터['train'][12502:12506]['label']}') # 정답확인\n정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][12502:12506]['label']) # 정답입력\n계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n출력정보들_확률 = np.exp(계산된숫자들_로짓)/np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\nprint(f'인공지능의예측: {계산된숫자들_로짓.argmax(axis=1)}')\nprint(f'인공지능의확신정도: {출력정보들_확률.max(axis=1)}')\nprint(f'손실(loss): {인공지능(**정리된숫자들_토큰화된자료).loss.item():.6f}')\n\n텍스트12502 -- 텍스트12506\n실제정답: [1, 1, 1, 1]\n인공지능의예측: [1 1 1 1]\n인공지능의확신정도: [0.52180934 0.51908445 0.521884   0.5197189 ]\n손실(loss): 0.652730\n\n\n- 똑같이 틀려도 오답에 대한 확신이 강할수록 loss가 크다.\n\nprint(\"텍스트0 -- 텍스트1\")\n입력정보들_원시텍스트 = 데이터['train'][:2]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\nprint(f'실제정답: {데이터['train'][:2]['label']}') # 정답확인\n정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][:2]['label']) # 정답입력\n계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n출력정보들_확률 = np.exp(계산된숫자들_로짓)/np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\nprint(f'인공지능의예측: {계산된숫자들_로짓.argmax(axis=1)}')\nprint(f'인공지능의확신정도: {출력정보들_확률.max(axis=1)}')\nprint(f'손실(loss): {인공지능(**정리된숫자들_토큰화된자료).loss.item():.6f}')\n\n텍스트0 -- 텍스트1\n실제정답: [0, 0]\n인공지능의예측: [1 1]\n인공지능의확신정도: [0.5217617 0.5278467]\n손실(loss): 0.744049\n\n\n\nprint(\"텍스트1 -- 텍스트2\")\n입력정보들_원시텍스트 = 데이터['train'][1:3]['text']\n정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\nprint(f'실제정답: {데이터['train'][1:3]['label']}') # 정답확인\n정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][1:3]['label']) # 정답입력\n계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n출력정보들_확률 = np.exp(계산된숫자들_로짓)/np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\nprint(f'인공지능의예측: {계산된숫자들_로짓.argmax(axis=1)}')\nprint(f'인공지능의확신정도: {출력정보들_확률.max(axis=1)}')\nprint(f'손실(loss): {인공지능(**정리된숫자들_토큰화된자료).loss.item():.6f}')\n\n텍스트1 -- 텍스트2\n실제정답: [0, 0]\n인공지능의예측: [1 1]\n인공지능의확신정도: [0.5278467 0.5195052]\n손실(loss): 0.741695\n\n\n\n\n\n\n\n\n\\(\\star\\star\\star\\) 학습이 가능한 이유 (대충 아이디어만)\n\n\n\n1. 랜덤으로 하나의 인공지능을 생성한다. (아래의 코드로 가능)\n인공지능 = 인공지능생성기()\n2. 하나의 미니배치를 선택한다.\n3. 인공지능의 파라메터중 하나의 숫자를 선택한다. 예를들면 아래와 같은 상황이 있다고 하자.\n인공지능.classifier.weight\nParameter containing:\ntensor([[-0.0234,  0.0279,  0.0242,  ...,  0.0091, -0.0063, -0.0133],\n        [ 0.0087,  0.0007, -0.0099,  ...,  0.0183, -0.0007,  0.0295]],\n       requires_grad=True)\n하나의 숫자 -0.0234를 선택한다.\n4. -0.0234의 값을 아주 조금 변화시킨다. 예를들면 -0.0233, -0.0235 와 같은 숫자로 바꾼다. 2에서 고정된 미니배치에 대하여 -0.0234, -0.0233, -0.0235 에 대한 loss를 계산해보고 비교한다.\n\n-0.0234이 최저 loss라면? 값을 안바꾸는게 좋겠음.\n-0.0233이 최저 loss라면?? 값을 -0.0233으로 바꿈.\n-0.0235이 최저 loss라면?? 값을 -0.0235으로 바꿈.\n\n5. 다음은 다른 모든 파라메터에 대하여 3-4을 반복한다. (과정을 반복할수록 loss는 작아지겠죠, 즉 인공지능은 정답을 잘 맞추겠죠)\n6. 다른 미니배치에 대하여 2-5를 반복한다.\n\n\n- 인공지능의 학습은 마법같은 신비한 현상이 아니고, 극한의 노가다를 통해 얻어지는 산물일 뿐이다."
  },
  {
    "objectID": "quiz/Quiz-7.html",
    "href": "quiz/Quiz-7.html",
    "title": "Quiz-7 (2024.11.19) // 범위: 09wk-2 까지",
    "section": "",
    "text": "항목\n허용 여부\n비고\n\n\n\n\n강의노트 참고\n허용\n수업 중 제공된 강의노트나 본인이 정리한 자료를 참고 가능\n\n\n구글 검색\n허용\n인터넷을 통한 자료 검색 및 정보 확인 가능\n\n\n생성 모형 사용\n허용 안함\n인공지능 기반 도구(GPT 등) 사용 불가"
  },
  {
    "objectID": "quiz/Quiz-4.html",
    "href": "quiz/Quiz-4.html",
    "title": "Quiz-4 (2024.10.22) // 범위: 05wk-1 까지",
    "section": "",
    "text": "항목\n허용 여부\n비고\n\n\n\n\n강의노트 참고\n허용\n수업 중 제공된 강의노트나 본인이 정리한 자료를 참고 가능\n\n\n구글 검색\n허용\n인터넷을 통한 자료 검색 및 정보 확인 가능\n\n\n생성 모형 사용\n허용 안함\n인공지능 기반 도구(GPT 등) 사용 불가\n\n\n\n\n\nimport datasets\nimport transformers\nimport torch\nimport torchvision.transforms\nimport evaluate\nimport numpy as np\n\n/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n아래는 datasets 라이브러리의 load_dataset 함수를 사용하여 beans라는 데이터셋을 불러오는 코드이다.\n\nbeans = datasets.load_dataset('beans')\nbeans\n\nDatasetDict({\n    train: Dataset({\n        features: ['image_file_path', 'image', 'labels'],\n        num_rows: 1034\n    })\n    validation: Dataset({\n        features: ['image_file_path', 'image', 'labels'],\n        num_rows: 133\n    })\n    test: Dataset({\n        features: ['image_file_path', 'image', 'labels'],\n        num_rows: 128\n    })\n})\n\n\nbeans 데이터셋은 train(1034개), validation(133개), test(128개)로 나뉘며, 각각 모델 학습, 검증, 성능 평가에 사용된다. 각 데이터셋은 이미지 파일 경로(image_file_path), 실제 이미지 데이터(image), 그리고 이미지에 대한 질병 라벨(labels)로 구성되어 있다. 이 라벨은 각 이미지가 어떤 질병을 나타내는지에 대한 정보를 제공한다.\n라벨 0은 “각진 잎 반점(angular_leaf_spot)”이라는 질병으로, 이는 콩 잎에 특정 형태의 반점이 생기는 현상을 의미한다. 라벨 1은 “콩 녹병(bean_rust)”으로, 콩 잎에 붉은 녹병이 나타나는 질병이다. 마지막으로 라벨 2는 “healthy(건강한 잎)”을 나타내며, 이 라벨이 붙은 이미지는 질병 없이 건강한 콩 잎을 뜻한다. 이러한 라벨을 통해 데이터셋의 각 이미지가 어떤 질병을 가지고 있는지 쉽게 파악할 수 있다.\n\nid2label = {0: 'angular_leaf_spot', 1: 'bean_rust', 2: 'healthy'}\nlabel2id = {'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2}\n\n\n1. beans 자료분석 – 100점\n\n배점: (1),(2),(3),(4) – 5점, (5),(6) – 30점 (7),(8) – 10점\n\n(1) beans['train'][10] 에 포함된 이미지의 크기와 라벨을 확인하라.\n(풀이)\n\nbeans['train'][10]['image'].size, beans['train'][10]['labels']\n\n((500, 500), 0)\n\n\n(2) torchvision.transforms.RandomResizedCrop을 사용하여 beans['train'][10]에 있는 이미지를 (224, 224) 크기로 변환하는 코드를 작성하라.\n(풀이)\n\nf1 = torchvision.transforms.RandomCrop(224) \nf1(beans['train'][10]['image'])\n\n\n\n\n\n\n\n\n(3) torchvision.transforms.ToTensor를 사용하여 beans['train'][10]에 포함된 이미지를 텐서(Tensor)로 변환하는 코드를 작성하라.\n(풀이)\n\nf2 = torchvision.transforms.ToTensor()\nf2(beans['train'][10]['image'])\n\ntensor([[[0.4706, 0.5255, 0.4549,  ..., 0.4902, 0.4471, 0.3529],\n         [0.5529, 0.5529, 0.4510,  ..., 0.6118, 0.5294, 0.4667],\n         [0.4157, 0.5490, 0.6157,  ..., 0.6706, 0.6000, 0.5333],\n         ...,\n         [0.7490, 0.6549, 0.5137,  ..., 0.3686, 0.3255, 0.2784],\n         [0.7255, 0.6667, 0.5608,  ..., 0.3961, 0.3412, 0.2824],\n         [0.6510, 0.6627, 0.5843,  ..., 0.3804, 0.3490, 0.3020]],\n\n        [[0.2863, 0.3412, 0.2784,  ..., 0.3255, 0.2824, 0.1961],\n         [0.3686, 0.3686, 0.2745,  ..., 0.4314, 0.3608, 0.2980],\n         [0.2275, 0.3608, 0.4353,  ..., 0.4824, 0.4118, 0.3490],\n         ...,\n         [0.7608, 0.6863, 0.5882,  ..., 0.2863, 0.2588, 0.2314],\n         [0.7294, 0.6941, 0.6275,  ..., 0.3137, 0.2745, 0.2353],\n         [0.6510, 0.6824, 0.6471,  ..., 0.2980, 0.2824, 0.2471]],\n\n        [[0.2078, 0.2627, 0.2039,  ..., 0.2706, 0.2275, 0.1490],\n         [0.2902, 0.2902, 0.1922,  ..., 0.3686, 0.2941, 0.2353],\n         [0.1412, 0.2745, 0.3451,  ..., 0.3961, 0.3333, 0.2784],\n         ...,\n         [0.4902, 0.3922, 0.2667,  ..., 0.2039, 0.1961, 0.1843],\n         [0.4706, 0.4196, 0.3216,  ..., 0.2314, 0.2118, 0.1882],\n         [0.4078, 0.4235, 0.3529,  ..., 0.2157, 0.2196, 0.2039]]])\n\n\n(4) torchvision.transforms.Normalize를 사용하여 이미지의 각 채널에 아래와 같은 정규화를 수행하는 변환을 선언하라:\n\n빨강 채널: \\(r \\to \\frac{r - 0.5}{0.5}\\)\n초록 채널: \\(g \\to \\frac{g - 0.5}{0.5}\\)\n파랑 채널: \\(b \\to \\frac{b - 0.5}{0.5}\\)\n\n그리고 torchvision.transforms.Compose를 활용하여 아래 순서대로 변환을 수행하는 코드를 작성하라.\n\n첫 번째: 이미지를 (224, 224) 크기로 무작위로 자르기 (RandomResizedCrop)\n두 번째: 이미지를 텐서로 변환 (ToTensor)\n세 번째: 각 채널별로 주어진 방식대로 정규화 (Normalize)\n\n이를 이용하여 beans['train'][10] 에 포함된 이미지를 변환하라.\n(풀이)\n\nf3 = torchvision.transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\nf = torchvision.transforms.Compose([f1, f2, f3])\nf(beans['train'][10]['image'])\n\ntensor([[[ 0.0431,  0.3725,  0.6863,  ..., -0.2549, -0.2471, -0.2157],\n         [ 0.1059,  0.2157,  0.4980,  ..., -0.2941, -0.2784, -0.2157],\n         [ 0.1451,  0.1451,  0.3804,  ..., -0.2941, -0.2941, -0.2549],\n         ...,\n         [-0.6235, -0.4510, -0.0431,  ...,  0.6706,  0.6549,  0.6471],\n         [-0.6549, -0.3882, -0.0431,  ...,  0.6863,  0.6784,  0.7255],\n         [-0.6549, -0.6471, -0.0902,  ...,  0.6000,  0.6000,  0.6314]],\n\n        [[-0.3647, -0.0745,  0.2157,  ...,  0.1373,  0.1529,  0.1608],\n         [-0.3255, -0.2706, -0.0118,  ...,  0.1059,  0.1216,  0.1765],\n         [-0.2706, -0.2941, -0.0902,  ...,  0.1059,  0.1059,  0.1451],\n         ...,\n         [-0.3255, -0.0902,  0.3961,  ...,  0.9294,  0.9137,  0.8431],\n         [-0.3569, -0.0275,  0.3961,  ...,  0.9059,  0.8980,  0.9216],\n         [-0.3490, -0.2863,  0.3333,  ...,  0.7961,  0.7882,  0.8275]],\n\n        [[-0.5373, -0.3020, -0.0510,  ..., -0.4039, -0.3882, -0.4353],\n         [-0.4824, -0.4667, -0.2471,  ..., -0.4353, -0.4039, -0.3961],\n         [-0.3804, -0.4745, -0.3098,  ..., -0.4196, -0.4196, -0.3961],\n         ...,\n         [-0.6941, -0.5765, -0.3176,  ...,  0.3098,  0.2941,  0.1922],\n         [-0.7255, -0.5294, -0.2863,  ...,  0.3020,  0.2941,  0.2863],\n         [-0.7176, -0.7882, -0.2784,  ...,  0.1608,  0.1765,  0.1922]]])\n\n\n(5) 아래의 코드를 이용하여 모델을 선언하라.\n\nmodel = transformers.AutoModelForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224-in21k\",\n    num_labels=3,\n    id2label=id2label,\n    label2id=label2id,\n)\n\nSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n위의 모델을 이용하여 beans['train'][10]의 이미지에 대한 loss를 구하라.\n(풀이1) – model(...)를 이용\n\nmodel_input = {\n    #'pixel_values':f(beans['train'][10]['image']).reshape(1,3,224,224),\n    'pixel_values':torch.stack([f(beans['train'][10]['image'])],axis=0), \n    'labels':torch.tensor([0])\n}\nmodel(**model_input)\n\nImageClassifierOutput(loss=tensor(1.0682, grad_fn=&lt;NllLossBackward0&gt;), logits=tensor([[ 0.0297, -0.0544,  0.0205]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n(풀이2) – model(**data_collator(...))를 이용1 – (\\(\\star\\)) 추천풀이\n\ndata_collator = transformers.DefaultDataCollator()\ndata_collator_input = [{'pixel_values':f(beans['train'][10]['image']), 'labels':torch.tensor(0)}]\nmodel(**data_collator(data_collator_input))\n\nImageClassifierOutput(loss=tensor(1.0359, grad_fn=&lt;NllLossBackward0&gt;), logits=tensor([[ 0.1254,  0.0804, -0.0237]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n(풀이3) – model(**data_collator(...))를 이용2 – (\\(\\star\\)) 추천풀이\n\ndef transforms(examples_dct):\n    examples_dct[\"pixel_values\"] = [f(img) for img in examples_dct[\"image\"]]\n    del examples_dct[\"image\"]\n    return examples_dct\nbeans_transformed = beans.with_transform(transforms)\ndata_collator = transformers.DefaultDataCollator()\ndata_collator_input = [beans_transformed['train'][10]]  \nmodel(**data_collator(data_collator_input))\n\nImageClassifierOutput(loss=tensor(1.0484, grad_fn=&lt;NllLossBackward0&gt;), logits=tensor([[ 0.0066, -0.1082, -0.0327]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\n(풀이4) – trainer를 이용 (지금상황에서 적합한 풀이는 아닌것 같은데 억지로 풀어보겠습니다..)\n\ndef transforms(examples_dct):\n    examples_dct[\"pixel_values\"] = [f(img) for img in examples_dct[\"image\"]]\n    del examples_dct[\"image\"]\n    return examples_dct\nbeans_transformed = beans.with_transform(transforms)\ndata_collator = transformers.DefaultDataCollator()\ntrainer_dummy = transformers.Trainer(\n    model=model,\n    #args=training_args,\n    data_collator=data_collator,\n    #train_dataset=beans_transformed[\"train\"],\n    #eval_dataset=beans_transformed[\"validation\"],\n    #tokenizer=image_processor,\n    #compute_metrics=compute_metrics,\n)\ntrainer_dummy.predict(\n    datasets.Dataset.from_dict(beans_transformed['train'][10:11])\n)\n\n\n\n\nPredictionOutput(predictions=array([[ 0.03504263, -0.08715277,  0.02764845]], dtype=float32), label_ids=array([0]), metrics={'test_loss': 1.056959629058838, 'test_model_preparation_time': 0.0023, 'test_runtime': 0.3726, 'test_samples_per_second': 2.684, 'test_steps_per_second': 2.684})\n\n\n(6) beans 데이터를 학습하는 코드를 작성하고 모델을 훈련하라.\n주의\n\ntrain 데이터를 훈련용으로, validation 데이터를 검증용으로 사용하라.\n검증자료에 대한 정확도가 90%이상일 경우만 정답으로 인정한다.\n\n(풀이)\n\n## Step1 \n#food = datasets.load_dataset(\"food101\", split=\"train[:5000]\").train_test_split(test_size=0.2)\nimage_processor = transformers.AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\ndef transforms(examples):\n    examples[\"pixel_values\"] = [f(img) for img in examples[\"image\"]]\n    del examples[\"image\"]\n    return examples\nbeans_transformed = beans.with_transform(transforms)\n## Step2 \nmodel = transformers.AutoModelForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224-in21k\",\n    num_labels=3,\n    id2label=id2label,\n    label2id=label2id,\n)\n## Step3 \ndata_collator = transformers.DefaultDataCollator()\naccuracy = evaluate.load(\"accuracy\")\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy.compute(predictions=predictions, references=labels)\ntraining_args = transformers.TrainingArguments(\n    output_dir=\"my_awesome_beans_model\",\n    remove_unused_columns=False,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=16,\n    gradient_accumulation_steps=4,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    warmup_ratio=0.1,\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    push_to_hub=False,\n    report_to=\"none\"\n)\ntrainer = transformers.Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=beans_transformed[\"train\"],\n    eval_dataset=beans_transformed[\"validation\"],\n    tokenizer=image_processor,\n    compute_metrics=compute_metrics,\n)\ntrainer.train()\n## Step4 \n\nFast image processor class &lt;class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'&gt; is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\nSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n    \n      \n      \n      [32/32 00:27, Epoch 1/2]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n0\n1.018900\n0.670571\n0.924812\n\n\n1\n0.564200\n0.490068\n0.954887\n\n\n\n\n\n\nTrainOutput(global_step=32, training_loss=0.7599691525101662, metrics={'train_runtime': 27.98, 'train_samples_per_second': 73.91, 'train_steps_per_second': 1.144, 'total_flos': 1.5824006103590093e+17, 'train_loss': 0.7599691525101662, 'epoch': 1.9692307692307693})\n\n\n(7) 학습된 모델을 사용하여 train, validation, test 데이터셋 각각에 대한 정확도를 계산하라.\n(풀이)\n\ntrainer.predict(beans_transformed['train']).metrics['test_accuracy']\n\n\n\n\n0.9197292069632496\n\n\n\ntrainer.predict(beans_transformed['validation']).metrics['test_accuracy']\n\n\n\n\n0.9323308270676691\n\n\n\ntrainer.predict(beans_transformed['test']).metrics['test_accuracy']\n\n\n\n\n0.8984375\n\n\n(8) beans['train'][10]의 이미지에 대한 loss를 구하라. (5) 과 비교하여 개선점이 있는가?\n(풀이1)\n\ntrainer.predict(\n    datasets.Dataset.from_dict(beans_transformed['train'][10:11])\n)\n\n\n\n\nPredictionOutput(predictions=array([[ 0.51362973,  0.22127132, -0.7328805 ]], dtype=float32), label_ids=array([0]), metrics={'test_loss': 0.710007905960083, 'test_accuracy': 1.0, 'test_runtime': 0.1001, 'test_samples_per_second': 9.99, 'test_steps_per_second': 9.99})\n\n\n(풀이2)\n\ndata_collator_input = [beans_transformed['train'][10]]\nmodel.to(\"cpu\")\nmodel(**data_collator(data_collator_input))\n\nImageClassifierOutput(loss=tensor(0.7825, grad_fn=&lt;NllLossBackward0&gt;), logits=tensor([[ 0.3677,  0.1649, -0.6252]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)"
  },
  {
    "objectID": "quiz/Quiz-2.html",
    "href": "quiz/Quiz-2.html",
    "title": "Quiz-2 (2024.09.24) // 범위: 02wk-1 까지",
    "section": "",
    "text": "항목\n허용 여부\n비고\n\n\n\n\n강의노트 참고\n허용\n수업 중 제공된 강의노트나 본인이 정리한 자료를 참고 가능\n\n\n구글 검색\n허용\n인터넷을 통한 자료 검색 및 정보 확인 가능\n\n\n생성 모형 사용\n허용 안함\n인공지능 기반 도구(GPT 등) 사용 불가\n\n\n\n\n\n# pip install datasets evaluate accelerate\n\n\n1. emotion 자료 탐색 – 10점\n\n(1)-(2) 모두 부분점수 없음.\n\n아래는 Hugging Face의 emotion 데이터셋을 로드하는 코드이다:\n\nfrom datasets import load_dataset\nemotion = load_dataset('emotion')\n\n/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nemotion['train'][-2]는 훈련 데이터의 두 번째 마지막 항목을 출력한다. 출력된 샘플은 다음과 같다.\n\nemotion['train'][-2]\n\n{'text': 'i feel like this was such a rude comment and im glad that t',\n 'label': 3}\n\n\n출력된 샘플은 딕셔너리 형식으로, text에는 문장 “i feel like this was such a rude comment and im glad that t”이 담겨 있다. 이 문장의 감정은 label 항목에 저장되어 있으며, 값은 3으로 나타난다. label 값은 해당 텍스트가 표현하는 감정을 숫자로 표현한 것이다.\n감정 레이블은 총 6가지로 나뉘며, 각각의 감정은 다음과 같이 정의된다:\n\nemotion['train'].features['label'].names\n\n['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n\n\n\n{\n  0: 'sadness',\n  1: 'joy',\n  2: 'love',\n  3: 'anger',\n  4: 'fear',\n  5: 'surprise'\n}\n\n{0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n\n\n따라서, 문장 “i feel like this was such a rude comment and im glad that t” 의 감정은 label이 3이므로, “anger”에 해당한다.\n(1) emotion 데이터셋의 각 분할(train, validation, test)에서 감정별로 몇 개의 데이터가 있는지를 조사하라. 즉 아래의 표에서 빈칸에 해당하는 숫자를 계산할 수 있는 코드를 제시하라. – 5점\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\n0:Sadness\n1:Joy\n2:Love\n3:Anger\n4:Fear\n5:Surprise\nTotal\n\n\n\n\nTrain\n??\n??\n??\n??\n??\n??\n16000\n\n\nValidation\n??\n??\n??\n??\n??\n??\n2000\n\n\nTest\n??\n??\n??\n??\n??\n??\n2000\n\n\n\nnote: 정답예시: 아래와 같은 형식으로 출력하는 코드를 작성하면 정답으로 인정\ntrain\n{0: 4666, 1: 5362, 2: 1304, 3: 2159, 4: 1937, 5: 572}\n--\nvalidation\n{0: 550, 1: 704, 2: 178, 3: 275, 4: 212, 5: 81}\n--\ntest\n{0: 581, 1: 695, 2: 159, 3: 275, 4: 224, 5: 66}\n--\nhint: 아래중 원하는 형태를 이용하여 풀이하면 편리하다.\n\nemotion['train']['label']\nemotion['train'].to_dict()\nemotion['train'].to_pandas()\n\n(풀이)\n\n{key: {i:emotion[key]['label'].count(i) for i in set(emotion[key]['label'])} for key in emotion}\n\n{'train': {0: 4666, 1: 5362, 2: 1304, 3: 2159, 4: 1937, 5: 572},\n 'validation': {0: 550, 1: 704, 2: 178, 3: 275, 4: 212, 5: 81},\n 'test': {0: 581, 1: 695, 2: 159, 3: 275, 4: 224, 5: 66}}\n\n\n(2) emotion 데이터셋의 test셋에서 각 감정(label)별로 가장 짧은 길이를 가진 텍스트를 출력하는 코드를 작성하라. – 5점\nnote: 정답예시는 아래와 같다.\n['i feels so lame',\n 'i feel any better',\n 'i just feel tender',\n 'i feel so damn agitated',\n 'i feel alarmed',\n 'i feel all funny sometimes']\n(풀이1)\n\n_라벨0만 = [dct['text'] for dct in emotion['test'] if dct['label']==0]\nmin(map(len,_라벨0만))\n[txt for txt in _라벨0만 if len(txt)==min(map(len,_라벨0만))]\n\n['i feels so lame']\n\n\n\ndef func(_라벨0만):\n    min(map(len,_라벨0만))\n    return [txt for txt in _라벨0만 if len(txt)==min(map(len,_라벨0만))]\n\n\nlist(map(func,[[dct['text'] for dct in emotion['test'] if dct['label']==lbl] for lbl in range(6)]))\n\n[['i feels so lame'],\n ['i feel any better'],\n ['i just feel tender'],\n ['i feel so damn agitated'],\n ['i feel alarmed'],\n ['i feel all funny sometimes']]\n\n\n(풀이2)\n\ndf = emotion['test'].to_pandas()\ndf['length'] = list(map(len,df['text']))\ndf = df.sort_values('length')\ndf \n\n\n\n\n\n\n\n\ntext\nlabel\nlength\n\n\n\n\n221\ni feel alarmed\n4\n14\n\n\n1632\nid feel frantic\n4\n15\n\n\n634\ni feels so lame\n0\n15\n\n\n873\ni feel soo lonely\n0\n17\n\n\n1389\ni feel any better\n1\n17\n\n\n...\n...\n...\n...\n\n\n611\ni couldnt help feeling for him and this awful ...\n0\n284\n\n\n119\ni feel like i know who most of them are by now...\n1\n287\n\n\n966\ni have the joy of allowing kids to feel like t...\n1\n289\n\n\n475\ni feel very honored to have been shortlisted w...\n1\n294\n\n\n618\ni am feeling a little more relaxed i am certai...\n1\n296\n\n\n\n\n2000 rows × 3 columns\n\n\n\n\nfor _,subdf in df.groupby('label'):\n    display(subdf.text.iloc[0])\n\n'i feels so lame'\n\n\n'i feel any better'\n\n\n'i just feel tender'\n\n\n'i feel so damn agitated'\n\n\n'i feel alarmed'\n\n\n'i feel all funny sometimes'\n\n\n\n\n2. emotion 자료 감성분석 – 80점\n\n(1)은 부분점수 있음, (2)는 부분점수 없음.\n\n(1) 아래의 reference 를 참고하여 emotion에 대한 감성분석모델을 학습하는 코드를 작성하라. – 60점\nref:\n\nhttps://guebin.github.io/MP2024/posts/02wk-1.html\nhttps://huggingface.co/docs/transformers/tasks/sequence_classification\n\n세부지침 – 세부지침을 따르지 않을시 감점이 있음 (지침1은 30점감점 지침2는 5점감점)\n지침1. Trainer생성시 eval_dataset에는 emotion['validation']를 전처리한 데이터를 이용하라. (emotion['test'] 가 아니라)\n지침2. TrainingArguments에서 num_train_epochs은 1로 설정하라.\nhint: imdb 자료의 경우 num_labels = 2 이지만, emotion 자료의 경우 그렇지 않음을 유의하라.\n(풀이)\n\nimport datasets\nimport transformers\nimport evaluate\nimport numpy as np\n\n\n## Step1 \n데이터불러오기 = datasets.load_dataset\n데이터전처리하기1 = 토크나이저 = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") \ndef 데이터전처리하기2(examples):\n    return 데이터전처리하기1(examples[\"text\"], truncation=True)\n## Step2 \n인공지능생성하기 = transformers.AutoModelForSequenceClassification.from_pretrained\n## Step3 \n데이터콜렉터 = transformers.DataCollatorWithPadding(tokenizer=토크나이저)\ndef 평가하기(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    accuracy = evaluate.load(\"accuracy\")\n    return accuracy.compute(predictions=predictions, references=labels)\n트레이너세부지침생성기 = transformers.TrainingArguments\n트레이너생성기 = transformers.Trainer\n## Step4 \n강인공지능생성하기 = transformers.pipeline\n\n/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n\n\n\n## Step1 \n데이터 = 데이터불러오기('emotion')\n전처리된데이터 = 데이터.map(데이터전처리하기2,batched=True)\n전처리된훈련자료, 전처리된검증자료 = 전처리된데이터['train'], 전처리된데이터['validation']\n## Step2 \n인공지능 = 인공지능생성하기(\"distilbert/distilbert-base-uncased\", num_labels=6)\n## Step3 \n트레이너세부지침 = 트레이너세부지침생성기(\n    output_dir=\"my_awesome_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=1, # 전체문제세트를 2번 공부하라..\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n)\n트레이너 = 트레이너생성기(\n    model=인공지능,\n    args=트레이너세부지침,\n    train_dataset=전처리된훈련자료,\n    eval_dataset=전처리된검증자료,\n    tokenizer=토크나이저,\n    data_collator=데이터콜렉터,\n    compute_metrics=평가하기,\n)\n트레이너.train()\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n    \n      \n      \n      [1000/1000 00:26, Epoch 1/1]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n0.264700\n0.203025\n0.932500\n\n\n\n\n\n\nTrainOutput(global_step=1000, training_loss=0.5039049835205078, metrics={'train_runtime': 27.0307, 'train_samples_per_second': 591.921, 'train_steps_per_second': 36.995, 'total_flos': 194625049506048.0, 'train_loss': 0.5039049835205078, 'epoch': 1.0})\n\n\n(2) 1-(2)에서 구해진 text에 대하여 감성분석을 수행하라. – 20점\n힌트 1-(2)를 풀지못하였다면 아래의 코드를 이용하여 강제설정할 것\n['i feels so lame',\n 'i feel any better',\n 'i just feel tender',\n 'i feel so damn agitated',\n 'i feel alarmed',\n 'i feel all funny sometimes']\n\n## Step4 \n강인공지능 = 강인공지능생성하기(\"sentiment-analysis\", model=\"my_awesome_model/checkpoint-1000\")\n강인공지능([\n    'i feels so lame',\n    'i feel any better',\n    'i just feel tender',\n    'i feel so damn agitated',\n    'i feel alarmed',\n    'i feel all funny sometimes'\n])\n\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n\n\n[{'label': 'LABEL_0', 'score': 0.9876920580863953},\n {'label': 'LABEL_1', 'score': 0.9838089942932129},\n {'label': 'LABEL_2', 'score': 0.9324933886528015},\n {'label': 'LABEL_3', 'score': 0.7004759907722473},\n {'label': 'LABEL_4', 'score': 0.9285796880722046},\n {'label': 'LABEL_5', 'score': 0.8900141716003418}]\n\n\n\n\n3. O/X. – 10점\n\n모두 맞출경우만 정답으로 인정\n\n아래의 제시문을 읽고 올바르게 해석한 사람을 모두 고르라.\n\n\n\n\n\n\nNote\n\n\n\n나는 50,000개의 “텍스트-라벨” 데이터를 인공지능에게 학습시켰다. 학습이 끝난 후, 50,000개의 데이터 중 20개의 샘플을 무작위로 뽑아 테스트한 결과, 인공지능은 20개의 텍스트에 대한 라벨을 모두 정확히 맞췄다. 이 결과만으로 인공지능이 영화 리뷰에 대한 감성 분석(긍정/부정)을 성공적으로 학습했다고 결론을 내려도 될까?\n\n\n민지: 제시문에 따르면 50,000개의 훈련 데이터를 사용하여 인공지능을 학습시켰으니, train_data의 크기는 50,000이라고 볼 수 있어.\n하니: 50,000개의 데이터 중 일부를 무작위로 샘플링하여 평가하는 것은 올바른 방법이 아니야. 학습에 사용되지 않은 별도의 테스트 데이터를 사용해 성능을 평가해야 인공지능이 제대로 학습했는지 알 수 있어.\n다니엘: 하니의 말이 맞아. 50,000개의 데이터 중 20개를 샘플링한 게 아니라, 50,000개의 데이터를 모두 올바르게 맞췄다고 하더라도, 새로운 데이터에 대해 성능이 좋다고 단정할 수는 없어. 중요한 건 새로운 데이터에 대한 예측 성능이지.\n해린: 맞아, 훈련 데이터 (=학습 데이터) 를 너무 반복해서 학습하다 보면, 인공지능이 그 데이터에만 지나치게 맞춰져서 새로운 데이터를 잘 처리하지 못할 수 있어. 결국 모델이 학습 데이터에서는 좋은 성능을 내지만, 학습하지 않은 데이터나 실제 환경에서는 성능이 떨어질 위험이 생기는 거야.”\n혜인: 그렇구나! 그래서 50,000개의 데이터가 있더라도, 그 중 일부만 학습에 사용하고, 나머지는 평가용으로 따로 남겨두기도 하는 거네. 이렇게 하면 인공지능이 새로운 데이터에서도 잘 작동하는지, 성능을 확인할 수 있는 거잖아?\n\n모두 정답"
  },
  {
    "objectID": "quiz/Quiz-1.html",
    "href": "quiz/Quiz-1.html",
    "title": "Quiz-1 (2024.09.05) // 범위: 파이썬기본문법",
    "section": "",
    "text": "항목\n허용 여부\n비고\n\n\n\n\n강의노트 참고\n허용\n수업 중 제공된 강의노트나 본인이 정리한 자료를 참고 가능\n\n\n구글 검색\n허용\n인터넷을 통한 자료 검색 및 정보 확인 가능\n\n\n생성 모형 사용\n허용 안함\n인공지능 기반 도구(GPT 등) 사용 불가\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\n\n모든 문항은 부분점수 없음.\n일부문항은 부분문제를 모두 맞출경우만 정답으로 인정함.\n.ipynb 파일 형태로 LMS에 제출된 답안지만 채점하며 그 외의 형식 (.hwp등)은 채점하지 않음. 즉 0점 처리함. 제출방법 모르면 물어볼것!\n\n\n\n\n공지사항: (1) 부정행위 적발시 F학점 처리. 부정행위는 판단하는 기준은 교수주관임. (2) 지연제출은 별다른 감점이 없을 예정. 지속적인 지연제출은 감점 대상이 댈 수 있음. (3) 지각역시 별다른 패널티가 없을 예정이나 반복적인 지각은 지양할 것. (4) 모든 퀴즈를 응시하더라도 성적미달시 F가 나올 수 있음.\n\n\n1. 10점\n아래의 코드를 관찰하고 올바르게 해석한 학생을 모두 골라라.\n\n올바르게 해석한 학생을 모두 정확하게 맞출경우만 20점으로 인정하고 그 외의는 0점 처리함\n\n\ndef double_result(func):\n    def wrapper(*args, **kwargs):\n        result = func(*args, **kwargs)\n        return result * 2\n    return wrapper\n\ndef add(a, b):\n    return a + b\n\n@double_result\ndef add2(a, b):\n    return a + b\n\n민수: 데코레이터는 함수 결과를 두 배로 만들어 반환하므로 add2(4, 4)의 결과는 8 * 2 = 16이다. 출력 결과는 16이다.\n선철: add(-2, 2) 와 add2(-2, 2)의 출력결과는 같다.\n예지: 데코레이터는 매개변수(=함수의 입력값)를 두 배로 해서 함수에 전달한다. 그래서 add2(4, 4)는 (4 * 2) + (4 * 2)가 되어 출력 결과는 16이다.\n준현: 데코레이터는 add2 함수의 입력값을 곱해서 리턴하도록 동작한다. 따라서 add2(4, 4)의 결과는 (4 * 4) = 16 이 출력된다.\n(풀이)\n답: 민수, 선철\n\n\n2. 10점\n아래의 코드를 관찰하고 올바르게 해석한 학생을 모두 골라라.\n\n올바르게 해석한 학생을 모두 정확하게 맞출경우만 20점으로 인정하고 그 외의는 0점 처리함\n\n\ndef infinite_sequence():\n    num = 0\n    while True:\n        yield num\n        num += 1\n\ngen = infinite_sequence()\n\nfor _ in range(5):\n    print(next(gen))\n\n0\n1\n2\n3\n4\n\n\n\nset(dir(gen)) & {'__next__'}\n\n{'__next__'}\n\n\n토르: infinite_sequence()는 무한 루프를 돌며 num 값을 하나씩 yield로 반환하고, next()로 값을 호출할 때마다 그 값을 증가시킨다. 따라서 이 코드는 (0, 1, 2, 3, 4)를 순서대로 출력한다.\n캡틴: infinite_sequence()는 5개의 값을 반환한 후 더 이상 값을 생성하지 않고 종료된다. 즉, 그 이후에 next(gen)을 호출하면 더 이상 값이 반환되지 않고 StopIteration 예외가 발생한다.\n위도우: gen은 호출가능한 객체 (callable object) 이다.\n헐크: 이 코드에서 추가로 아래의 코드를 실행하면\nfor _ in range(10):\n    print(next(gen))\n다음 10개의 값(5, 6, 7, 8, 9, 10, 11, 12, 13, 14)이 순서대로 출력된다.\n(풀이)\n답: 토르, 헐크\n\n\n3. 20점\n아래의 코드를 관찰하고 올바르게 해석한 학생을 모두 골라라.\n\n올바르게 해석한 학생을 모두 정확하게 맞출경우만 20점으로 인정하고 그 외의는 0점 처리함\n\n\nclass Dummy:\n    def __init__(self, a):\n        self.a = a\n\nclass Dummy2(Dummy):\n    def __init__(self, a):\n        super().__init__(a)\n        self.a = self.a + 1\n\nclass Dummy3(Dummy):\n    def __init__(self, a):\n        super().__init__(a)\n        self.a = self.a + 2\n\nclass Dummy4(Dummy3, Dummy2):\n    def __init__(self, a):\n        super(Dummy2, self).__init__(a)\n        self.a = self.a + 3\n\n민지: Dummy4 인스턴스를 생성할 때 super(Dummy2, self).__init__(a)가 호출되어, MRO에 따라 먼저 Dummy3의 __init__ 메소드가 실행된다. 그 후 Dummy4의 __init__ 에서 self.a = self.a + 3 이 실행된다.\n시혁: Dummy4 인스턴스를 생성할 때 super(Dummy2, self).__init__(a)가 호출되면 Dummy2의 __init__이 실행된다. 그 후 Dummy4의 __init__ 에서 self.a = self.a + 3 이 실행된다.\n희진: Dummy4는 다중 상속을 받았으므로 Dummy2, 3의 __init__이 모두 실행된다. 그 후 Dummy4의 __init__ 에서 self.a = self.a + 3 이 실행된다.\n팜하니: Dummy4는 다중 상속을 사용하므로 Dummy, Dummy2, Dummy3 의 __init__ 순서대로 모두 호출한다. 그리고 마지막으로 Dummy4의 __init__ 에서 self.a = self.a + 3 이 실행된다.\n(풀이)\n답: 모두 틀렸음\n\n\n4. 20점\n\n!pip install datasets # 우선 이걸 실행해서 패키지 설치하세요, 못하겠으면 손들고 저를 부르세요\n\n아래의 코드를 관찰하고 올바르게 해석한 학생을 모두 골라라.\n\n올바르게 해석한 학생을 모두 정확하게 맞출경우만 20점으로 인정하고 그 외의는 0점 처리함\n\n\nfrom datasets import load_dataset\nimdb = load_dataset(\"imdb\")\n\n/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n동용: load_dataset은 function 클래스에서 생성된 인스턴스이다.\n태한: imdb 는 __len__ 메소드를 가지고 있다. 따라서 len(imdb) 는 실행가능한 코드임을 알 수 있다.\n대영: imdb['train'], imdb['test'] 역시 모두 __len__ 메소드를 가지고 있다. 그리고 len(imdb['train']) + len(imdb['test']) 의 값은 len(imbd)의 값과 같다.\n현수: imdb['train']는 __iter__ 메소드를 가지고 있다. 따라서 아래의 코드가 실행가능하다.\niterator = iter(imdb['train'])\nprint(next(iterator))\nprint(next(iterator))\n(풀이)\n답: 동용, 태한, 현수\n\n채점시 동용과 대영은 틀려도 봐줬습니다. 동용은 개념이 약하면 헷갈릴 수 있다고 생각했고, 대영은 본의아니게 낚시처럼 문제가 되어서요, 실수할수 있다고 생각했습니다.\n\n\n\n5. 40점\n주어진 load_dataset() 함수의 도움말에 대한 설명을 보고, 설명이 맞으면 O, 틀리면 X를 선택하라.\n\n모두 맞출 경우만 정답으로 인정\n\nload_dataset?\nSignature:\nload_dataset(\n    path: str,\n    name: Optional[str] = None,\n    data_dir: Optional[str] = None,\n    data_files: Union[str, Sequence[str], Mapping[str, Union[str, Sequence[str]]], NoneType] = None,\n    split: Union[str, datasets.splits.Split, NoneType] = None,\n    cache_dir: Optional[str] = None,\n    features: Optional[datasets.features.features.Features] = None,\n    download_config: Optional[datasets.download.download_config.DownloadConfig] = None,\n    download_mode: Union[datasets.download.download_manager.DownloadMode, str, NoneType] = None,\n    verification_mode: Union[datasets.utils.info_utils.VerificationMode, str, NoneType] = None,\n    ignore_verifications='deprecated',\n    keep_in_memory: Optional[bool] = None,\n    save_infos: bool = False,\n    revision: Union[str, datasets.utils.version.Version, NoneType] = None,\n    token: Union[bool, str, NoneType] = None,\n    use_auth_token='deprecated',\n    task='deprecated',\n    streaming: bool = False,\n    num_proc: Optional[int] = None,\n    storage_options: Optional[Dict] = None,\n    trust_remote_code: bool = None,\n    **config_kwargs,\n) -&gt; Union[datasets.dataset_dict.DatasetDict, datasets.arrow_dataset.Dataset, datasets.dataset_dict.IterableDatasetDict, datasets.iterable_dataset.IterableDataset]\nDocstring:\n여기서 부터는 너무 기니까 생략~\n1 path는 위치 인자로 전달될 수 있다.\n2. name 인자는 함수 호출 시 키워드 인자로 전달될 수 있으며, 기본값은 None이다.\n3. load_dataset() 함수는 가변 위치 인자를 허용하지 않는다.\n4. config_kwargs는 가변 키워드 인자이며, 추가적인 키워드 인자를 받기 위한 것이다.\n5. path의 타입 힌트는 str이다. 하지만 이것이path 인자의 값을 문자열 값만 전달될 수 있다는 의미는 아니다. Python에서 타입 힌트는 강제적인 제약이 아니라 개발자에게 코드의 의도를 알리기 위한 도구일 뿐이다. 따라서 Python에서는 타입 힌트를 무시하고 다른 타입의 값을 전달할 수 있다.\n6. split 인자의 타입 힌트는 Union[str, datasets.splits.Split, NoneType]이므로, 문자열(str), datasets.Split 객체, 또는 None의 형태를 입력으로 권장한다.\n7. download_config 인자는 가변 키워드 인자로 전달된다.\n8. data_dir은 위치 인자로도 전달될 수 있다.\n9. cache_dir 인자의 타입 힌트는 Optional[str]이므로, 문자열(str) 또는 None의 형태를 입력으로 권장한다.\n10. **config_kwargs는 함수 호출 시 몇 개의 키워드 인자를 받을 수 있는지 미리 알 필요가 없다.\n(풀이)\n7만 틀렸음.\n\n8번은 헷갈릴만 해서 틀려도 봐줬습니다. 8번이외에 한개정도 더 실수한 건 만점드렸습니다."
  },
  {
    "objectID": "quiz.html",
    "href": "quiz.html",
    "title": "Quiz",
    "section": "",
    "text": "공지사항\n\n퀴즈1은 점수에 포함되지 않습니다.\n퀴즈2의 2-(2)의 채점은 가산점으로 처리합니다. (따라서 퀴즈2는 80점만점)\n\n퀴즈\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nNov 26, 2024\n\n\nQuiz-8 (2024.11.26) // 범위: 10wk-1 까지\n\n\n최규빈 \n\n\n\n\nNov 19, 2024\n\n\nQuiz-7 (2024.11.19) // 범위: 09wk-2 까지\n\n\n최규빈 \n\n\n\n\nNov 12, 2024\n\n\nQuiz-6 (2024.11.12) // 범위: 08wk-2 까지\n\n\n최규빈 \n\n\n\n\nNov 5, 2024\n\n\nQuiz-5 (2024.11.05) // 범위: 07wk-1 까지\n\n\n최규빈 \n\n\n\n\nOct 22, 2024\n\n\nQuiz-4 (2024.10.22) // 범위: 05wk-1 까지\n\n\n최규빈 \n\n\n\n\nOct 8, 2024\n\n\nQuiz-3 (2024.10.08) // 범위: 04wk-1 까지\n\n\n최규빈 \n\n\n\n\nSep 24, 2024\n\n\nQuiz-2 (2024.09.24) // 범위: 02wk-1 까지\n\n\n최규빈 \n\n\n\n\nSep 5, 2024\n\n\nQuiz-1 (2024.09.05) // 범위: 파이썬기본문법\n\n\n최규빈 \n\n\n\n\n\nNo matching items"
  }
]